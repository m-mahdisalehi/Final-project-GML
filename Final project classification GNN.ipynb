{"cells":[{"cell_type":"markdown","metadata":{"id":"i-1mlOAZXtXq"},"source":["## Sixth Session (Related to the Course Project)"]},{"cell_type":"markdown","metadata":{"id":"ItTJq8jSXtXu"},"source":["---------------"]},{"cell_type":"markdown","metadata":{"id":"FEbR_mHxXtXv"},"source":["## Graph Classification with [Deep Graph Library (DGL)](https://docs.dgl.ai/index.html) for the graduate course \"[Graph Machine learning](https://github.com/zahta/graph_ml)\"\n","\n","### Dataset: BBBP\n","\n","##### by [Zahra Taheri](https://github.com/zahta), 06 June 2023"]},{"cell_type":"markdown","metadata":{"id":"yt-KEa1PXtXw"},"source":["---------------"]},{"cell_type":"markdown","metadata":{"id":"Af9kWw2TXtXw"},"source":["### This Tutorial Is Prepared Based on the Following References\n","\n","- [FunQG: Molecular Representation Learning via Quotient Graphs](https://pubs.acs.org/doi/10.1021/acs.jcim.3c00445)\n","- [Supporting Information of FunQG](https://pubs.acs.org/doi/suppl/10.1021/acs.jcim.3c00445/suppl_file/ci3c00445_si_001.pdf)\n","- [GitHub Repository of FunQG](https://github.com/hhaji/funqg)"]},{"cell_type":"code","source":["!pip install dgl"],"metadata":{"id":"ABbgpnL2GZMO","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1688405928252,"user_tz":-210,"elapsed":6225,"user":{"displayName":"Mahdi Salehi","userId":"13663744920024374425"}},"outputId":"35d0b004-7899-4c00-e404-8ff5a3bd50b9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting dgl\n","  Downloading dgl-1.1.1-cp310-cp310-manylinux1_x86_64.whl (6.3 MB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/6.3 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.3/6.3 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/6.3 MB\u001b[0m \u001b[31m27.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m65.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m48.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from dgl) (1.22.4)\n","Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from dgl) (1.10.1)\n","Requirement already satisfied: networkx>=2.1 in /usr/local/lib/python3.10/dist-packages (from dgl) (3.1)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from dgl) (2.27.1)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from dgl) (4.65.0)\n","Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from dgl) (5.9.5)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->dgl) (1.26.16)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->dgl) (2023.5.7)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->dgl) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->dgl) (3.4)\n","Installing collected packages: dgl\n","Successfully installed dgl-1.1.1\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Os4tcuBUXtXx"},"outputs":[],"source":["%matplotlib inline\n","import os\n","\n","os.environ[\"DGLBACKEND\"] = \"pytorch\"\n","import dgl\n","import numpy as np\n","import networkx as nx\n","import torch\n","import torch.nn as nn\n","import dgl.function as fn\n","import torch.nn.functional as F\n","import shutil\n","from torch.utils.data import DataLoader\n","import cloudpickle\n","from dgl.nn import GraphConv"]},{"cell_type":"markdown","metadata":{"id":"jEajxfFQXtXz"},"source":["#### Set Path"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EiiiFA2zXtX0"},"outputs":[],"source":["current_dir = \"./\"\n","checkpoint_path = current_dir + \"save_models/model_checkpoints/\" + \"checkpoint\"\n","os.makedirs(checkpoint_path, exist_ok=True)\n","\n","best_model_path = current_dir + \"save_models/best_model/\"\n","\n","folder_data_temp = current_dir +\"data_temp/\"\n","shutil.rmtree(folder_data_temp, ignore_errors=True)\n","\n","path_save = current_dir + \"graph_data.zip\"\n","shutil.unpack_archive(path_save, folder_data_temp)"]},{"cell_type":"markdown","metadata":{"id":"rBuQRkYAXtX0"},"source":["#### Custom PyTorch Datasets"]},{"cell_type":"code","source":["\"\"\" Classification Dataset \"\"\"\n","class DGLDatasetClass(torch.utils.data.Dataset):\n","    def __init__(self, address):\n","        # Constructor method for the dataset\n","        # address: the file path of the dataset\n","        self.address=address+\".bin\"\n","        # Load the dataset from the given file path\n","        self.list_graphs, train_labels_masks_globals = dgl.load_graphs(self.address)\n","        num_graphs =len(self.list_graphs)\n","        # Extract the labels, masks and global features from the dataset\n","        self.labels = train_labels_masks_globals[\"labels\"].view(num_graphs,-1)\n","        self.masks = train_labels_masks_globals[\"masks\"].view(num_graphs,-1)\n","        self.globals = train_labels_masks_globals[\"globals\"].view(num_graphs,-1)\n","\n","    def __len__(self):\n","        # Method to return the length of the dataset\n","        return len(self.list_graphs)\n","\n","    def __getitem__(self, idx):\n","        # Method to return a single sample from the dataset\n","        # idx: the index of the sample to be returned\n","        return  self.list_graphs[idx], self.labels[idx], self.masks[idx], self.globals[idx]"],"metadata":{"id":"ArNErn2GeIaJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oiDW39kGXtX1"},"source":["#### Defining Train, Validation, and Test Set"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kXNJWz4CXtX2","outputId":"4247de3c-de90-439b-814c-3bead7e9766b","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1688405992126,"user_tz":-210,"elapsed":472,"user":{"displayName":"Mahdi Salehi","userId":"13663744920024374425"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["1631 203 205\n"]}],"source":["path_data_temp = folder_data_temp + \"scaffold\"+\"_\"+str(0)\n","train_set = DGLDatasetClass(address=path_data_temp+\"_train\")\n","val_set = DGLDatasetClass(address=path_data_temp+\"_val\")\n","test_set = DGLDatasetClass(address=path_data_temp+\"_test\")\n","\n","print(len(train_set), len(val_set), len(test_set))\n"]},{"cell_type":"markdown","metadata":{"id":"5wjivnkXXtX4"},"source":["#### Data Loader"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kEdfFqmhXtX4"},"outputs":[],"source":["def collate(batch):\n","    # batch is a list of tuples (graphs, labels, masks, globals)\n","    # Concatenate a sequence of graphs\n","    graphs = [e[0] for e in batch]\n","    g = dgl.batch(graphs)\n","\n","    # Concatenate a sequence of tensors (labels) along a new dimension\n","    labels = [e[1] for e in batch]\n","    labels = torch.stack(labels, 0)\n","\n","    # Concatenate a sequence of tensors (masks) along a new dimension\n","    masks = [e[2] for e in batch]\n","    masks = torch.stack(masks, 0)\n","\n","    # Concatenate a sequence of tensors (globals) along a new dimension\n","    globals = [e[3] for e in batch]\n","    globals = torch.stack(globals, 0)\n","\n","    return g, labels, masks, globals\n","\n","\n","def loader(batch_size):\n","    # This function returns a set of data loaders for the training, validation, and test sets\n","    train_dataloader = DataLoader(train_set,\n","                              batch_size=batch_size,\n","                              collate_fn=collate,\n","                              drop_last=False,\n","                              shuffle=True,\n","                              num_workers=1)\n","\n","    val_dataloader =  DataLoader(val_set,\n","                             batch_size=batch_size,\n","                             collate_fn=collate,\n","                             drop_last=False,\n","                             shuffle=False,\n","                             num_workers=1)\n","\n","    test_dataloader = DataLoader(test_set,\n","                             batch_size=batch_size,\n","                             collate_fn=collate,\n","                             drop_last=False,\n","                             shuffle=False,\n","                             num_workers=1)\n","    return train_dataloader, val_dataloader, test_dataloader"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a-gFoy-oXtX5"},"outputs":[],"source":["train_dataloader, val_dataloader, test_dataloader = loader(batch_size=32)"]},{"cell_type":"markdown","metadata":{"id":"PLPEIlKKXtX5"},"source":["#### Defining A GNN"]},{"cell_type":"markdown","metadata":{"id":"DVtYqdKrXtX5"},"source":["##### Some Variables"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dRL8fXUMXtX6"},"outputs":[],"source":["#BBBP dataset has 1 task. Some other datasets may have some more number of tasks, e.g., tox21 has 12 tasks.\n","num_tasks = 1\n","\n","# Size of global feature of each graph\n","global_size = 200\n","\n","# Number of epochs to train the model\n","num_epochs = 100\n","\n","# Number of steps to wait if the model performance on the validation set does not improve\n","patience = 10\n","\n","#Configurations to instantiate the model\n","config = {\"node_feature_size\":127, \"edge_feature_size\":12, \"hidden_size\":100}\n"]},{"cell_type":"markdown","source":["#GNN 1"],"metadata":{"id":"w0eFHz6qPAZD"}},{"cell_type":"code","source":["class GNN1(nn.Module):\n","    def __init__(self, config, global_size = 200, num_tasks = 1):\n","        super().__init__()\n","        self.config = config\n","        self.num_tasks = num_tasks\n","\n","        # Node feature size\n","        self.node_feature_size = self.config.get('node_feature_size', 127)\n","\n","        # Edge feature size\n","        self.edge_feature_size = self.config.get('edge_feature_size', 12)\n","\n","        # Hidden size\n","        self.hidden_size = self.config.get('hidden_size', 100)\n","\n","        # Create two GraphConv layers for the message passing\n","        self.conv1 = GraphConv(self.node_feature_size, self.hidden_size,allow_zero_in_degree=True)\n","        self.conv2 = GraphConv(self.hidden_size, self.num_tasks,allow_zero_in_degree=True)\n","\n","    # Define the forward method of the model\n","    def forward(self, mol_dgl_graph, globals):\n","        # The forward method takes a molecular graph and a global feature vector as input\n","        mol_dgl_graph.ndata[\"v\"]= mol_dgl_graph.ndata[\"v\"][:,:self.node_feature_size]\n","        # Truncate the node feature size to the specified size\n","        mol_dgl_graph.edata[\"e\"] = mol_dgl_graph.edata[\"e\"][:,:self.edge_feature_size]\n","        # Truncate the edge feature size to the specified size\n","        h = self.conv1(mol_dgl_graph, mol_dgl_graph.ndata[\"v\"])\n","        h = F.relu(h)\n","        h = self.conv2(mol_dgl_graph, h)\n","        mol_dgl_graph.ndata[\"h\"] = h\n","        # Store the final node representations as node features in the graph\n","        return dgl.mean_nodes(mol_dgl_graph, \"h\")\n","        # Compute the mean of the node features across all nodes and return it as the output of the model"],"metadata":{"id":"FpQVm2LSoX3A"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iUj9ibRrXtX7"},"source":["#### Function to Compute Score of the Model"]},{"cell_type":"code","source":["from sklearn.metrics import roc_auc_score\n","\n","def compute_score(model, data_loader, val_size, num_tasks):\n","    # A function to compute the performance score of the model on the given data\n","    # model: the trained model to be evaluated\n","    # data_loader: the data loader for the validation or test set\n","    model.eval()\n","    metric = roc_auc_score\n","    with torch.no_grad():\n","        # Initialize empty tensors to store the predictions, labels, and masks\n","        prediction_all= torch.empty(0)\n","        labels_all= torch.empty(0)\n","        masks_all= torch.empty(0)\n","        # Iterate over the data loader and make predictions for each batch\n","        for i, (mol_dgl_graph, labels, masks, globals) in enumerate(data_loader):\n","            prediction = model(mol_dgl_graph, globals)\n","            prediction = torch.sigmoid(prediction)\n","            # Concatenate the predictions, labels, and masks for all batches\n","            prediction_all = torch.cat((prediction_all, prediction), 0)\n","            labels_all = torch.cat((labels_all, labels), 0)\n","            masks_all = torch.cat((masks_all, masks), 0)\n","        # Compute the average score across all tasks\n","        average = torch.tensor([0.])\n","        for i in range(num_tasks):\n","            a1 = prediction_all[:, i][masks_all[:,i]==1]\n","            a2 = labels_all[:, i][masks_all[:,i]==1]\n","            try:\n","                # Compute the metric score for the current task\n","                t = metric(a2.int().cpu(), a1.cpu()).item()\n","            except ValueError:\n","                # If the metric cannot be computed, set the score to 0\n","                t = 0\n","            average += t\n","    return average.item()/num_tasks"],"metadata":{"id":"aD9QZrlkerU-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"J2JQWmP_XtX7"},"source":["#### Loss Function"]},{"cell_type":"code","source":["def loss_func(output, label, mask, num_tasks):\n","    # A function to compute the loss between the model's output and the true labels\n","    # mask: the binary mask indicating which samples should be included in the loss calculation\n","    pos_weight = torch.ones((1, num_tasks))\n","    # Create a tensor of ones with the same shape as the output tensor (to be used as positive weights)\n","    pos_weight\n","    criterion = torch.nn.BCEWithLogitsLoss(reduction='none', pos_weight=pos_weight)\n","    # Create a loss function that applies binary cross-entropy with logits\n","    # The 'none' reduction parameter is used to keep the loss for each sample separate\n","    # The positive weights are used to give more weight to the positive class (to handle class imbalance)\n","    loss = mask*criterion(output,label)\n","    # Compute the loss for each sample, but only include the samples specified by the mask\n","    loss = loss.sum() / mask.sum()\n","    return loss"],"metadata":{"id":"5wqQaWXkhgVG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MznGLA38XtX8"},"source":["#### Training and Evaluation"]},{"cell_type":"markdown","metadata":{"id":"IyULqGodXtX9"},"source":["##### Training Function"]},{"cell_type":"code","source":["def train_epoch(train_dataloader, model, optimizer):\n","    # A function to train the model for one epoch on the given data\n","    epoch_train_loss = 0\n","    iterations = 0\n","    model.train() # Prepare model for training\n","    for i, (mol_dgl_graph, labels, masks, globals) in enumerate(train_dataloader):\n","        # Iterate over the data loader and get the input samples\n","        prediction = model(mol_dgl_graph, globals)\n","        # Make predictions for the input samples using the model\n","        loss_train = loss_func(prediction, labels, masks, num_tasks)\n","        # Compute the loss between the predictions and the true labels\n","        optimizer.zero_grad(set_to_none=True)\n","        # Zero the gradients of the optimizer (to prevent accumulation from previous iterations)\n","        loss_train.backward()\n","        # Compute the gradients of the loss with respect to the model parameters\n","        optimizer.step()\n","        # Update the model parameters using the computed gradients\n","        epoch_train_loss += loss_train.detach().item()\n","        iterations += 1\n","    epoch_train_loss /= iterations\n","    return epoch_train_loss"],"metadata":{"id":"j0dQtMuRigQ9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def train_evaluate():\n","    # A function to train and evaluate the model\n","    model = GNN1(config, global_size, num_tasks)\n","    # Create a new instance of the GNN1 model\n","    optimizer = torch.optim.Adam(model.parameters(), lr = 0.001)\n","    # Create an Adam optimizer to update the model parameters during training\n","    best_val = 0\n","    patience_count = 1\n","    epoch = 1\n","    while epoch <= num_epochs:\n","        # Train and evaluate the model for the specified number of epochs\n","        if patience_count <= patience:\n","            # Check if the patience count has been exceeded\n","            model.train()\n","            # Set the model to training mode\n","            loss_train = train_epoch(train_dataloader, model, optimizer)\n","            # Train the model on the training set for one epoch\n","            model.eval()\n","            # Set the model to evaluation mode\n","            score_val = compute_score(model, val_dataloader, len(val_set), num_tasks)\n","            # Evaluate the model on the validation set and compute the validation score\n","            if score_val > best_val:\n","                # Check if the current validation score is better than the best validation score so far\n","                best_val = score_val\n","                # If so, update the best validation score and save a checkpoint of the model\n","                print(\"Save checkpoint\")\n","                path = os.path.join(checkpoint_path, 'checkpoint.pth')\n","                dict_checkpoint = {\"score_val\": score_val}\n","                dict_checkpoint.update({\"model_state_dict\": model.state_dict(), \"optimizer_state\": optimizer.state_dict()})\n","                with open(path, \"wb\") as outputfile:\n","                    cloudpickle.dump(dict_checkpoint, outputfile)\n","                patience_count = 1\n","            else:\n","                # If not, increment the patience count\n","                print(\"Patience\", patience_count)\n","                patience_count += 1\n","\n","            print(\"Epoch: {}/{} | Training Loss: {:.3f} | Valid Score: {:.3f}\".format(\n","            epoch, num_epochs, loss_train, score_val))\n","            print(\" \")\n","            print(\"Epoch: {}/{} | Best Valid Score Until Now: {:.3f}\".format(epoch, num_epochs, best_val), \"\\n\")\n","        epoch += 1\n","\n","    # Save the best model\n","    shutil.rmtree(best_model_path, ignore_errors=True)\n","    shutil.copytree(checkpoint_path, best_model_path)\n","    # Remove any existing best model and copy the checkpoint to the best model path\n","    print(\"Final results:\")\n","    print(\"Average Valid Score: {:.3f}\".format(np.mean(best_val)), \"\\n\")\n","    # Print the final average validation score"],"metadata":{"id":"ZDflF9U1j8KJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8mC9VeiZXtX-"},"source":["##### Function to compute test set score of the final saved model"]},{"cell_type":"code","source":["def test_evaluate():\n","    # A function to evaluate the trained model on the test set\n","    final_model = GNN1(config, global_size, num_tasks)\n","    # Create a new instance of the GNN1 model\n","    path = os.path.join(best_model_path, 'checkpoint.pth')\n","    # Load the best model checkpoint from the best model path\n","    with open(path, 'rb') as f:\n","        checkpoint = cloudpickle.load(f)\n","    final_model.load_state_dict(checkpoint[\"model_state_dict\"])\n","    # Load the saved model state dictionary into the final model\n","    final_model.eval()\n","    # Set the final model to evaluation mode\n","    test_score = compute_score(final_model, test_dataloader, len(test_set), num_tasks)\n","    # Evaluate the final model on the test set and compute the test score\n","    print(\"Test Score: {:.3f}\".format(test_score), \"\\n\")\n","    print(\"Execution time: {:.3f} seconds\".format(time.time() - start_time))"],"metadata":{"id":"xrYT9MIQlTxY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nUaNqb0oXtX-"},"source":["##### Train the model and evaluate its performance"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oObkdncJXtX-","outputId":"b487a567-0a1d-4270-e331-e980f4ec853f","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1688155851711,"user_tz":-210,"elapsed":19171,"user":{"displayName":"Mahdi Salehi","userId":"13663744920024374425"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Save checkpoint\n","Epoch: 1/100 | Training Loss: 0.608 | Valid Score: 0.450\n"," \n","Epoch: 1/100 | Best Valid Score Until Now: 0.450 \n","\n","Save checkpoint\n","Epoch: 2/100 | Training Loss: 0.541 | Valid Score: 0.738\n"," \n","Epoch: 2/100 | Best Valid Score Until Now: 0.738 \n","\n","Save checkpoint\n","Epoch: 3/100 | Training Loss: 0.503 | Valid Score: 0.807\n"," \n","Epoch: 3/100 | Best Valid Score Until Now: 0.807 \n","\n","Save checkpoint\n","Epoch: 4/100 | Training Loss: 0.474 | Valid Score: 0.821\n"," \n","Epoch: 4/100 | Best Valid Score Until Now: 0.821 \n","\n","Save checkpoint\n","Epoch: 5/100 | Training Loss: 0.458 | Valid Score: 0.823\n"," \n","Epoch: 5/100 | Best Valid Score Until Now: 0.823 \n","\n","Save checkpoint\n","Epoch: 6/100 | Training Loss: 0.443 | Valid Score: 0.824\n"," \n","Epoch: 6/100 | Best Valid Score Until Now: 0.824 \n","\n","Save checkpoint\n","Epoch: 7/100 | Training Loss: 0.436 | Valid Score: 0.829\n"," \n","Epoch: 7/100 | Best Valid Score Until Now: 0.829 \n","\n","Patience 1\n","Epoch: 8/100 | Training Loss: 0.430 | Valid Score: 0.826\n"," \n","Epoch: 8/100 | Best Valid Score Until Now: 0.829 \n","\n","Patience 2\n","Epoch: 9/100 | Training Loss: 0.426 | Valid Score: 0.824\n"," \n","Epoch: 9/100 | Best Valid Score Until Now: 0.829 \n","\n","Patience 3\n","Epoch: 10/100 | Training Loss: 0.423 | Valid Score: 0.824\n"," \n","Epoch: 10/100 | Best Valid Score Until Now: 0.829 \n","\n","Save checkpoint\n","Epoch: 11/100 | Training Loss: 0.418 | Valid Score: 0.831\n"," \n","Epoch: 11/100 | Best Valid Score Until Now: 0.831 \n","\n","Patience 1\n","Epoch: 12/100 | Training Loss: 0.419 | Valid Score: 0.823\n"," \n","Epoch: 12/100 | Best Valid Score Until Now: 0.831 \n","\n","Patience 2\n","Epoch: 13/100 | Training Loss: 0.417 | Valid Score: 0.829\n"," \n","Epoch: 13/100 | Best Valid Score Until Now: 0.831 \n","\n","Patience 3\n","Epoch: 14/100 | Training Loss: 0.412 | Valid Score: 0.825\n"," \n","Epoch: 14/100 | Best Valid Score Until Now: 0.831 \n","\n","Patience 4\n","Epoch: 15/100 | Training Loss: 0.410 | Valid Score: 0.827\n"," \n","Epoch: 15/100 | Best Valid Score Until Now: 0.831 \n","\n","Save checkpoint\n","Epoch: 16/100 | Training Loss: 0.412 | Valid Score: 0.831\n"," \n","Epoch: 16/100 | Best Valid Score Until Now: 0.831 \n","\n","Patience 1\n","Epoch: 17/100 | Training Loss: 0.408 | Valid Score: 0.826\n"," \n","Epoch: 17/100 | Best Valid Score Until Now: 0.831 \n","\n","Patience 2\n","Epoch: 18/100 | Training Loss: 0.408 | Valid Score: 0.831\n"," \n","Epoch: 18/100 | Best Valid Score Until Now: 0.831 \n","\n","Patience 3\n","Epoch: 19/100 | Training Loss: 0.411 | Valid Score: 0.827\n"," \n","Epoch: 19/100 | Best Valid Score Until Now: 0.831 \n","\n","Patience 4\n","Epoch: 20/100 | Training Loss: 0.405 | Valid Score: 0.824\n"," \n","Epoch: 20/100 | Best Valid Score Until Now: 0.831 \n","\n","Patience 5\n","Epoch: 21/100 | Training Loss: 0.403 | Valid Score: 0.824\n"," \n","Epoch: 21/100 | Best Valid Score Until Now: 0.831 \n","\n","Patience 6\n","Epoch: 22/100 | Training Loss: 0.403 | Valid Score: 0.820\n"," \n","Epoch: 22/100 | Best Valid Score Until Now: 0.831 \n","\n","Patience 7\n","Epoch: 23/100 | Training Loss: 0.401 | Valid Score: 0.823\n"," \n","Epoch: 23/100 | Best Valid Score Until Now: 0.831 \n","\n","Patience 8\n","Epoch: 24/100 | Training Loss: 0.399 | Valid Score: 0.821\n"," \n","Epoch: 24/100 | Best Valid Score Until Now: 0.831 \n","\n","Patience 9\n","Epoch: 25/100 | Training Loss: 0.398 | Valid Score: 0.825\n"," \n","Epoch: 25/100 | Best Valid Score Until Now: 0.831 \n","\n","Patience 10\n","Epoch: 26/100 | Training Loss: 0.396 | Valid Score: 0.828\n"," \n","Epoch: 26/100 | Best Valid Score Until Now: 0.831 \n","\n","Final results:\n","Average Valid Score: 0.831 \n","\n","Test Score: 0.677 \n","\n","Execution time: 19.546 seconds\n"]}],"source":["import time\n","start_time = time.time()\n","\n","train_evaluate()\n","test_evaluate()"]},{"cell_type":"code","source":[],"metadata":{"id":"PCApTkqjO6wy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#GNN 2"],"metadata":{"id":"pNf7ilqnPGsl"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"4NBLj8xyPEcJ"},"outputs":[],"source":["class GNN2(nn.Module):\n","    def __init__(self, config, global_size = 200, num_tasks = 1):\n","        super().__init__()\n","        self.config = config\n","        self.num_tasks = num_tasks\n","\n","        # Node feature size\n","        self.node_feature_size = self.config.get('node_feature_size', 127)\n","\n","        # Edge feature size\n","        self.edge_feature_size = self.config.get('edge_feature_size', 12)\n","\n","        # Hidden size\n","        self.hidden_size = self.config.get('hidden_size', 100)\n","\n","        self.conv1 = GraphConv(self.node_feature_size, self.hidden_size,allow_zero_in_degree=True)\n","        self.conv2 = GraphConv(self.hidden_size, self.hidden_size,allow_zero_in_degree=True)\n","        self.conv3 = GraphConv(self.hidden_size, self.num_tasks,allow_zero_in_degree=True)\n","\n","    # def forward(self, g, in_feat):\n","    def forward(self, mol_dgl_graph, globals):\n","        mol_dgl_graph.ndata[\"v\"]= mol_dgl_graph.ndata[\"v\"][:,:self.node_feature_size]\n","        mol_dgl_graph.edata[\"e\"] = mol_dgl_graph.edata[\"e\"][:,:self.edge_feature_size]\n","        h = self.conv1(mol_dgl_graph, mol_dgl_graph.ndata[\"v\"])\n","        h = F.relu(h)\n","        h = self.conv2(mol_dgl_graph, h)\n","        h = F.relu(h)\n","        h = self.conv3(mol_dgl_graph, h)\n","        mol_dgl_graph.ndata[\"h\"] = h\n","        return dgl.mean_nodes(mol_dgl_graph, \"h\")"]},{"cell_type":"markdown","metadata":{"id":"J4nhKuPbPEcS"},"source":["#### Function to Compute Score of the Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"o9YPSyL1PEcS"},"outputs":[],"source":["from sklearn.metrics import roc_auc_score\n","\n","def compute_score(model, data_loader, val_size, num_tasks):\n","    model.eval()\n","    metric = roc_auc_score\n","    with torch.no_grad():\n","        prediction_all= torch.empty(0)\n","        labels_all= torch.empty(0)\n","        masks_all= torch.empty(0)\n","        for i, (mol_dgl_graph, labels, masks, globals) in enumerate(data_loader):\n","            prediction = model(mol_dgl_graph, globals)\n","            prediction = torch.sigmoid(prediction)\n","            prediction_all = torch.cat((prediction_all, prediction), 0)\n","            labels_all = torch.cat((labels_all, labels), 0)\n","            masks_all = torch.cat((masks_all, masks), 0)\n","        average = torch.tensor([0.])\n","        for i in range(num_tasks):\n","            a1 = prediction_all[:, i][masks_all[:,i]==1]\n","            a2 = labels_all[:, i][masks_all[:,i]==1]\n","            try:\n","                t = metric(a2.int().cpu(), a1.cpu()).item()\n","            except ValueError:\n","                t = 0\n","            average += t\n","    return average.item()/num_tasks"]},{"cell_type":"markdown","metadata":{"id":"Q6haNQ7DPEcS"},"source":["#### Loss Function"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gDbNe6qIPEcS"},"outputs":[],"source":["def loss_func(output, label, mask, num_tasks):\n","    pos_weight = torch.ones((1, num_tasks))\n","    pos_weight\n","    criterion = torch.nn.BCEWithLogitsLoss(reduction='none', pos_weight=pos_weight)\n","    loss = mask*criterion(output,label)\n","    loss = loss.sum() / mask.sum()\n","    return loss"]},{"cell_type":"markdown","metadata":{"id":"k-OIZ3QpPEcS"},"source":["#### Training and Evaluation"]},{"cell_type":"markdown","metadata":{"id":"PYKIbIx3PEcS"},"source":["##### Training Function"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Lj99-N2MPEcS"},"outputs":[],"source":["def train_epoch(train_dataloader, model, optimizer):\n","    epoch_train_loss = 0\n","    iterations = 0\n","    model.train() # Prepare model for training\n","    for i, (mol_dgl_graph, labels, masks, globals) in enumerate(train_dataloader):\n","        prediction = model(mol_dgl_graph, globals)\n","        loss_train = loss_func(prediction, labels, masks, num_tasks)\n","        optimizer.zero_grad(set_to_none=True)\n","        loss_train.backward()\n","        optimizer.step()\n","        epoch_train_loss += loss_train.detach().item()\n","        iterations += 1\n","    epoch_train_loss /= iterations\n","    return epoch_train_loss"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VfJYJtomPEcT"},"outputs":[],"source":["def train_evaluate():\n","\n","    model = GNN2(config, global_size, num_tasks)\n","    optimizer = torch.optim.Adam(model.parameters(), lr = 0.001)\n","\n","    best_val = 0\n","    patience_count = 1\n","    epoch = 1\n","\n","    while epoch <= num_epochs:\n","        if patience_count <= patience:\n","            model.train()\n","            loss_train = train_epoch(train_dataloader, model, optimizer)\n","            model.eval()\n","            score_val = compute_score(model, val_dataloader, len(val_set), num_tasks)\n","            if score_val > best_val:\n","                best_val = score_val\n","                print(\"Save checkpoint\")\n","                path = os.path.join(checkpoint_path, 'checkpoint.pth')\n","                dict_checkpoint = {\"score_val\": score_val}\n","                dict_checkpoint.update({\"model_state_dict\": model.state_dict(), \"optimizer_state\": optimizer.state_dict()})\n","                with open(path, \"wb\") as outputfile:\n","                    cloudpickle.dump(dict_checkpoint, outputfile)\n","                patience_count = 1\n","            else:\n","                print(\"Patience\", patience_count)\n","                patience_count += 1\n","\n","            print(\"Epoch: {}/{} | Training Loss: {:.3f} | Valid Score: {:.3f}\".format(\n","            epoch, num_epochs, loss_train, score_val))\n","\n","            print(\" \")\n","            print(\"Epoch: {}/{} | Best Valid Score Until Now: {:.3f}\".format(epoch, num_epochs, best_val), \"\\n\")\n","        epoch += 1\n","\n","    # best model save\n","    shutil.rmtree(best_model_path, ignore_errors=True)\n","    shutil.copytree(checkpoint_path, best_model_path)\n","\n","    print(\"Final results:\")\n","    print(\"Average Valid Score: {:.3f}\".format(np.mean(best_val)), \"\\n\")\n"]},{"cell_type":"markdown","metadata":{"id":"UKSImJ2NPEcT"},"source":["##### Function to compute test set score of the final saved model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rOOniCqiPEcT"},"outputs":[],"source":["def test_evaluate():\n","    final_model = GNN2(config, global_size, num_tasks)\n","    path = os.path.join(best_model_path, 'checkpoint.pth')\n","    with open(path, 'rb') as f:\n","        checkpoint = cloudpickle.load(f)\n","    final_model.load_state_dict(checkpoint[\"model_state_dict\"])\n","    final_model.eval()\n","    test_score = compute_score(final_model, test_dataloader, len(test_set), num_tasks)\n","\n","    print(\"Test Score: {:.3f}\".format(test_score), \"\\n\")\n","    print(\"Execution time: {:.3f} seconds\".format(time.time() - start_time))\n"]},{"cell_type":"markdown","metadata":{"id":"hWC0F2ZYPEcT"},"source":["##### Train the model and evaluate its performance"]},{"cell_type":"code","execution_count":null,"metadata":{"outputId":"c674094f-8be3-47d5-8583-582d9c052bfc","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1688412899138,"user_tz":-210,"elapsed":18484,"user":{"displayName":"Mahdi Salehi","userId":"13663744920024374425"}},"id":"OPkwMMSEPEcT"},"outputs":[{"output_type":"stream","name":"stdout","text":["Save checkpoint\n","Epoch: 1/100 | Training Loss: 0.572 | Valid Score: 0.798\n"," \n","Epoch: 1/100 | Best Valid Score Until Now: 0.798 \n","\n","Save checkpoint\n","Epoch: 2/100 | Training Loss: 0.495 | Valid Score: 0.843\n"," \n","Epoch: 2/100 | Best Valid Score Until Now: 0.843 \n","\n","Patience 1\n","Epoch: 3/100 | Training Loss: 0.448 | Valid Score: 0.832\n"," \n","Epoch: 3/100 | Best Valid Score Until Now: 0.843 \n","\n","Save checkpoint\n","Epoch: 4/100 | Training Loss: 0.441 | Valid Score: 0.845\n"," \n","Epoch: 4/100 | Best Valid Score Until Now: 0.845 \n","\n","Patience 1\n","Epoch: 5/100 | Training Loss: 0.428 | Valid Score: 0.841\n"," \n","Epoch: 5/100 | Best Valid Score Until Now: 0.845 \n","\n","Save checkpoint\n","Epoch: 6/100 | Training Loss: 0.413 | Valid Score: 0.848\n"," \n","Epoch: 6/100 | Best Valid Score Until Now: 0.848 \n","\n","Patience 1\n","Epoch: 7/100 | Training Loss: 0.412 | Valid Score: 0.841\n"," \n","Epoch: 7/100 | Best Valid Score Until Now: 0.848 \n","\n","Patience 2\n","Epoch: 8/100 | Training Loss: 0.405 | Valid Score: 0.841\n"," \n","Epoch: 8/100 | Best Valid Score Until Now: 0.848 \n","\n","Patience 3\n","Epoch: 9/100 | Training Loss: 0.405 | Valid Score: 0.836\n"," \n","Epoch: 9/100 | Best Valid Score Until Now: 0.848 \n","\n","Patience 4\n","Epoch: 10/100 | Training Loss: 0.394 | Valid Score: 0.840\n"," \n","Epoch: 10/100 | Best Valid Score Until Now: 0.848 \n","\n","Patience 5\n","Epoch: 11/100 | Training Loss: 0.395 | Valid Score: 0.839\n"," \n","Epoch: 11/100 | Best Valid Score Until Now: 0.848 \n","\n","Patience 6\n","Epoch: 12/100 | Training Loss: 0.392 | Valid Score: 0.828\n"," \n","Epoch: 12/100 | Best Valid Score Until Now: 0.848 \n","\n","Patience 7\n","Epoch: 13/100 | Training Loss: 0.386 | Valid Score: 0.836\n"," \n","Epoch: 13/100 | Best Valid Score Until Now: 0.848 \n","\n","Patience 8\n","Epoch: 14/100 | Training Loss: 0.381 | Valid Score: 0.831\n"," \n","Epoch: 14/100 | Best Valid Score Until Now: 0.848 \n","\n","Patience 9\n","Epoch: 15/100 | Training Loss: 0.380 | Valid Score: 0.828\n"," \n","Epoch: 15/100 | Best Valid Score Until Now: 0.848 \n","\n","Patience 10\n","Epoch: 16/100 | Training Loss: 0.375 | Valid Score: 0.827\n"," \n","Epoch: 16/100 | Best Valid Score Until Now: 0.848 \n","\n","Final results:\n","Average Valid Score: 0.848 \n","\n","Test Score: 0.641 \n","\n","Execution time: 18.238 seconds\n"]}],"source":["import time\n","start_time = time.time()\n","\n","train_evaluate()\n","test_evaluate()"]},{"cell_type":"code","source":[],"metadata":{"id":"dzGPjvG9Q27-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#GNN 3"],"metadata":{"id":"151ejFNHQ4-X"}},{"cell_type":"code","source":["class GNN3(nn.Module):\n","    def __init__(self, config, global_size = 200, num_tasks = 1):\n","        super().__init__()\n","        self.config = config\n","        self.num_tasks = num_tasks\n","\n","        # Node feature size\n","        self.node_feature_size = self.config.get('node_feature_size', 127)\n","\n","        # Edge feature size\n","        self.edge_feature_size = self.config.get('edge_feature_size', 12)\n","\n","        # Hidden size\n","        self.hidden_size = self.config.get('hidden_size', 100)\n","\n","\n","        self.conv1 = GraphConv(self.node_feature_size, 64 ,allow_zero_in_degree=True)\n","        self.conv2 = GraphConv(64, 128,allow_zero_in_degree=True)\n","        self.conv3 = GraphConv(128, 256,allow_zero_in_degree=True)\n","        self.conv4 = GraphConv(256, 128,allow_zero_in_degree=True)\n","        self.conv5 = GraphConv(128, self.num_tasks,allow_zero_in_degree=True)\n","\n","        self.dropout = nn.Dropout(p=0.2)\n","        self.bn1 = nn.BatchNorm1d(64)\n","        self.bn2 = nn.BatchNorm1d(128)\n","        self.bn3 = nn.BatchNorm1d(256)\n","\n","    # def forward(self, g, in_feat):\n","    def forward(self, mol_dgl_graph, globals):\n","        mol_dgl_graph.ndata[\"v\"]= mol_dgl_graph.ndata[\"v\"][:,:self.node_feature_size]\n","        mol_dgl_graph.edata[\"e\"] = mol_dgl_graph.edata[\"e\"][:,:self.edge_feature_size]\n","        h = self.conv1(mol_dgl_graph, mol_dgl_graph.ndata[\"v\"])\n","        h = F.relu(h)\n","        h = self.bn1(h)\n","        h = self.conv2(mol_dgl_graph, h)\n","        h = F.relu(h)\n","        h = self.dropout(h)\n","        h = self.bn2(h)\n","        h = self.conv3(mol_dgl_graph, h)\n","        h = F.relu(h)\n","        h = self.dropout(h)\n","        h = self.bn3(h)\n","        h = self.conv4(mol_dgl_graph, h)\n","        h = F.relu(h)\n","        h = self.bn2(h)\n","        h = self.conv5(mol_dgl_graph, h)\n","        mol_dgl_graph.ndata[\"h\"] = h\n","        return dgl.mean_nodes(mol_dgl_graph, \"h\")"],"metadata":{"id":"NZk2d4EJRKm7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3m4GzZHLQ3S7"},"source":["#### Function to Compute Score of the Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6lt3iSRAQ3S7"},"outputs":[],"source":["from sklearn.metrics import roc_auc_score\n","\n","def compute_score(model, data_loader, val_size, num_tasks):\n","    model.eval()\n","    metric = roc_auc_score\n","    with torch.no_grad():\n","        prediction_all= torch.empty(0)\n","        labels_all= torch.empty(0)\n","        masks_all= torch.empty(0)\n","        for i, (mol_dgl_graph, labels, masks, globals) in enumerate(data_loader):\n","            prediction = model(mol_dgl_graph, globals)\n","            prediction = torch.sigmoid(prediction)\n","            prediction_all = torch.cat((prediction_all, prediction), 0)\n","            labels_all = torch.cat((labels_all, labels), 0)\n","            masks_all = torch.cat((masks_all, masks), 0)\n","        average = torch.tensor([0.])\n","        for i in range(num_tasks):\n","            a1 = prediction_all[:, i][masks_all[:,i]==1]\n","            a2 = labels_all[:, i][masks_all[:,i]==1]\n","            try:\n","                t = metric(a2.int().cpu(), a1.cpu()).item()\n","            except ValueError:\n","                t = 0\n","            average += t\n","    return average.item()/num_tasks"]},{"cell_type":"markdown","metadata":{"id":"Ovvaykz3Q3S8"},"source":["#### Loss Function"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gl2NehsPQ3S8"},"outputs":[],"source":["def loss_func(output, label, mask, num_tasks):\n","    pos_weight = torch.ones((1, num_tasks))\n","    pos_weight\n","    criterion = torch.nn.BCEWithLogitsLoss(reduction='none', pos_weight=pos_weight)\n","    loss = mask*criterion(output,label)\n","    loss = loss.sum() / mask.sum()\n","    return loss"]},{"cell_type":"markdown","metadata":{"id":"Q6XEH862Q3S8"},"source":["#### Training and Evaluation"]},{"cell_type":"markdown","metadata":{"id":"SJTAx55xQ3S8"},"source":["##### Training Function"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MCacLcFAQ3S8"},"outputs":[],"source":["def train_epoch(train_dataloader, model, optimizer):\n","    epoch_train_loss = 0\n","    iterations = 0\n","    model.train() # Prepare model for training\n","    for i, (mol_dgl_graph, labels, masks, globals) in enumerate(train_dataloader):\n","        prediction = model(mol_dgl_graph, globals)\n","        loss_train = loss_func(prediction, labels, masks, num_tasks)\n","        optimizer.zero_grad(set_to_none=True)\n","        loss_train.backward()\n","        optimizer.step()\n","        epoch_train_loss += loss_train.detach().item()\n","        iterations += 1\n","    epoch_train_loss /= iterations\n","    return epoch_train_loss"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kBd-FMbdQ3S9"},"outputs":[],"source":["def train_evaluate():\n","\n","    model = GNN3(config, global_size, num_tasks)\n","    optimizer = torch.optim.Adam(model.parameters(), lr = 0.001)\n","\n","    best_val = 0\n","    patience_count = 1\n","    epoch = 1\n","\n","    while epoch <= num_epochs:\n","        if patience_count <= patience:\n","            model.train()\n","            loss_train = train_epoch(train_dataloader, model, optimizer)\n","            model.eval()\n","            score_val = compute_score(model, val_dataloader, len(val_set), num_tasks)\n","            if score_val > best_val:\n","                best_val = score_val\n","                print(\"Save checkpoint\")\n","                path = os.path.join(checkpoint_path, 'checkpoint.pth')\n","                dict_checkpoint = {\"score_val\": score_val}\n","                dict_checkpoint.update({\"model_state_dict\": model.state_dict(), \"optimizer_state\": optimizer.state_dict()})\n","                with open(path, \"wb\") as outputfile:\n","                    cloudpickle.dump(dict_checkpoint, outputfile)\n","                patience_count = 1\n","            else:\n","                print(\"Patience\", patience_count)\n","                patience_count += 1\n","\n","            print(\"Epoch: {}/{} | Training Loss: {:.3f} | Valid Score: {:.3f}\".format(\n","            epoch, num_epochs, loss_train, score_val))\n","\n","            print(\" \")\n","            print(\"Epoch: {}/{} | Best Valid Score Until Now: {:.3f}\".format(epoch, num_epochs, best_val), \"\\n\")\n","        epoch += 1\n","\n","    # best model save\n","    shutil.rmtree(best_model_path, ignore_errors=True)\n","    shutil.copytree(checkpoint_path, best_model_path)\n","\n","    print(\"Final results:\")\n","    print(\"Average Valid Score: {:.3f}\".format(np.mean(best_val)), \"\\n\")\n"]},{"cell_type":"markdown","metadata":{"id":"FeTf7qu0Q3S9"},"source":["##### Function to compute test set score of the final saved model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tyiQzKOMQ3S9"},"outputs":[],"source":["def test_evaluate():\n","    final_model = GNN3(config, global_size, num_tasks)\n","    path = os.path.join(best_model_path, 'checkpoint.pth')\n","    with open(path, 'rb') as f:\n","        checkpoint = cloudpickle.load(f)\n","    final_model.load_state_dict(checkpoint[\"model_state_dict\"])\n","    final_model.eval()\n","    test_score = compute_score(final_model, test_dataloader, len(test_set), num_tasks)\n","\n","    print(\"Test Score: {:.3f}\".format(test_score), \"\\n\")\n","    print(\"Execution time: {:.3f} seconds\".format(time.time() - start_time))\n"]},{"cell_type":"markdown","metadata":{"id":"hDzvOfe4Q3S9"},"source":["##### Train the model and evaluate its performance"]},{"cell_type":"code","source":["import time\n","start_time = time.time()\n","\n","train_evaluate()\n","test_evaluate()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1688416631764,"user_tz":-210,"elapsed":43290,"user":{"displayName":"Mahdi Salehi","userId":"13663744920024374425"}},"outputId":"122bbfcd-b904-4115-a97b-3ee763999afa","id":"6zjkpN4xQ3S9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Save checkpoint\n","Epoch: 1/100 | Training Loss: 0.532 | Valid Score: 0.836\n"," \n","Epoch: 1/100 | Best Valid Score Until Now: 0.836 \n","\n","Patience 1\n","Epoch: 2/100 | Training Loss: 0.417 | Valid Score: 0.812\n"," \n","Epoch: 2/100 | Best Valid Score Until Now: 0.836 \n","\n","Patience 2\n","Epoch: 3/100 | Training Loss: 0.384 | Valid Score: 0.812\n"," \n","Epoch: 3/100 | Best Valid Score Until Now: 0.836 \n","\n","Save checkpoint\n","Epoch: 4/100 | Training Loss: 0.351 | Valid Score: 0.839\n"," \n","Epoch: 4/100 | Best Valid Score Until Now: 0.839 \n","\n","Patience 1\n","Epoch: 5/100 | Training Loss: 0.346 | Valid Score: 0.808\n"," \n","Epoch: 5/100 | Best Valid Score Until Now: 0.839 \n","\n","Patience 2\n","Epoch: 6/100 | Training Loss: 0.320 | Valid Score: 0.820\n"," \n","Epoch: 6/100 | Best Valid Score Until Now: 0.839 \n","\n","Patience 3\n","Epoch: 7/100 | Training Loss: 0.332 | Valid Score: 0.827\n"," \n","Epoch: 7/100 | Best Valid Score Until Now: 0.839 \n","\n","Patience 4\n","Epoch: 8/100 | Training Loss: 0.311 | Valid Score: 0.799\n"," \n","Epoch: 8/100 | Best Valid Score Until Now: 0.839 \n","\n","Patience 5\n","Epoch: 9/100 | Training Loss: 0.304 | Valid Score: 0.834\n"," \n","Epoch: 9/100 | Best Valid Score Until Now: 0.839 \n","\n","Patience 6\n","Epoch: 10/100 | Training Loss: 0.291 | Valid Score: 0.825\n"," \n","Epoch: 10/100 | Best Valid Score Until Now: 0.839 \n","\n","Save checkpoint\n","Epoch: 11/100 | Training Loss: 0.302 | Valid Score: 0.840\n"," \n","Epoch: 11/100 | Best Valid Score Until Now: 0.840 \n","\n","Patience 1\n","Epoch: 12/100 | Training Loss: 0.275 | Valid Score: 0.798\n"," \n","Epoch: 12/100 | Best Valid Score Until Now: 0.840 \n","\n","Patience 2\n","Epoch: 13/100 | Training Loss: 0.283 | Valid Score: 0.829\n"," \n","Epoch: 13/100 | Best Valid Score Until Now: 0.840 \n","\n","Save checkpoint\n","Epoch: 14/100 | Training Loss: 0.270 | Valid Score: 0.849\n"," \n","Epoch: 14/100 | Best Valid Score Until Now: 0.849 \n","\n","Save checkpoint\n","Epoch: 15/100 | Training Loss: 0.282 | Valid Score: 0.855\n"," \n","Epoch: 15/100 | Best Valid Score Until Now: 0.855 \n","\n","Patience 1\n","Epoch: 16/100 | Training Loss: 0.265 | Valid Score: 0.791\n"," \n","Epoch: 16/100 | Best Valid Score Until Now: 0.855 \n","\n","Patience 2\n","Epoch: 17/100 | Training Loss: 0.263 | Valid Score: 0.814\n"," \n","Epoch: 17/100 | Best Valid Score Until Now: 0.855 \n","\n","Patience 3\n","Epoch: 18/100 | Training Loss: 0.258 | Valid Score: 0.817\n"," \n","Epoch: 18/100 | Best Valid Score Until Now: 0.855 \n","\n","Patience 4\n","Epoch: 19/100 | Training Loss: 0.238 | Valid Score: 0.818\n"," \n","Epoch: 19/100 | Best Valid Score Until Now: 0.855 \n","\n","Patience 5\n","Epoch: 20/100 | Training Loss: 0.245 | Valid Score: 0.843\n"," \n","Epoch: 20/100 | Best Valid Score Until Now: 0.855 \n","\n","Patience 6\n","Epoch: 21/100 | Training Loss: 0.242 | Valid Score: 0.814\n"," \n","Epoch: 21/100 | Best Valid Score Until Now: 0.855 \n","\n","Patience 7\n","Epoch: 22/100 | Training Loss: 0.247 | Valid Score: 0.811\n"," \n","Epoch: 22/100 | Best Valid Score Until Now: 0.855 \n","\n","Patience 8\n","Epoch: 23/100 | Training Loss: 0.227 | Valid Score: 0.816\n"," \n","Epoch: 23/100 | Best Valid Score Until Now: 0.855 \n","\n","Patience 9\n","Epoch: 24/100 | Training Loss: 0.222 | Valid Score: 0.783\n"," \n","Epoch: 24/100 | Best Valid Score Until Now: 0.855 \n","\n","Patience 10\n","Epoch: 25/100 | Training Loss: 0.238 | Valid Score: 0.794\n"," \n","Epoch: 25/100 | Best Valid Score Until Now: 0.855 \n","\n","Final results:\n","Average Valid Score: 0.855 \n","\n","Test Score: 0.805 \n","\n","Execution time: 43.008 seconds\n"]}]},{"cell_type":"markdown","source":["#Graph SAGE 1"],"metadata":{"id":"qRQy86mUEv-6"}},{"cell_type":"code","source":[],"metadata":{"id":"xCYtrTMn0fSt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import dgl.function as fn\n","from dgl.nn import SAGEConv\n","\n","class GraphSAGE(nn.Module):\n","    def __init__(self, config, global_size = 200, num_tasks = 1):\n","        super().__init__()\n","        self.config = config\n","        self.num_tasks = num_tasks\n","\n","        # Node feature size\n","        self.node_feature_size = self.config.get('node_feature_size', 127)\n","\n","        # Edge feature size\n","        self.edge_feature_size = self.config.get('edge_feature_size', 12)\n","\n","        # Hidden size\n","        self.hidden_size = self.config.get('hidden_size', 100)\n","\n","        self.conv1 = SAGEConv(self.node_feature_size, self.hidden_size, aggregator_type='mean')\n","        self.conv2 = SAGEConv(self.hidden_size, self.num_tasks, aggregator_type='mean')\n","\n","    def forward(self, mol_dgl_graph, globals):\n","        mol_dgl_graph.ndata[\"v\"] = mol_dgl_graph.ndata[\"v\"][:,:self.node_feature_size]\n","        mol_dgl_graph.edata[\"e\"] = mol_dgl_graph.edata[\"e\"][:,:self.edge_feature_size]\n","        h = self.conv1(mol_dgl_graph, mol_dgl_graph.ndata[\"v\"])\n","        h = F.relu(h)\n","        h = self.conv2(mol_dgl_graph, h)\n","        mol_dgl_graph.ndata[\"h\"] = h\n","        return dgl.mean_nodes(mol_dgl_graph, \"h\")"],"metadata":{"id":"zyYZLW2i_If9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NKYpESFc8KIZ"},"source":["#### Function to Compute Score of the Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s03Vd_TM8KIa"},"outputs":[],"source":["from sklearn.metrics import roc_auc_score\n","\n","def compute_score(model, data_loader, val_size, num_tasks):\n","    model.eval()\n","    metric = roc_auc_score\n","    with torch.no_grad():\n","        prediction_all= torch.empty(0)\n","        labels_all= torch.empty(0)\n","        masks_all= torch.empty(0)\n","        for i, (mol_dgl_graph, labels, masks, globals) in enumerate(data_loader):\n","            prediction = model(mol_dgl_graph, globals)\n","            prediction = torch.sigmoid(prediction)\n","            prediction_all = torch.cat((prediction_all, prediction), 0)\n","            labels_all = torch.cat((labels_all, labels), 0)\n","            masks_all = torch.cat((masks_all, masks), 0)\n","        average = torch.tensor([0.])\n","        for i in range(num_tasks):\n","            a1 = prediction_all[:, i][masks_all[:,i]==1]\n","            a2 = labels_all[:, i][masks_all[:,i]==1]\n","            try:\n","                t = metric(a2.int().cpu(), a1.cpu()).item()\n","            except ValueError:\n","                t = 0\n","            average += t\n","    return average.item()/num_tasks"]},{"cell_type":"markdown","metadata":{"id":"Nedfu9gu8KIa"},"source":["#### Loss Function"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xKTUb9_F8KIa"},"outputs":[],"source":["def loss_func(output, label, mask, num_tasks):\n","    pos_weight = torch.ones((1, num_tasks))\n","    pos_weight\n","    criterion = torch.nn.BCEWithLogitsLoss(reduction='none', pos_weight=pos_weight)\n","    loss = mask*criterion(output,label)\n","    loss = loss.sum() / mask.sum()\n","    return loss"]},{"cell_type":"markdown","metadata":{"id":"qc2-Jczl8KIa"},"source":["#### Training and Evaluation"]},{"cell_type":"markdown","metadata":{"id":"NGmlCV658KIb"},"source":["##### Training Function"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Yb6T7RDr8KIb"},"outputs":[],"source":["def train_epoch(train_dataloader, model, optimizer):\n","    epoch_train_loss = 0\n","    iterations = 0\n","    model.train() # Prepare model for training\n","    for i, (mol_dgl_graph, labels, masks, globals) in enumerate(train_dataloader):\n","        prediction = model(mol_dgl_graph, globals)\n","        loss_train = loss_func(prediction, labels, masks, num_tasks)\n","        optimizer.zero_grad(set_to_none=True)\n","        loss_train.backward()\n","        optimizer.step()\n","        epoch_train_loss += loss_train.detach().item()\n","        iterations += 1\n","    epoch_train_loss /= iterations\n","    return epoch_train_loss"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_UktmkAs8KIb"},"outputs":[],"source":["def train_evaluate():\n","\n","    model = GraphSAGE(config, global_size, num_tasks)\n","    optimizer = torch.optim.Adam(model.parameters(), lr = 0.001)\n","\n","    best_val = 0\n","    patience_count = 1\n","    epoch = 1\n","\n","    while epoch <= num_epochs:\n","        if patience_count <= patience:\n","            model.train()\n","            loss_train = train_epoch(train_dataloader, model, optimizer)\n","            model.eval()\n","            score_val = compute_score(model, val_dataloader, len(val_set), num_tasks)\n","            if score_val > best_val:\n","                best_val = score_val\n","                print(\"Save checkpoint\")\n","                path = os.path.join(checkpoint_path, 'checkpoint.pth')\n","                dict_checkpoint = {\"score_val\": score_val}\n","                dict_checkpoint.update({\"model_state_dict\": model.state_dict(), \"optimizer_state\": optimizer.state_dict()})\n","                with open(path, \"wb\") as outputfile:\n","                    cloudpickle.dump(dict_checkpoint, outputfile)\n","                patience_count = 1\n","            else:\n","                print(\"Patience\", patience_count)\n","                patience_count += 1\n","\n","            print(\"Epoch: {}/{} | Training Loss: {:.3f} | Valid Score: {:.3f}\".format(\n","            epoch, num_epochs, loss_train, score_val))\n","\n","            print(\" \")\n","            print(\"Epoch: {}/{} | Best Valid Score Until Now: {:.3f}\".format(epoch, num_epochs, best_val), \"\\n\")\n","        epoch += 1\n","\n","    # best model save\n","    shutil.rmtree(best_model_path, ignore_errors=True)\n","    shutil.copytree(checkpoint_path, best_model_path)\n","\n","    print(\"Final results:\")\n","    print(\"Average Valid Score: {:.3f}\".format(np.mean(best_val)), \"\\n\")\n"]},{"cell_type":"markdown","metadata":{"id":"wNC4PWvX8KIc"},"source":["##### Function to compute test set score of the final saved model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iHU7Hluq8KIc"},"outputs":[],"source":["def test_evaluate():\n","    final_model = GraphSAGE(config, global_size, num_tasks)\n","    path = os.path.join(best_model_path, 'checkpoint.pth')\n","    with open(path, 'rb') as f:\n","        checkpoint = cloudpickle.load(f)\n","    final_model.load_state_dict(checkpoint[\"model_state_dict\"])\n","    final_model.eval()\n","    test_score = compute_score(final_model, test_dataloader, len(test_set), num_tasks)\n","\n","    print(\"Test Score: {:.3f}\".format(test_score), \"\\n\")\n","    print(\"Execution time: {:.3f} seconds\".format(time.time() - start_time))\n"]},{"cell_type":"markdown","metadata":{"id":"kkfNctR98KIc"},"source":["##### Train the model and evaluate its performance"]},{"cell_type":"code","source":["import time\n","start_time = time.time()\n","\n","train_evaluate()\n","test_evaluate()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ppiu3PZh8RUv","executionInfo":{"status":"ok","timestamp":1688241637627,"user_tz":-210,"elapsed":10365,"user":{"displayName":"Mahdi Salehi","userId":"13663744920024374425"}},"outputId":"b2d8d756-d3bb-4449-9ab7-835b5723275d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Save checkpoint\n","Epoch: 1/100 | Training Loss: 0.586 | Valid Score: 0.780\n"," \n","Epoch: 1/100 | Best Valid Score Until Now: 0.780 \n","\n","Save checkpoint\n","Epoch: 2/100 | Training Loss: 0.469 | Valid Score: 0.835\n"," \n","Epoch: 2/100 | Best Valid Score Until Now: 0.835 \n","\n","Save checkpoint\n","Epoch: 3/100 | Training Loss: 0.437 | Valid Score: 0.842\n"," \n","Epoch: 3/100 | Best Valid Score Until Now: 0.842 \n","\n","Patience 1\n","Epoch: 4/100 | Training Loss: 0.420 | Valid Score: 0.835\n"," \n","Epoch: 4/100 | Best Valid Score Until Now: 0.842 \n","\n","Patience 2\n","Epoch: 5/100 | Training Loss: 0.422 | Valid Score: 0.831\n"," \n","Epoch: 5/100 | Best Valid Score Until Now: 0.842 \n","\n","Patience 3\n","Epoch: 6/100 | Training Loss: 0.401 | Valid Score: 0.831\n"," \n","Epoch: 6/100 | Best Valid Score Until Now: 0.842 \n","\n","Patience 4\n","Epoch: 7/100 | Training Loss: 0.394 | Valid Score: 0.836\n"," \n","Epoch: 7/100 | Best Valid Score Until Now: 0.842 \n","\n","Patience 5\n","Epoch: 8/100 | Training Loss: 0.389 | Valid Score: 0.826\n"," \n","Epoch: 8/100 | Best Valid Score Until Now: 0.842 \n","\n","Patience 6\n","Epoch: 9/100 | Training Loss: 0.384 | Valid Score: 0.829\n"," \n","Epoch: 9/100 | Best Valid Score Until Now: 0.842 \n","\n","Patience 7\n","Epoch: 10/100 | Training Loss: 0.381 | Valid Score: 0.829\n"," \n","Epoch: 10/100 | Best Valid Score Until Now: 0.842 \n","\n","Patience 8\n","Epoch: 11/100 | Training Loss: 0.377 | Valid Score: 0.827\n"," \n","Epoch: 11/100 | Best Valid Score Until Now: 0.842 \n","\n","Patience 9\n","Epoch: 12/100 | Training Loss: 0.376 | Valid Score: 0.827\n"," \n","Epoch: 12/100 | Best Valid Score Until Now: 0.842 \n","\n","Patience 10\n","Epoch: 13/100 | Training Loss: 0.369 | Valid Score: 0.821\n"," \n","Epoch: 13/100 | Best Valid Score Until Now: 0.842 \n","\n","Final results:\n","Average Valid Score: 0.842 \n","\n","Test Score: 0.636 \n","\n","Execution time: 10.487 seconds\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"hqjDaWbb_9uI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Graph SAGE 2\n"],"metadata":{"id":"-26bxpYdHJXt"}},{"cell_type":"code","source":["import dgl.function as fn\n","from dgl.nn import SAGEConv\n","\n","class GraphSAGE2(nn.Module):\n","    def __init__(self, config, global_size = 200, num_tasks = 1):\n","        super().__init__()\n","        self.config = config\n","        self.num_tasks = num_tasks\n","\n","        # Node feature size\n","        self.node_feature_size = self.config.get('node_feature_size', 127)\n","\n","        # Edge feature size\n","        self.edge_feature_size = self.config.get('edge_feature_size', 12)\n","\n","        # Hidden size\n","        self.hidden_size = self.config.get('hidden_size', 100)\n","\n","        self.conv1 = SAGEConv(self.node_feature_size, self.hidden_size, aggregator_type='mean')\n","        self.conv2 = SAGEConv(self.hidden_size, self.hidden_size , aggregator_type='mean')\n","        self.conv3 = SAGEConv(self.hidden_size, self.num_tasks, aggregator_type='mean')\n","\n","    def forward(self, mol_dgl_graph, globals):\n","        mol_dgl_graph.ndata[\"v\"] = mol_dgl_graph.ndata[\"v\"][:,:self.node_feature_size]\n","        mol_dgl_graph.edata[\"e\"] = mol_dgl_graph.edata[\"e\"][:,:self.edge_feature_size]\n","        h = self.conv1(mol_dgl_graph, mol_dgl_graph.ndata[\"v\"])\n","        h = F.relu(h)\n","        h = self.conv2(mol_dgl_graph, h)\n","        h = F.relu(h)\n","        h = self.conv3(mol_dgl_graph, h)\n","        mol_dgl_graph.ndata[\"h\"] = h\n","        return dgl.mean_nodes(mol_dgl_graph, \"h\")"],"metadata":{"id":"z6FLAUGTDobB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xtUSe9WYHJXu"},"source":["#### Function to Compute Score of the Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_ukyOBBjHJXu"},"outputs":[],"source":["from sklearn.metrics import roc_auc_score\n","\n","def compute_score(model, data_loader, val_size, num_tasks):\n","    model.eval()\n","    metric = roc_auc_score\n","    with torch.no_grad():\n","        prediction_all= torch.empty(0)\n","        labels_all= torch.empty(0)\n","        masks_all= torch.empty(0)\n","        for i, (mol_dgl_graph, labels, masks, globals) in enumerate(data_loader):\n","            prediction = model(mol_dgl_graph, globals)\n","            prediction = torch.sigmoid(prediction)\n","            prediction_all = torch.cat((prediction_all, prediction), 0)\n","            labels_all = torch.cat((labels_all, labels), 0)\n","            masks_all = torch.cat((masks_all, masks), 0)\n","        average = torch.tensor([0.])\n","        for i in range(num_tasks):\n","            a1 = prediction_all[:, i][masks_all[:,i]==1]\n","            a2 = labels_all[:, i][masks_all[:,i]==1]\n","            try:\n","                t = metric(a2.int().cpu(), a1.cpu()).item()\n","            except ValueError:\n","                t = 0\n","            average += t\n","    return average.item()/num_tasks"]},{"cell_type":"markdown","metadata":{"id":"GHIG7fG3HJXu"},"source":["#### Loss Function"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8ylwVjJtHJXv"},"outputs":[],"source":["def loss_func(output, label, mask, num_tasks):\n","    pos_weight = torch.ones((1, num_tasks))\n","    pos_weight\n","    criterion = torch.nn.BCEWithLogitsLoss(reduction='none', pos_weight=pos_weight)\n","    loss = mask*criterion(output,label)\n","    loss = loss.sum() / mask.sum()\n","    return loss"]},{"cell_type":"markdown","metadata":{"id":"W3RJX27HHJXv"},"source":["#### Training and Evaluation"]},{"cell_type":"markdown","metadata":{"id":"i3qa2TiSHJXv"},"source":["##### Training Function"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QoCbKr5ZHJXv"},"outputs":[],"source":["def train_epoch(train_dataloader, model, optimizer):\n","    epoch_train_loss = 0\n","    iterations = 0\n","    model.train() # Prepare model for training\n","    for i, (mol_dgl_graph, labels, masks, globals) in enumerate(train_dataloader):\n","        prediction = model(mol_dgl_graph, globals)\n","        loss_train = loss_func(prediction, labels, masks, num_tasks)\n","        optimizer.zero_grad(set_to_none=True)\n","        loss_train.backward()\n","        optimizer.step()\n","        epoch_train_loss += loss_train.detach().item()\n","        iterations += 1\n","    epoch_train_loss /= iterations\n","    return epoch_train_loss"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MrwQc9MzHJXv"},"outputs":[],"source":["def train_evaluate():\n","\n","    model = GraphSAGE2(config, global_size, num_tasks)\n","    optimizer = torch.optim.Adam(model.parameters(), lr = 0.001)\n","\n","    best_val = 0\n","    patience_count = 1\n","    epoch = 1\n","\n","    while epoch <= num_epochs:\n","        if patience_count <= patience:\n","            model.train()\n","            loss_train = train_epoch(train_dataloader, model, optimizer)\n","            model.eval()\n","            score_val = compute_score(model, val_dataloader, len(val_set), num_tasks)\n","            if score_val > best_val:\n","                best_val = score_val\n","                print(\"Save checkpoint\")\n","                path = os.path.join(checkpoint_path, 'checkpoint.pth')\n","                dict_checkpoint = {\"score_val\": score_val}\n","                dict_checkpoint.update({\"model_state_dict\": model.state_dict(), \"optimizer_state\": optimizer.state_dict()})\n","                with open(path, \"wb\") as outputfile:\n","                    cloudpickle.dump(dict_checkpoint, outputfile)\n","                patience_count = 1\n","            else:\n","                print(\"Patience\", patience_count)\n","                patience_count += 1\n","\n","            print(\"Epoch: {}/{} | Training Loss: {:.3f} | Valid Score: {:.3f}\".format(\n","            epoch, num_epochs, loss_train, score_val))\n","\n","            print(\" \")\n","            print(\"Epoch: {}/{} | Best Valid Score Until Now: {:.3f}\".format(epoch, num_epochs, best_val), \"\\n\")\n","        epoch += 1\n","\n","    # best model save\n","    shutil.rmtree(best_model_path, ignore_errors=True)\n","    shutil.copytree(checkpoint_path, best_model_path)\n","\n","    print(\"Final results:\")\n","    print(\"Average Valid Score: {:.3f}\".format(np.mean(best_val)), \"\\n\")\n"]},{"cell_type":"markdown","metadata":{"id":"juyL5pwfHJXv"},"source":["##### Function to compute test set score of the final saved model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"U3AGERx5HJXw"},"outputs":[],"source":["def test_evaluate():\n","    final_model = GraphSAGE2(config, global_size, num_tasks)\n","    path = os.path.join(best_model_path, 'checkpoint.pth')\n","    with open(path, 'rb') as f:\n","        checkpoint = cloudpickle.load(f)\n","    final_model.load_state_dict(checkpoint[\"model_state_dict\"])\n","    final_model.eval()\n","    test_score = compute_score(final_model, test_dataloader, len(test_set), num_tasks)\n","\n","    print(\"Test Score: {:.3f}\".format(test_score), \"\\n\")\n","    print(\"Execution time: {:.3f} seconds\".format(time.time() - start_time))\n"]},{"cell_type":"markdown","metadata":{"id":"vE1suzMJHJXw"},"source":["##### Train the model and evaluate its performance"]},{"cell_type":"code","source":["import time\n","start_time = time.time()\n","\n","train_evaluate()\n","test_evaluate()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1688243572047,"user_tz":-210,"elapsed":31227,"user":{"displayName":"Mahdi Salehi","userId":"13663744920024374425"}},"outputId":"f6bc4200-ea0e-4aac-f305-884566a503ca","id":"EDB_P7pDHJXw"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Save checkpoint\n","Epoch: 1/100 | Training Loss: 0.513 | Valid Score: 0.774\n"," \n","Epoch: 1/100 | Best Valid Score Until Now: 0.774 \n","\n","Save checkpoint\n","Epoch: 2/100 | Training Loss: 0.422 | Valid Score: 0.802\n"," \n","Epoch: 2/100 | Best Valid Score Until Now: 0.802 \n","\n","Save checkpoint\n","Epoch: 3/100 | Training Loss: 0.405 | Valid Score: 0.814\n"," \n","Epoch: 3/100 | Best Valid Score Until Now: 0.814 \n","\n","Patience 1\n","Epoch: 4/100 | Training Loss: 0.389 | Valid Score: 0.799\n"," \n","Epoch: 4/100 | Best Valid Score Until Now: 0.814 \n","\n","Save checkpoint\n","Epoch: 5/100 | Training Loss: 0.376 | Valid Score: 0.823\n"," \n","Epoch: 5/100 | Best Valid Score Until Now: 0.823 \n","\n","Patience 1\n","Epoch: 6/100 | Training Loss: 0.363 | Valid Score: 0.813\n"," \n","Epoch: 6/100 | Best Valid Score Until Now: 0.823 \n","\n","Patience 2\n","Epoch: 7/100 | Training Loss: 0.351 | Valid Score: 0.822\n"," \n","Epoch: 7/100 | Best Valid Score Until Now: 0.823 \n","\n","Patience 3\n","Epoch: 8/100 | Training Loss: 0.359 | Valid Score: 0.817\n"," \n","Epoch: 8/100 | Best Valid Score Until Now: 0.823 \n","\n","Patience 4\n","Epoch: 9/100 | Training Loss: 0.361 | Valid Score: 0.794\n"," \n","Epoch: 9/100 | Best Valid Score Until Now: 0.823 \n","\n","Save checkpoint\n","Epoch: 10/100 | Training Loss: 0.343 | Valid Score: 0.828\n"," \n","Epoch: 10/100 | Best Valid Score Until Now: 0.828 \n","\n","Patience 1\n","Epoch: 11/100 | Training Loss: 0.337 | Valid Score: 0.820\n"," \n","Epoch: 11/100 | Best Valid Score Until Now: 0.828 \n","\n","Patience 2\n","Epoch: 12/100 | Training Loss: 0.332 | Valid Score: 0.822\n"," \n","Epoch: 12/100 | Best Valid Score Until Now: 0.828 \n","\n","Patience 3\n","Epoch: 13/100 | Training Loss: 0.324 | Valid Score: 0.828\n"," \n","Epoch: 13/100 | Best Valid Score Until Now: 0.828 \n","\n","Patience 4\n","Epoch: 14/100 | Training Loss: 0.322 | Valid Score: 0.828\n"," \n","Epoch: 14/100 | Best Valid Score Until Now: 0.828 \n","\n","Patience 5\n","Epoch: 15/100 | Training Loss: 0.357 | Valid Score: 0.780\n"," \n","Epoch: 15/100 | Best Valid Score Until Now: 0.828 \n","\n","Patience 6\n","Epoch: 16/100 | Training Loss: 0.325 | Valid Score: 0.818\n"," \n","Epoch: 16/100 | Best Valid Score Until Now: 0.828 \n","\n","Save checkpoint\n","Epoch: 17/100 | Training Loss: 0.310 | Valid Score: 0.839\n"," \n","Epoch: 17/100 | Best Valid Score Until Now: 0.839 \n","\n","Patience 1\n","Epoch: 18/100 | Training Loss: 0.297 | Valid Score: 0.818\n"," \n","Epoch: 18/100 | Best Valid Score Until Now: 0.839 \n","\n","Patience 2\n","Epoch: 19/100 | Training Loss: 0.296 | Valid Score: 0.778\n"," \n","Epoch: 19/100 | Best Valid Score Until Now: 0.839 \n","\n","Save checkpoint\n","Epoch: 20/100 | Training Loss: 0.296 | Valid Score: 0.842\n"," \n","Epoch: 20/100 | Best Valid Score Until Now: 0.842 \n","\n","Save checkpoint\n","Epoch: 21/100 | Training Loss: 0.295 | Valid Score: 0.846\n"," \n","Epoch: 21/100 | Best Valid Score Until Now: 0.846 \n","\n","Patience 1\n","Epoch: 22/100 | Training Loss: 0.292 | Valid Score: 0.841\n"," \n","Epoch: 22/100 | Best Valid Score Until Now: 0.846 \n","\n","Patience 2\n","Epoch: 23/100 | Training Loss: 0.300 | Valid Score: 0.832\n"," \n","Epoch: 23/100 | Best Valid Score Until Now: 0.846 \n","\n","Patience 3\n","Epoch: 24/100 | Training Loss: 0.277 | Valid Score: 0.831\n"," \n","Epoch: 24/100 | Best Valid Score Until Now: 0.846 \n","\n","Patience 4\n","Epoch: 25/100 | Training Loss: 0.271 | Valid Score: 0.832\n"," \n","Epoch: 25/100 | Best Valid Score Until Now: 0.846 \n","\n","Patience 5\n","Epoch: 26/100 | Training Loss: 0.273 | Valid Score: 0.817\n"," \n","Epoch: 26/100 | Best Valid Score Until Now: 0.846 \n","\n","Patience 6\n","Epoch: 27/100 | Training Loss: 0.269 | Valid Score: 0.831\n"," \n","Epoch: 27/100 | Best Valid Score Until Now: 0.846 \n","\n","Patience 7\n","Epoch: 28/100 | Training Loss: 0.258 | Valid Score: 0.840\n"," \n","Epoch: 28/100 | Best Valid Score Until Now: 0.846 \n","\n","Patience 8\n","Epoch: 29/100 | Training Loss: 0.258 | Valid Score: 0.835\n"," \n","Epoch: 29/100 | Best Valid Score Until Now: 0.846 \n","\n","Patience 9\n","Epoch: 30/100 | Training Loss: 0.251 | Valid Score: 0.828\n"," \n","Epoch: 30/100 | Best Valid Score Until Now: 0.846 \n","\n","Patience 10\n","Epoch: 31/100 | Training Loss: 0.252 | Valid Score: 0.827\n"," \n","Epoch: 31/100 | Best Valid Score Until Now: 0.846 \n","\n","Final results:\n","Average Valid Score: 0.846 \n","\n","Test Score: 0.792 \n","\n","Execution time: 31.166 seconds\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"7C1uneejJqFJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Graph SAGE 3\n"],"metadata":{"id":"mN9WFnZkJqSQ"}},{"cell_type":"code","source":["from dgl.nn.pytorch.conv import SAGEConv\n","class GraphSAGE3(nn.Module):\n","    def __init__(self, config, global_size = 200, num_tasks = 1):\n","        super().__init__()\n","        self.config = config\n","        self.num_tasks = num_tasks\n","\n","        # Node feature size\n","        self.node_feature_size = self.config.get('node_feature_size', 127)\n","\n","        # Edge feature size\n","        self.edge_feature_size = self.config.get('edge_feature_size', 12)\n","\n","        # Hidden size\n","        self.hidden_size = self.config.get('hidden_size', 100)\n","\n","        self.conv1 = SAGEConv(self.node_feature_size, 64 ,aggregator_type='lstm')\n","        self.conv2 = SAGEConv(64, 128,aggregator_type='lstm')\n","        self.conv3 = SAGEConv(128, 256,aggregator_type='lstm')\n","        self.conv4 = SAGEConv(256, 128,aggregator_type='lstm')\n","        self.conv5 = SAGEConv(128, self.num_tasks,aggregator_type='lstm')\n","\n","        self.dropout = nn.Dropout(p=0.2)\n","        self.bn1 = nn.BatchNorm1d(64)\n","        self.bn2 = nn.BatchNorm1d(128)\n","        self.bn3 = nn.BatchNorm1d(256)\n","\n","    # def forward(self, g, in_feat):\n","    def forward(self, mol_dgl_graph, globals):\n","        mol_dgl_graph.ndata[\"v\"]= mol_dgl_graph.ndata[\"v\"][:,:self.node_feature_size]\n","        mol_dgl_graph.edata[\"e\"] = mol_dgl_graph.edata[\"e\"][:,:self.edge_feature_size]\n","        h = self.conv1(mol_dgl_graph, mol_dgl_graph.ndata[\"v\"])\n","        h = F.relu(h)\n","        h = self.bn1(h)\n","        h = self.conv2(mol_dgl_graph, h)\n","        h = F.relu(h)\n","        h = self.dropout(h)\n","        h = self.bn2(h)\n","        h = self.conv3(mol_dgl_graph, h)\n","        h = F.relu(h)\n","        h = self.dropout(h)\n","        h = self.bn3(h)\n","        h = self.conv4(mol_dgl_graph, h)\n","        h = F.relu(h)\n","        h = self.bn2(h)\n","        h = self.conv5(mol_dgl_graph, h)\n","        mol_dgl_graph.ndata[\"h\"] = h\n","        return dgl.mean_nodes(mol_dgl_graph, \"h\")"],"metadata":{"id":"hoXL7x0nJqSQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rAdLky2AJqSQ"},"source":["#### Function to Compute Score of the Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kPRObsY8JqSQ"},"outputs":[],"source":["from sklearn.metrics import roc_auc_score\n","\n","def compute_score(model, data_loader, val_size, num_tasks):\n","    model.eval()\n","    metric = roc_auc_score\n","    with torch.no_grad():\n","        prediction_all= torch.empty(0)\n","        labels_all= torch.empty(0)\n","        masks_all= torch.empty(0)\n","        for i, (mol_dgl_graph, labels, masks, globals) in enumerate(data_loader):\n","            prediction = model(mol_dgl_graph, globals)\n","            prediction = torch.sigmoid(prediction)\n","            prediction_all = torch.cat((prediction_all, prediction), 0)\n","            labels_all = torch.cat((labels_all, labels), 0)\n","            masks_all = torch.cat((masks_all, masks), 0)\n","        average = torch.tensor([0.])\n","        for i in range(num_tasks):\n","            a1 = prediction_all[:, i][masks_all[:,i]==1]\n","            a2 = labels_all[:, i][masks_all[:,i]==1]\n","            try:\n","                t = metric(a2.int().cpu(), a1.cpu()).item()\n","            except ValueError:\n","                t = 0\n","            average += t\n","    return average.item()/num_tasks"]},{"cell_type":"markdown","metadata":{"id":"83NnpyKTJqSQ"},"source":["#### Loss Function"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"21dboGDZJqSR"},"outputs":[],"source":["def loss_func(output, label, mask, num_tasks):\n","    pos_weight = torch.ones((1, num_tasks))\n","    pos_weight\n","    criterion = torch.nn.BCEWithLogitsLoss(reduction='none', pos_weight=pos_weight)\n","    loss = mask*criterion(output,label)\n","    loss = loss.sum() / mask.sum()\n","    return loss"]},{"cell_type":"markdown","metadata":{"id":"D_uu7FwMJqSR"},"source":["#### Training and Evaluation"]},{"cell_type":"markdown","metadata":{"id":"gFdN2C46JqSR"},"source":["##### Training Function"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qG-RozYSJqSR"},"outputs":[],"source":["def train_epoch(train_dataloader, model, optimizer):\n","    epoch_train_loss = 0\n","    iterations = 0\n","    model.train() # Prepare model for training\n","    for i, (mol_dgl_graph, labels, masks, globals) in enumerate(train_dataloader):\n","        prediction = model(mol_dgl_graph, globals)\n","        loss_train = loss_func(prediction, labels, masks, num_tasks)\n","        optimizer.zero_grad(set_to_none=True)\n","        loss_train.backward()\n","        optimizer.step()\n","        epoch_train_loss += loss_train.detach().item()\n","        iterations += 1\n","    epoch_train_loss /= iterations\n","    return epoch_train_loss"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Bheko1y7JqSR"},"outputs":[],"source":["def train_evaluate():\n","\n","    model = GraphSAGE3(config, global_size, num_tasks)\n","    optimizer = torch.optim.Adam(model.parameters(), lr = 0.001)\n","\n","    best_val = 0\n","    patience_count = 1\n","    epoch = 1\n","\n","    while epoch <= num_epochs:\n","        if patience_count <= patience:\n","            model.train()\n","            loss_train = train_epoch(train_dataloader, model, optimizer)\n","            model.eval()\n","            score_val = compute_score(model, val_dataloader, len(val_set), num_tasks)\n","            if score_val > best_val:\n","                best_val = score_val\n","                print(\"Save checkpoint\")\n","                path = os.path.join(checkpoint_path, 'checkpoint.pth')\n","                dict_checkpoint = {\"score_val\": score_val}\n","                dict_checkpoint.update({\"model_state_dict\": model.state_dict(), \"optimizer_state\": optimizer.state_dict()})\n","                with open(path, \"wb\") as outputfile:\n","                    cloudpickle.dump(dict_checkpoint, outputfile)\n","                patience_count = 1\n","            else:\n","                print(\"Patience\", patience_count)\n","                patience_count += 1\n","\n","            print(\"Epoch: {}/{} | Training Loss: {:.3f} | Valid Score: {:.3f}\".format(\n","            epoch, num_epochs, loss_train, score_val))\n","\n","            print(\" \")\n","            print(\"Epoch: {}/{} | Best Valid Score Until Now: {:.3f}\".format(epoch, num_epochs, best_val), \"\\n\")\n","        epoch += 1\n","\n","    # best model save\n","    shutil.rmtree(best_model_path, ignore_errors=True)\n","    shutil.copytree(checkpoint_path, best_model_path)\n","\n","    print(\"Final results:\")\n","    print(\"Average Valid Score: {:.3f}\".format(np.mean(best_val)), \"\\n\")\n"]},{"cell_type":"markdown","metadata":{"id":"pJHyDijFJqSR"},"source":["##### Function to compute test set score of the final saved model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AmByayL2JqSR"},"outputs":[],"source":["def test_evaluate():\n","    final_model = GraphSAGE3(config, global_size, num_tasks)\n","    path = os.path.join(best_model_path, 'checkpoint.pth')\n","    with open(path, 'rb') as f:\n","        checkpoint = cloudpickle.load(f)\n","    final_model.load_state_dict(checkpoint[\"model_state_dict\"])\n","    final_model.eval()\n","    test_score = compute_score(final_model, test_dataloader, len(test_set), num_tasks)\n","\n","    print(\"Test Score: {:.3f}\".format(test_score), \"\\n\")\n","    print(\"Execution time: {:.3f} seconds\".format(time.time() - start_time))\n"]},{"cell_type":"markdown","metadata":{"id":"L4TSy9SbJqSS"},"source":["##### Train the model and evaluate its performance"]},{"cell_type":"code","source":["import time\n","start_time = time.time()\n","\n","train_evaluate()\n","test_evaluate()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1688244710387,"user_tz":-210,"elapsed":152897,"user":{"displayName":"Mahdi Salehi","userId":"13663744920024374425"}},"outputId":"2ef1fe5a-c2d7-4874-eea7-5bec593d6de7","id":"uI4lxjgNJqSS"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Save checkpoint\n","Epoch: 1/100 | Training Loss: 0.480 | Valid Score: 0.815\n"," \n","Epoch: 1/100 | Best Valid Score Until Now: 0.815 \n","\n","Save checkpoint\n","Epoch: 2/100 | Training Loss: 0.372 | Valid Score: 0.824\n"," \n","Epoch: 2/100 | Best Valid Score Until Now: 0.824 \n","\n","Save checkpoint\n","Epoch: 3/100 | Training Loss: 0.338 | Valid Score: 0.854\n"," \n","Epoch: 3/100 | Best Valid Score Until Now: 0.854 \n","\n","Patience 1\n","Epoch: 4/100 | Training Loss: 0.312 | Valid Score: 0.841\n"," \n","Epoch: 4/100 | Best Valid Score Until Now: 0.854 \n","\n","Save checkpoint\n","Epoch: 5/100 | Training Loss: 0.291 | Valid Score: 0.855\n"," \n","Epoch: 5/100 | Best Valid Score Until Now: 0.855 \n","\n","Patience 1\n","Epoch: 6/100 | Training Loss: 0.283 | Valid Score: 0.841\n"," \n","Epoch: 6/100 | Best Valid Score Until Now: 0.855 \n","\n","Patience 2\n","Epoch: 7/100 | Training Loss: 0.254 | Valid Score: 0.846\n"," \n","Epoch: 7/100 | Best Valid Score Until Now: 0.855 \n","\n","Save checkpoint\n","Epoch: 8/100 | Training Loss: 0.238 | Valid Score: 0.864\n"," \n","Epoch: 8/100 | Best Valid Score Until Now: 0.864 \n","\n","Patience 1\n","Epoch: 9/100 | Training Loss: 0.224 | Valid Score: 0.851\n"," \n","Epoch: 9/100 | Best Valid Score Until Now: 0.864 \n","\n","Patience 2\n","Epoch: 10/100 | Training Loss: 0.225 | Valid Score: 0.854\n"," \n","Epoch: 10/100 | Best Valid Score Until Now: 0.864 \n","\n","Patience 3\n","Epoch: 11/100 | Training Loss: 0.202 | Valid Score: 0.851\n"," \n","Epoch: 11/100 | Best Valid Score Until Now: 0.864 \n","\n","Patience 4\n","Epoch: 12/100 | Training Loss: 0.193 | Valid Score: 0.832\n"," \n","Epoch: 12/100 | Best Valid Score Until Now: 0.864 \n","\n","Patience 5\n","Epoch: 13/100 | Training Loss: 0.177 | Valid Score: 0.830\n"," \n","Epoch: 13/100 | Best Valid Score Until Now: 0.864 \n","\n","Patience 6\n","Epoch: 14/100 | Training Loss: 0.151 | Valid Score: 0.837\n"," \n","Epoch: 14/100 | Best Valid Score Until Now: 0.864 \n","\n","Patience 7\n","Epoch: 15/100 | Training Loss: 0.149 | Valid Score: 0.851\n"," \n","Epoch: 15/100 | Best Valid Score Until Now: 0.864 \n","\n","Patience 8\n","Epoch: 16/100 | Training Loss: 0.149 | Valid Score: 0.861\n"," \n","Epoch: 16/100 | Best Valid Score Until Now: 0.864 \n","\n","Patience 9\n","Epoch: 17/100 | Training Loss: 0.126 | Valid Score: 0.811\n"," \n","Epoch: 17/100 | Best Valid Score Until Now: 0.864 \n","\n","Patience 10\n","Epoch: 18/100 | Training Loss: 0.112 | Valid Score: 0.842\n"," \n","Epoch: 18/100 | Best Valid Score Until Now: 0.864 \n","\n","Final results:\n","Average Valid Score: 0.864 \n","\n","Test Score: 0.902 \n","\n","Execution time: 153.130 seconds\n"]}]},{"cell_type":"markdown","source":["# GAT 1"],"metadata":{"id":"EarbDqzrnKMA"}},{"cell_type":"code","source":[],"metadata":{"id":"8YTn5lvTRfBT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from dgl.nn.pytorch import GATConv\n","class GATConv1(nn.Module):\n","    def __init__(self, config, global_size=200, num_tasks=1):\n","        super().__init__()\n","        self.config = config\n","        self.num_tasks = num_tasks\n","\n","        # Node feature size\n","        self.node_feature_size = self.config.get('node_feature_size', 127)\n","\n","        # Edge feature size\n","        self.edge_feature_size = self.config.get('edge_feature_size', 12)\n","\n","        # Hidden size\n","        self.hidden_size = self.config.get('hidden_size', 100)\n","\n","        # Number of attention heads\n","        self.num_heads = self.config.get('num_heads', 1)\n","\n","        # Dropout probability\n","        self.dropout = self.config.get('dropout', 0.0)\n","\n","        # GAT layer\n","        self.conv1 = GATConv(\n","            self.node_feature_size,\n","            self.hidden_size,\n","            num_heads=self.num_heads,\n","            feat_drop=self.dropout,\n","            attn_drop=self.dropout,allow_zero_in_degree=True\n","        )\n","\n","        # Linear layer\n","        self.fc = nn.Linear(\n","            self.hidden_size * self.num_heads,\n","            self.hidden_size\n","        )\n","\n","        # GAT layer\n","        self.conv2 = GATConv(\n","            self.hidden_size,\n","            self.num_tasks,\n","            num_heads=1,\n","            feat_drop=self.dropout,\n","            attn_drop=self.dropout,allow_zero_in_degree=True\n","        )\n","\n","    def forward(self, mol_dgl_graph, globals):\n","        mol_dgl_graph.ndata[\"v\"] = mol_dgl_graph.ndata[\"v\"][:, :self.node_feature_size]\n","        mol_dgl_graph.edata[\"e\"] = mol_dgl_graph.edata[\"e\"][:, :self.edge_feature_size]\n","\n","        # First GAT layer\n","        h = self.conv1(mol_dgl_graph, mol_dgl_graph.ndata[\"v\"]).flatten(1)\n","        h = F.relu(h)\n","        h = self.fc(h)\n","        h = F.dropout(h, p=self.dropout, training=self.training)\n","\n","        # Second GAT layer\n","        h = self.conv2(mol_dgl_graph, h).squeeze(1)\n","        mol_dgl_graph.ndata[\"h\"] = h\n","\n","        return dgl.mean_nodes(mol_dgl_graph, \"h\")"],"metadata":{"id":"mHupBXIeDiuF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"j59rw7ziDeq5"},"source":["#### Function to Compute Score of the Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eFKUOfWpDeq6"},"outputs":[],"source":["from sklearn.metrics import roc_auc_score\n","\n","def compute_score(model, data_loader, val_size, num_tasks):\n","    model.eval()\n","    metric = roc_auc_score\n","    with torch.no_grad():\n","        prediction_all= torch.empty(0)\n","        labels_all= torch.empty(0)\n","        masks_all= torch.empty(0)\n","        for i, (mol_dgl_graph, labels, masks, globals) in enumerate(data_loader):\n","            prediction = model(mol_dgl_graph, globals)\n","            prediction = torch.sigmoid(prediction)\n","            prediction_all = torch.cat((prediction_all, prediction), 0)\n","            labels_all = torch.cat((labels_all, labels), 0)\n","            masks_all = torch.cat((masks_all, masks), 0)\n","        average = torch.tensor([0.])\n","        for i in range(num_tasks):\n","            a1 = prediction_all[:, i][masks_all[:,i]==1]\n","            a2 = labels_all[:, i][masks_all[:,i]==1]\n","            try:\n","                t = metric(a2.int().cpu(), a1.cpu()).item()\n","            except ValueError:\n","                t = 0\n","            average += t\n","    return average.item()/num_tasks"]},{"cell_type":"markdown","metadata":{"id":"KfAkKfnoDeq6"},"source":["#### Loss Function"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"R4LMkTl6Deq6"},"outputs":[],"source":["def loss_func(output, label, mask, num_tasks):\n","    pos_weight = torch.ones((1, num_tasks))\n","    pos_weight\n","    criterion = torch.nn.BCEWithLogitsLoss(reduction='none', pos_weight=pos_weight)\n","    loss = mask*criterion(output,label)\n","    loss = loss.sum() / mask.sum()\n","    return loss"]},{"cell_type":"markdown","metadata":{"id":"FsjeDCKsDeq6"},"source":["#### Training and Evaluation"]},{"cell_type":"markdown","metadata":{"id":"nGddTcSHDeq6"},"source":["##### Training Function"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wKJ1ljH9Deq7"},"outputs":[],"source":["def train_epoch(train_dataloader, model, optimizer):\n","    epoch_train_loss = 0\n","    iterations = 0\n","    model.train() # Prepare model for training\n","    for i, (mol_dgl_graph, labels, masks, globals) in enumerate(train_dataloader):\n","        prediction = model(mol_dgl_graph, globals)\n","        loss_train = loss_func(prediction, labels, masks, num_tasks)\n","        optimizer.zero_grad(set_to_none=True)\n","        loss_train.backward()\n","        optimizer.step()\n","        epoch_train_loss += loss_train.detach().item()\n","        iterations += 1\n","    epoch_train_loss /= iterations\n","    return epoch_train_loss"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-63AbuCzDeq7"},"outputs":[],"source":["def train_evaluate():\n","\n","    model = GATConv1(config, global_size, num_tasks)\n","    optimizer = torch.optim.Adam(model.parameters(), lr = 0.001)\n","\n","    best_val = 0\n","    patience_count = 1\n","    epoch = 1\n","\n","    while epoch <= num_epochs:\n","        if patience_count <= patience:\n","            model.train()\n","            loss_train = train_epoch(train_dataloader, model, optimizer)\n","            model.eval()\n","            score_val = compute_score(model, val_dataloader, len(val_set), num_tasks)\n","            if score_val > best_val:\n","                best_val = score_val\n","                print(\"Save checkpoint\")\n","                path = os.path.join(checkpoint_path, 'checkpoint.pth')\n","                dict_checkpoint = {\"score_val\": score_val}\n","                dict_checkpoint.update({\"model_state_dict\": model.state_dict(), \"optimizer_state\": optimizer.state_dict()})\n","                with open(path, \"wb\") as outputfile:\n","                    cloudpickle.dump(dict_checkpoint, outputfile)\n","                patience_count = 1\n","            else:\n","                print(\"Patience\", patience_count)\n","                patience_count += 1\n","\n","            print(\"Epoch: {}/{} | Training Loss: {:.3f} | Valid Score: {:.3f}\".format(\n","            epoch, num_epochs, loss_train, score_val))\n","\n","            print(\" \")\n","            print(\"Epoch: {}/{} | Best Valid Score Until Now: {:.3f}\".format(epoch, num_epochs, best_val), \"\\n\")\n","        epoch += 1\n","\n","    # best model save\n","    shutil.rmtree(best_model_path, ignore_errors=True)\n","    shutil.copytree(checkpoint_path, best_model_path)\n","\n","    print(\"Final results:\")\n","    print(\"Average Valid Score: {:.3f}\".format(np.mean(best_val)), \"\\n\")\n"]},{"cell_type":"markdown","metadata":{"id":"NSfIc5bwDeq7"},"source":["##### Function to compute test set score of the final saved model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KnJ4pdLODeq7"},"outputs":[],"source":["def test_evaluate():\n","    final_model = GATConv1(config, global_size, num_tasks)\n","    path = os.path.join(best_model_path, 'checkpoint.pth')\n","    with open(path, 'rb') as f:\n","        checkpoint = cloudpickle.load(f)\n","    final_model.load_state_dict(checkpoint[\"model_state_dict\"])\n","    final_model.eval()\n","    test_score = compute_score(final_model, test_dataloader, len(test_set), num_tasks)\n","\n","    print(\"Test Score: {:.3f}\".format(test_score), \"\\n\")\n","    print(\"Execution time: {:.3f} seconds\".format(time.time() - start_time))\n"]},{"cell_type":"markdown","metadata":{"id":"TV10xXalDeq8"},"source":["##### Train the model and evaluate its performance"]},{"cell_type":"code","execution_count":null,"metadata":{"outputId":"7f806acb-ccb8-4c7d-e31a-0b742f16e77d","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1688412435981,"user_tz":-210,"elapsed":19013,"user":{"displayName":"Mahdi Salehi","userId":"13663744920024374425"}},"id":"Kb9VVPAyDeq8"},"outputs":[{"output_type":"stream","name":"stdout","text":["Save checkpoint\n","Epoch: 1/100 | Training Loss: 0.561 | Valid Score: 0.772\n"," \n","Epoch: 1/100 | Best Valid Score Until Now: 0.772 \n","\n","Save checkpoint\n","Epoch: 2/100 | Training Loss: 0.466 | Valid Score: 0.817\n"," \n","Epoch: 2/100 | Best Valid Score Until Now: 0.817 \n","\n","Save checkpoint\n","Epoch: 3/100 | Training Loss: 0.425 | Valid Score: 0.820\n"," \n","Epoch: 3/100 | Best Valid Score Until Now: 0.820 \n","\n","Save checkpoint\n","Epoch: 4/100 | Training Loss: 0.423 | Valid Score: 0.838\n"," \n","Epoch: 4/100 | Best Valid Score Until Now: 0.838 \n","\n","Save checkpoint\n","Epoch: 5/100 | Training Loss: 0.402 | Valid Score: 0.838\n"," \n","Epoch: 5/100 | Best Valid Score Until Now: 0.838 \n","\n","Save checkpoint\n","Epoch: 6/100 | Training Loss: 0.404 | Valid Score: 0.844\n"," \n","Epoch: 6/100 | Best Valid Score Until Now: 0.844 \n","\n","Patience 1\n","Epoch: 7/100 | Training Loss: 0.396 | Valid Score: 0.837\n"," \n","Epoch: 7/100 | Best Valid Score Until Now: 0.844 \n","\n","Patience 2\n","Epoch: 8/100 | Training Loss: 0.395 | Valid Score: 0.813\n"," \n","Epoch: 8/100 | Best Valid Score Until Now: 0.844 \n","\n","Patience 3\n","Epoch: 9/100 | Training Loss: 0.394 | Valid Score: 0.832\n"," \n","Epoch: 9/100 | Best Valid Score Until Now: 0.844 \n","\n","Patience 4\n","Epoch: 10/100 | Training Loss: 0.386 | Valid Score: 0.837\n"," \n","Epoch: 10/100 | Best Valid Score Until Now: 0.844 \n","\n","Patience 5\n","Epoch: 11/100 | Training Loss: 0.380 | Valid Score: 0.829\n"," \n","Epoch: 11/100 | Best Valid Score Until Now: 0.844 \n","\n","Patience 6\n","Epoch: 12/100 | Training Loss: 0.385 | Valid Score: 0.834\n"," \n","Epoch: 12/100 | Best Valid Score Until Now: 0.844 \n","\n","Patience 7\n","Epoch: 13/100 | Training Loss: 0.379 | Valid Score: 0.826\n"," \n","Epoch: 13/100 | Best Valid Score Until Now: 0.844 \n","\n","Patience 8\n","Epoch: 14/100 | Training Loss: 0.373 | Valid Score: 0.824\n"," \n","Epoch: 14/100 | Best Valid Score Until Now: 0.844 \n","\n","Patience 9\n","Epoch: 15/100 | Training Loss: 0.365 | Valid Score: 0.837\n"," \n","Epoch: 15/100 | Best Valid Score Until Now: 0.844 \n","\n","Patience 10\n","Epoch: 16/100 | Training Loss: 0.361 | Valid Score: 0.834\n"," \n","Epoch: 16/100 | Best Valid Score Until Now: 0.844 \n","\n","Final results:\n","Average Valid Score: 0.844 \n","\n","Test Score: 0.688 \n","\n","Execution time: 18.712 seconds\n"]}],"source":["import time\n","start_time = time.time()\n","\n","train_evaluate()\n","test_evaluate()"]},{"cell_type":"markdown","source":["# GAT 2"],"metadata":{"id":"Hl2BlGQxTQTn"}},{"cell_type":"code","source":[],"metadata":{"id":"AAGqScW_TQT0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from dgl.nn.pytorch import GATConv\n","class GATConv2(nn.Module):\n","    def __init__(self, config, global_size=200, num_tasks=1):\n","        super().__init__()\n","        self.config = config\n","        self.num_tasks = num_tasks\n","\n","        # Node feature size\n","        self.node_feature_size = self.config.get('node_feature_size', 127)\n","\n","        # Edge feature size\n","        self.edge_feature_size = self.config.get('edge_feature_size', 12)\n","\n","        # Hidden size\n","        self.hidden_size = self.config.get('hidden_size', 100)\n","\n","        # Number of attention heads\n","        self.num_heads = self.config.get('num_heads', 1)\n","\n","        # Dropout probability\n","        self.dropout = self.config.get('dropout', 0.0)\n","\n","        # GAT layer\n","        self.conv1 = GATConv(self.node_feature_size,self.hidden_size,num_heads=self.num_heads,feat_drop=self.dropout,attn_drop=self.dropout,allow_zero_in_degree=True)\n","\n","        # Linear layer\n","        self.fc = nn.Linear(self.hidden_size * self.num_heads,self.hidden_size)\n","\n","        # GAT layer\n","        self.conv2 = GATConv(self.hidden_size,self.hidden_size,num_heads=1,feat_drop=self.dropout,attn_drop=self.dropout,allow_zero_in_degree=True)\n","\n","        # GAT layer\n","        self.conv3 = GATConv(self.hidden_size,self.num_tasks,num_heads=1,feat_drop=self.dropout,attn_drop=self.dropout,allow_zero_in_degree=True)\n","\n","    def forward(self, mol_dgl_graph, globals):\n","        mol_dgl_graph.ndata[\"v\"] = mol_dgl_graph.ndata[\"v\"][:, :self.node_feature_size]\n","        mol_dgl_graph.edata[\"e\"] = mol_dgl_graph.edata[\"e\"][:, :self.edge_feature_size]\n","\n","        # First GAT layer\n","        h = self.conv1(mol_dgl_graph, mol_dgl_graph.ndata[\"v\"]).flatten(1)\n","        h = F.relu(h)\n","        h = self.fc(h)\n","        h = F.dropout(h, p=self.dropout, training=self.training)\n","\n","        # Second GAT layer\n","        h = self.conv2(mol_dgl_graph, h).squeeze(1)\n","        h = F.relu(h)\n","        h = F.dropout(h, p=self.dropout, training=self.training)\n","\n","        # Third GAT layer\n","        if self.num_tasks == 1:\n","            h = self.conv3(mol_dgl_graph, h).squeeze(1)\n","        else:\n","            hs = []\n","            for i in range(self.num_tasks):\n","                hi = self.conv3(mol_dgl_graph, h).squeeze(1)\n","                hs.append(hi)\n","            h = torch.stack(hs, dim=1)\n","\n","        mol_dgl_graph.ndata[\"h\"] = h\n","\n","        if self.num_tasks == 1:\n","            return dgl.mean_nodes(mol_dgl_graph, \"h\")\n","        else:\n","            return dgl.mean_nodes(mol_dgl_graph, \"h\"), h"],"metadata":{"id":"FoDynBi1TQT0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-e6OMkz1TQT0"},"source":["#### Function to Compute Score of the Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eoh6CKZmTQT1"},"outputs":[],"source":["from sklearn.metrics import roc_auc_score\n","\n","def compute_score(model, data_loader, val_size, num_tasks):\n","    model.eval()\n","    metric = roc_auc_score\n","    with torch.no_grad():\n","        prediction_all= torch.empty(0)\n","        labels_all= torch.empty(0)\n","        masks_all= torch.empty(0)\n","        for i, (mol_dgl_graph, labels, masks, globals) in enumerate(data_loader):\n","            prediction = model(mol_dgl_graph, globals)\n","            prediction = torch.sigmoid(prediction)\n","            prediction_all = torch.cat((prediction_all, prediction), 0)\n","            labels_all = torch.cat((labels_all, labels), 0)\n","            masks_all = torch.cat((masks_all, masks), 0)\n","        average = torch.tensor([0.])\n","        for i in range(num_tasks):\n","            a1 = prediction_all[:, i][masks_all[:,i]==1]\n","            a2 = labels_all[:, i][masks_all[:,i]==1]\n","            try:\n","                t = metric(a2.int().cpu(), a1.cpu()).item()\n","            except ValueError:\n","                t = 0\n","            average += t\n","    return average.item()/num_tasks"]},{"cell_type":"markdown","metadata":{"id":"nP1LtR40TQT1"},"source":["#### Loss Function"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jBjwcQqtTQT1"},"outputs":[],"source":["def loss_func(output, label, mask, num_tasks):\n","    pos_weight = torch.ones((1, num_tasks))\n","    pos_weight\n","    criterion = torch.nn.BCEWithLogitsLoss(reduction='none', pos_weight=pos_weight)\n","    loss = mask*criterion(output,label)\n","    loss = loss.sum() / mask.sum()\n","    return loss"]},{"cell_type":"markdown","metadata":{"id":"wjLu_1I0TQT1"},"source":["#### Training and Evaluation"]},{"cell_type":"markdown","metadata":{"id":"qvG0qjbYTQT2"},"source":["##### Training Function"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7U1f1E6yTQT2"},"outputs":[],"source":["def train_epoch(train_dataloader, model, optimizer):\n","    epoch_train_loss = 0\n","    iterations = 0\n","    model.train() # Prepare model for training\n","    for i, (mol_dgl_graph, labels, masks, globals) in enumerate(train_dataloader):\n","        prediction = model(mol_dgl_graph, globals)\n","        loss_train = loss_func(prediction, labels, masks, num_tasks)\n","        optimizer.zero_grad(set_to_none=True)\n","        loss_train.backward()\n","        optimizer.step()\n","        epoch_train_loss += loss_train.detach().item()\n","        iterations += 1\n","    epoch_train_loss /= iterations\n","    return epoch_train_loss"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"16H-3PDqTQT2"},"outputs":[],"source":["def train_evaluate():\n","\n","    model = GATConv2(config, global_size, num_tasks)\n","    optimizer = torch.optim.Adam(model.parameters(), lr = 0.001)\n","\n","    best_val = 0\n","    patience_count = 1\n","    epoch = 1\n","\n","    while epoch <= num_epochs:\n","        if patience_count <= patience:\n","            model.train()\n","            loss_train = train_epoch(train_dataloader, model, optimizer)\n","            model.eval()\n","            score_val = compute_score(model, val_dataloader, len(val_set), num_tasks)\n","            if score_val > best_val:\n","                best_val = score_val\n","                print(\"Save checkpoint\")\n","                path = os.path.join(checkpoint_path, 'checkpoint.pth')\n","                dict_checkpoint = {\"score_val\": score_val}\n","                dict_checkpoint.update({\"model_state_dict\": model.state_dict(), \"optimizer_state\": optimizer.state_dict()})\n","                with open(path, \"wb\") as outputfile:\n","                    cloudpickle.dump(dict_checkpoint, outputfile)\n","                patience_count = 1\n","            else:\n","                print(\"Patience\", patience_count)\n","                patience_count += 1\n","\n","            print(\"Epoch: {}/{} | Training Loss: {:.3f} | Valid Score: {:.3f}\".format(\n","            epoch, num_epochs, loss_train, score_val))\n","\n","            print(\" \")\n","            print(\"Epoch: {}/{} | Best Valid Score Until Now: {:.3f}\".format(epoch, num_epochs, best_val), \"\\n\")\n","        epoch += 1\n","\n","    # best model save\n","    shutil.rmtree(best_model_path, ignore_errors=True)\n","    shutil.copytree(checkpoint_path, best_model_path)\n","\n","    print(\"Final results:\")\n","    print(\"Average Valid Score: {:.3f}\".format(np.mean(best_val)), \"\\n\")\n"]},{"cell_type":"markdown","metadata":{"id":"POpH6zcTTQT3"},"source":["##### Function to compute test set score of the final saved model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LSWct7fLTQT4"},"outputs":[],"source":["def test_evaluate():\n","    final_model = GATConv2(config, global_size, num_tasks)\n","    path = os.path.join(best_model_path, 'checkpoint.pth')\n","    with open(path, 'rb') as f:\n","        checkpoint = cloudpickle.load(f)\n","    final_model.load_state_dict(checkpoint[\"model_state_dict\"])\n","    final_model.eval()\n","    test_score = compute_score(final_model, test_dataloader, len(test_set), num_tasks)\n","\n","    print(\"Test Score: {:.3f}\".format(test_score), \"\\n\")\n","    print(\"Execution time: {:.3f} seconds\".format(time.time() - start_time))\n"]},{"cell_type":"markdown","metadata":{"id":"gzj3gR3CTQT4"},"source":["##### Train the model and evaluate its performance"]},{"cell_type":"code","execution_count":null,"metadata":{"outputId":"8d78980a-7075-4b51-e012-dd24aafeaf8a","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1688314777216,"user_tz":-210,"elapsed":39035,"user":{"displayName":"Mahdi Salehi","userId":"13663744920024374425"}},"id":"jThbanBhTQT4"},"outputs":[{"output_type":"stream","name":"stdout","text":["Save checkpoint\n","Epoch: 1/100 | Training Loss: 0.518 | Valid Score: 0.767\n"," \n","Epoch: 1/100 | Best Valid Score Until Now: 0.767 \n","\n","Save checkpoint\n","Epoch: 2/100 | Training Loss: 0.461 | Valid Score: 0.792\n"," \n","Epoch: 2/100 | Best Valid Score Until Now: 0.792 \n","\n","Save checkpoint\n","Epoch: 3/100 | Training Loss: 0.438 | Valid Score: 0.801\n"," \n","Epoch: 3/100 | Best Valid Score Until Now: 0.801 \n","\n","Save checkpoint\n","Epoch: 4/100 | Training Loss: 0.450 | Valid Score: 0.804\n"," \n","Epoch: 4/100 | Best Valid Score Until Now: 0.804 \n","\n","Patience 1\n","Epoch: 5/100 | Training Loss: 0.428 | Valid Score: 0.798\n"," \n","Epoch: 5/100 | Best Valid Score Until Now: 0.804 \n","\n","Patience 2\n","Epoch: 6/100 | Training Loss: 0.421 | Valid Score: 0.801\n"," \n","Epoch: 6/100 | Best Valid Score Until Now: 0.804 \n","\n","Save checkpoint\n","Epoch: 7/100 | Training Loss: 0.412 | Valid Score: 0.811\n"," \n","Epoch: 7/100 | Best Valid Score Until Now: 0.811 \n","\n","Patience 1\n","Epoch: 8/100 | Training Loss: 0.410 | Valid Score: 0.799\n"," \n","Epoch: 8/100 | Best Valid Score Until Now: 0.811 \n","\n","Patience 2\n","Epoch: 9/100 | Training Loss: 0.405 | Valid Score: 0.810\n"," \n","Epoch: 9/100 | Best Valid Score Until Now: 0.811 \n","\n","Save checkpoint\n","Epoch: 10/100 | Training Loss: 0.411 | Valid Score: 0.815\n"," \n","Epoch: 10/100 | Best Valid Score Until Now: 0.815 \n","\n","Patience 1\n","Epoch: 11/100 | Training Loss: 0.402 | Valid Score: 0.808\n"," \n","Epoch: 11/100 | Best Valid Score Until Now: 0.815 \n","\n","Patience 2\n","Epoch: 12/100 | Training Loss: 0.394 | Valid Score: 0.814\n"," \n","Epoch: 12/100 | Best Valid Score Until Now: 0.815 \n","\n","Save checkpoint\n","Epoch: 13/100 | Training Loss: 0.389 | Valid Score: 0.816\n"," \n","Epoch: 13/100 | Best Valid Score Until Now: 0.816 \n","\n","Patience 1\n","Epoch: 14/100 | Training Loss: 0.389 | Valid Score: 0.806\n"," \n","Epoch: 14/100 | Best Valid Score Until Now: 0.816 \n","\n","Patience 2\n","Epoch: 15/100 | Training Loss: 0.388 | Valid Score: 0.803\n"," \n","Epoch: 15/100 | Best Valid Score Until Now: 0.816 \n","\n","Patience 3\n","Epoch: 16/100 | Training Loss: 0.380 | Valid Score: 0.791\n"," \n","Epoch: 16/100 | Best Valid Score Until Now: 0.816 \n","\n","Save checkpoint\n","Epoch: 17/100 | Training Loss: 0.372 | Valid Score: 0.816\n"," \n","Epoch: 17/100 | Best Valid Score Until Now: 0.816 \n","\n","Patience 1\n","Epoch: 18/100 | Training Loss: 0.376 | Valid Score: 0.779\n"," \n","Epoch: 18/100 | Best Valid Score Until Now: 0.816 \n","\n","Patience 2\n","Epoch: 19/100 | Training Loss: 0.379 | Valid Score: 0.803\n"," \n","Epoch: 19/100 | Best Valid Score Until Now: 0.816 \n","\n","Patience 3\n","Epoch: 20/100 | Training Loss: 0.379 | Valid Score: 0.802\n"," \n","Epoch: 20/100 | Best Valid Score Until Now: 0.816 \n","\n","Patience 4\n","Epoch: 21/100 | Training Loss: 0.368 | Valid Score: 0.803\n"," \n","Epoch: 21/100 | Best Valid Score Until Now: 0.816 \n","\n","Patience 5\n","Epoch: 22/100 | Training Loss: 0.370 | Valid Score: 0.806\n"," \n","Epoch: 22/100 | Best Valid Score Until Now: 0.816 \n","\n","Patience 6\n","Epoch: 23/100 | Training Loss: 0.364 | Valid Score: 0.809\n"," \n","Epoch: 23/100 | Best Valid Score Until Now: 0.816 \n","\n","Patience 7\n","Epoch: 24/100 | Training Loss: 0.363 | Valid Score: 0.803\n"," \n","Epoch: 24/100 | Best Valid Score Until Now: 0.816 \n","\n","Patience 8\n","Epoch: 25/100 | Training Loss: 0.359 | Valid Score: 0.808\n"," \n","Epoch: 25/100 | Best Valid Score Until Now: 0.816 \n","\n","Patience 9\n","Epoch: 26/100 | Training Loss: 0.358 | Valid Score: 0.796\n"," \n","Epoch: 26/100 | Best Valid Score Until Now: 0.816 \n","\n","Patience 10\n","Epoch: 27/100 | Training Loss: 0.353 | Valid Score: 0.798\n"," \n","Epoch: 27/100 | Best Valid Score Until Now: 0.816 \n","\n","Final results:\n","Average Valid Score: 0.816 \n","\n","Test Score: 0.698 \n","\n","Execution time: 38.806 seconds\n"]}],"source":["import time\n","start_time = time.time()\n","\n","train_evaluate()\n","test_evaluate()"]},{"cell_type":"code","source":[],"metadata":{"id":"12N3K84PX2UO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#GAT 3\n"],"metadata":{"id":"63aZjT9reVjF"}},{"cell_type":"code","source":["from dgl.nn.pytorch import GATConv\n","from torch.nn import BatchNorm1d\n","\n","class GATConv3(nn.Module):\n","    def __init__(self, config, global_size=200, num_tasks=1):\n","        super().__init__()\n","        self.config = config\n","        self.num_tasks = num_tasks\n","\n","        # Node feature size\n","        self.node_feature_size = self.config.get('node_feature_size', 127)\n","\n","        # Edge feature size\n","        self.edge_feature_size = self.config.get('edge_feature_size', 12)\n","\n","        # Hidden size\n","        self.hidden_size = self.config.get('hidden_size', 100)\n","\n","        # Number of attention heads\n","        self.num_heads = self.config.get('num_heads', 1)\n","\n","        # Dropout probability\n","        self.dropout = self.config.get('dropout', 0.0)\n","\n","        # GAT layer\n","        self.conv1 = GATConv(self.node_feature_size, 64, num_heads=self.num_heads,\n","                             feat_drop=self.dropout, attn_drop=self.dropout,\n","                             allow_zero_in_degree=True)\n","        self.conv2 = GATConv(64, 128, num_heads=1, feat_drop=self.dropout,\n","                             attn_drop=self.dropout, allow_zero_in_degree=True)\n","        self.conv3 = GATConv(128, 256, num_heads=1, feat_drop=self.dropout,\n","                             attn_drop=self.dropout, allow_zero_in_degree=True)\n","        self.conv4 = GATConv(256, 128, num_heads=1, feat_drop=self.dropout,\n","                             attn_drop=self.dropout, allow_zero_in_degree=True)\n","        self.conv5 = GATConv(128, self.num_tasks, num_heads=1, feat_drop=self.dropout,\n","                             attn_drop=self.dropout, allow_zero_in_degree=True)\n","\n","        # Linear layer\n","        self.fc1 = nn.Linear(64 * self.num_heads, 64)\n","        self.bn1 = BatchNorm1d(64)\n","        self.fc2 = nn.Linear(128 * self.num_heads, 128)\n","        self.bn2 = BatchNorm1d(128)\n","        self.fc3 = nn.Linear(256 * self.num_heads, 256)\n","        self.bn3 = BatchNorm1d(256)\n","\n","    def forward(self, mol_dgl_graph, globals):\n","        mol_dgl_graph.ndata[\"v\"] = mol_dgl_graph.ndata[\"v\"][:, :self.node_feature_size]\n","        mol_dgl_graph.edata[\"e\"] = mol_dgl_graph.edata[\"e\"][:, :self.edge_feature_size]\n","\n","        # First GAT layer\n","        h = self.conv1(mol_dgl_graph, mol_dgl_graph.ndata[\"v\"]).flatten(1)\n","        h = F.relu(h)\n","        h = self.fc1(h)\n","        h = self.bn1(h)  # add batch normalization\n","        h = F.dropout(h, p=self.dropout, training=self.training)\n","\n","        # Second GAT layer\n","        h = self.conv2(mol_dgl_graph, h).squeeze(1)\n","        h = F.relu(h)\n","        h = self.fc2(h)\n","        h = self.bn2(h)  # add batch normalization\n","        h = F.dropout(h, p=self.dropout, training=self.training)\n","\n","        h = self.conv3(mol_dgl_graph, h).squeeze(1)\n","        h = F.relu(h)\n","        h = self.fc3(h)\n","        h = self.bn3(h)  # add batch normalization\n","        h = F.dropout(h, p=self.dropout, training=self.training)\n","\n","        h = self.conv4(mol_dgl_graph, h).squeeze(1)\n","        h = F.relu(h)\n","        h = self.fc2(h)\n","        h = self.bn2(h)  # add batch normalization\n","        h = F.dropout(h, p=self.dropout, training=self.training)\n","\n","        # Third GAT layer\n","        if self.num_tasks == 1:\n","            h = self.conv5(mol_dgl_graph, h).squeeze(1)\n","        else:\n","            hs = []\n","            for i in range(self.num_tasks):\n","                hi = self.conv3(mol_dgl_graph, h).squeeze(1)\n","                hs.append(hi)\n","            h = torch.stack(hs, dim=1)\n","\n","        mol_dgl_graph.ndata[\"h\"]= h\n","\n","        if self.num_tasks == 1:\n","            return dgl.mean_nodes(mol_dgl_graph, \"h\")\n","        else:\n","            return dgl.mean_nodes(mol_dgl_graph, \"h\"), h"],"metadata":{"id":"MD_PBPqA-_EB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2H802Pb2-_EI"},"source":["#### Function to Compute Score of the Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nVMDMQhe-_EJ"},"outputs":[],"source":["from sklearn.metrics import roc_auc_score\n","\n","def compute_score(model, data_loader, val_size, num_tasks):\n","    model.eval()\n","    metric = roc_auc_score\n","    with torch.no_grad():\n","        prediction_all= torch.empty(0)\n","        labels_all= torch.empty(0)\n","        masks_all= torch.empty(0)\n","        for i, (mol_dgl_graph, labels, masks, globals) in enumerate(data_loader):\n","            prediction = model(mol_dgl_graph, globals)\n","            prediction = torch.sigmoid(prediction)\n","            prediction_all = torch.cat((prediction_all, prediction), 0)\n","            labels_all = torch.cat((labels_all, labels), 0)\n","            masks_all = torch.cat((masks_all, masks), 0)\n","        average = torch.tensor([0.])\n","        for i in range(num_tasks):\n","            a1 = prediction_all[:, i][masks_all[:,i]==1]\n","            a2 = labels_all[:, i][masks_all[:,i]==1]\n","            try:\n","                t = metric(a2.int().cpu(), a1.cpu()).item()\n","            except ValueError:\n","                t = 0\n","            average += t\n","    return average.item()/num_tasks"]},{"cell_type":"markdown","metadata":{"id":"ErVL5Q0l-_EJ"},"source":["#### Loss Function"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zF83P--v-_EJ"},"outputs":[],"source":["def loss_func(output, label, mask, num_tasks):\n","    pos_weight = torch.ones((1, num_tasks))\n","    pos_weight\n","    criterion = torch.nn.BCEWithLogitsLoss(reduction='none', pos_weight=pos_weight)\n","    loss = mask*criterion(output,label)\n","    loss = loss.sum() / mask.sum()\n","    return loss"]},{"cell_type":"markdown","metadata":{"id":"LLfwDhTU-_EJ"},"source":["#### Training and Evaluation"]},{"cell_type":"markdown","metadata":{"id":"SRV0gODc-_EK"},"source":["##### Training Function"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cb-5bUMN-_EK"},"outputs":[],"source":["def train_epoch(train_dataloader, model, optimizer):\n","    epoch_train_loss = 0\n","    iterations = 0\n","    model.train() # Prepare model for training\n","    for i, (mol_dgl_graph, labels, masks, globals) in enumerate(train_dataloader):\n","        prediction = model(mol_dgl_graph, globals)\n","        loss_train = loss_func(prediction, labels, masks, num_tasks)\n","        optimizer.zero_grad(set_to_none=True)\n","        loss_train.backward()\n","        optimizer.step()\n","        epoch_train_loss += loss_train.detach().item()\n","        iterations += 1\n","    epoch_train_loss /= iterations\n","    return epoch_train_loss"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pM8CvKT_-_EK"},"outputs":[],"source":["def train_evaluate():\n","\n","    model = GATConv3(config, global_size, num_tasks)\n","    optimizer = torch.optim.Adam(model.parameters(), lr = 0.001)\n","\n","    best_val = 0\n","    patience_count = 1\n","    epoch = 1\n","\n","    while epoch <= num_epochs:\n","        if patience_count <= patience:\n","            model.train()\n","            loss_train = train_epoch(train_dataloader, model, optimizer)\n","            model.eval()\n","            score_val = compute_score(model, val_dataloader, len(val_set), num_tasks)\n","            if score_val > best_val:\n","                best_val = score_val\n","                print(\"Save checkpoint\")\n","                path = os.path.join(checkpoint_path, 'checkpoint.pth')\n","                dict_checkpoint = {\"score_val\": score_val}\n","                dict_checkpoint.update({\"model_state_dict\": model.state_dict(), \"optimizer_state\": optimizer.state_dict()})\n","                with open(path, \"wb\") as outputfile:\n","                    cloudpickle.dump(dict_checkpoint, outputfile)\n","                patience_count = 1\n","            else:\n","                print(\"Patience\", patience_count)\n","                patience_count += 1\n","\n","            print(\"Epoch: {}/{} | Training Loss: {:.3f} | Valid Score: {:.3f}\".format(\n","            epoch, num_epochs, loss_train, score_val))\n","\n","            print(\" \")\n","            print(\"Epoch: {}/{} | Best Valid Score Until Now: {:.3f}\".format(epoch, num_epochs, best_val), \"\\n\")\n","        epoch += 1\n","\n","    # best model save\n","    shutil.rmtree(best_model_path, ignore_errors=True)\n","    shutil.copytree(checkpoint_path, best_model_path)\n","\n","    print(\"Final results:\")\n","    print(\"Average Valid Score: {:.3f}\".format(np.mean(best_val)), \"\\n\")\n"]},{"cell_type":"markdown","metadata":{"id":"X7Mtvoyo-_EK"},"source":["##### Function to compute test set score of the final saved model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LY4hX_sf-_EK"},"outputs":[],"source":["def test_evaluate():\n","    final_model = GATConv3(config, global_size, num_tasks)\n","    path = os.path.join(best_model_path, 'checkpoint.pth')\n","    with open(path, 'rb') as f:\n","        checkpoint = cloudpickle.load(f)\n","    final_model.load_state_dict(checkpoint[\"model_state_dict\"])\n","    final_model.eval()\n","    test_score = compute_score(final_model, test_dataloader, len(test_set), num_tasks)\n","\n","    print(\"Test Score: {:.3f}\".format(test_score), \"\\n\")\n","    print(\"Execution time: {:.3f} seconds\".format(time.time() - start_time))\n"]},{"cell_type":"markdown","metadata":{"id":"ICa5raYv-_EK"},"source":["##### Train the model and evaluate its performance"]},{"cell_type":"code","source":["import time\n","start_time = time.time()\n","\n","train_evaluate()\n","test_evaluate()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1688409723878,"user_tz":-210,"elapsed":39255,"user":{"displayName":"Mahdi Salehi","userId":"13663744920024374425"}},"outputId":"2cac0f53-2630-4f8d-fd32-8865153a11bf","id":"Km_uwLf5-_EL"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Save checkpoint\n","Epoch: 1/100 | Training Loss: 0.565 | Valid Score: 0.788\n"," \n","Epoch: 1/100 | Best Valid Score Until Now: 0.788 \n","\n","Patience 1\n","Epoch: 2/100 | Training Loss: 0.437 | Valid Score: 0.779\n"," \n","Epoch: 2/100 | Best Valid Score Until Now: 0.788 \n","\n","Save checkpoint\n","Epoch: 3/100 | Training Loss: 0.402 | Valid Score: 0.809\n"," \n","Epoch: 3/100 | Best Valid Score Until Now: 0.809 \n","\n","Patience 1\n","Epoch: 4/100 | Training Loss: 0.397 | Valid Score: 0.791\n"," \n","Epoch: 4/100 | Best Valid Score Until Now: 0.809 \n","\n","Patience 2\n","Epoch: 5/100 | Training Loss: 0.379 | Valid Score: 0.796\n"," \n","Epoch: 5/100 | Best Valid Score Until Now: 0.809 \n","\n","Patience 3\n","Epoch: 6/100 | Training Loss: 0.387 | Valid Score: 0.800\n"," \n","Epoch: 6/100 | Best Valid Score Until Now: 0.809 \n","\n","Save checkpoint\n","Epoch: 7/100 | Training Loss: 0.372 | Valid Score: 0.817\n"," \n","Epoch: 7/100 | Best Valid Score Until Now: 0.817 \n","\n","Patience 1\n","Epoch: 8/100 | Training Loss: 0.372 | Valid Score: 0.804\n"," \n","Epoch: 8/100 | Best Valid Score Until Now: 0.817 \n","\n","Patience 2\n","Epoch: 9/100 | Training Loss: 0.366 | Valid Score: 0.765\n"," \n","Epoch: 9/100 | Best Valid Score Until Now: 0.817 \n","\n","Patience 3\n","Epoch: 10/100 | Training Loss: 0.371 | Valid Score: 0.782\n"," \n","Epoch: 10/100 | Best Valid Score Until Now: 0.817 \n","\n","Patience 4\n","Epoch: 11/100 | Training Loss: 0.340 | Valid Score: 0.784\n"," \n","Epoch: 11/100 | Best Valid Score Until Now: 0.817 \n","\n","Patience 5\n","Epoch: 12/100 | Training Loss: 0.339 | Valid Score: 0.798\n"," \n","Epoch: 12/100 | Best Valid Score Until Now: 0.817 \n","\n","Patience 6\n","Epoch: 13/100 | Training Loss: 0.330 | Valid Score: 0.800\n"," \n","Epoch: 13/100 | Best Valid Score Until Now: 0.817 \n","\n","Patience 7\n","Epoch: 14/100 | Training Loss: 0.338 | Valid Score: 0.774\n"," \n","Epoch: 14/100 | Best Valid Score Until Now: 0.817 \n","\n","Patience 8\n","Epoch: 15/100 | Training Loss: 0.321 | Valid Score: 0.794\n"," \n","Epoch: 15/100 | Best Valid Score Until Now: 0.817 \n","\n","Patience 9\n","Epoch: 16/100 | Training Loss: 0.325 | Valid Score: 0.806\n"," \n","Epoch: 16/100 | Best Valid Score Until Now: 0.817 \n","\n","Patience 10\n","Epoch: 17/100 | Training Loss: 0.307 | Valid Score: 0.790\n"," \n","Epoch: 17/100 | Best Valid Score Until Now: 0.817 \n","\n","Final results:\n","Average Valid Score: 0.817 \n","\n","Test Score: 0.817 \n","\n","Execution time: 38.913 seconds\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"OwkTzCXH-_EL"},"execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.9"},"orig_nbformat":4,"colab":{"provenance":[],"gpuType":"T4"}},"nbformat":4,"nbformat_minor":0}