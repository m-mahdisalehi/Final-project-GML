{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPYcwqRD/5VisyCD2hvyF1a"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Dataset: FreeSolv\n"],"metadata":{"id":"TLE2X45dooAn"}},{"cell_type":"code","source":["!pip install dgl"],"metadata":{"id":"ABbgpnL2GZMO","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1689005605514,"user_tz":-210,"elapsed":15319,"user":{"displayName":"Mahdi Salehi","userId":"13663744920024374425"}},"outputId":"32a383aa-1e05-440d-89f1-43c8bec40b0c"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting dgl\n","  Downloading dgl-1.1.1-cp310-cp310-manylinux1_x86_64.whl (6.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m41.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from dgl) (1.22.4)\n","Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from dgl) (1.10.1)\n","Requirement already satisfied: networkx>=2.1 in /usr/local/lib/python3.10/dist-packages (from dgl) (3.1)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from dgl) (2.27.1)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from dgl) (4.65.0)\n","Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from dgl) (5.9.5)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->dgl) (1.26.16)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->dgl) (2023.5.7)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->dgl) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->dgl) (3.4)\n","Installing collected packages: dgl\n","Successfully installed dgl-1.1.1\n"]}]},{"cell_type":"code","execution_count":2,"metadata":{"id":"Os4tcuBUXtXx","executionInfo":{"status":"ok","timestamp":1689005612385,"user_tz":-210,"elapsed":6879,"user":{"displayName":"Mahdi Salehi","userId":"13663744920024374425"}}},"outputs":[],"source":["%matplotlib inline\n","import os\n","\n","os.environ[\"DGLBACKEND\"] = \"pytorch\"\n","import dgl\n","import numpy as np\n","import networkx as nx\n","import torch\n","import torch.nn as nn\n","import dgl.function as fn\n","import torch.nn.functional as F\n","import shutil\n","from torch.utils.data import DataLoader\n","import cloudpickle\n","from dgl.nn import GraphConv"]},{"cell_type":"markdown","metadata":{"id":"jEajxfFQXtXz"},"source":["#### Set Path"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"EiiiFA2zXtX0","executionInfo":{"status":"ok","timestamp":1689005612386,"user_tz":-210,"elapsed":5,"user":{"displayName":"Mahdi Salehi","userId":"13663744920024374425"}}},"outputs":[],"source":["current_dir = \"./\"\n","checkpoint_path = current_dir + \"save_models/model_checkpoints/\" + \"checkpoint\"\n","os.makedirs(checkpoint_path, exist_ok=True)\n","\n","best_model_path = current_dir + \"save_models/best_model/\"\n","\n","folder_data_temp = current_dir +\"data_temp/\"\n","shutil.rmtree(folder_data_temp, ignore_errors=True)\n","\n","path_save = current_dir + \"graph_data.zip\"\n","shutil.unpack_archive(path_save, folder_data_temp)"]},{"cell_type":"markdown","metadata":{"id":"rBuQRkYAXtX0"},"source":["#### Custom PyTorch Datasets"]},{"cell_type":"code","source":["from sklearn import preprocessing\n","\n","\"\"\" Regression Dataset \"\"\"\n","class DGLDatasetReg(torch.utils.data.Dataset):\n","    def __init__(self, address, transform=None, train=False, scaler=None, scaler_regression=None):\n","        # Constructor method for the dataset\n","        # address: the file path of the dataset\n","        # transform: any data transformation to be applied to the dataset\n","        # train: a boolean parameter indicating if the dataset is for training or not\n","        # scaler: a scaler object from the `preprocessing` module of scikit-learn for normalization\n","        # scaler_regression: a boolean parameter indicating if normalization should be applied to the labels or not\n","        self.train = train\n","        self.scaler = scaler\n","        # Load the dataset from the given file path\n","        self.data_set, train_labels_masks_globals = dgl.load_graphs(address + \".bin\")\n","        num_graphs = len(self.data_set)\n","        # Extract the labels, masks and global features from the dataset\n","        self.labels = train_labels_masks_globals[\"labels\"].view(num_graphs, -1)\n","        self.masks = train_labels_masks_globals[\"masks\"].view(num_graphs, -1)\n","        self.globals = train_labels_masks_globals[\"globals\"].view(num_graphs, -1)\n","        self.transform = transform\n","        self.scaler_regression = scaler_regression\n","\n","    def scaler_method(self):\n","        # Method to fit a scaler on the training labels and return it\n","        # Returns None if the dataset is not for training\n","        if self.train:\n","            scaler = preprocessing.StandardScaler().fit(self.labels)\n","            self.scaler = scaler\n","        return self.scaler\n","\n","    def __len__(self):\n","        # Method to return the length of the dataset\n","        return len(self.data_set)\n","\n","    def __getitem__(self, idx):\n","        # Method to return a single sample from the dataset\n","        # idx: the index of the sample to be returned\n","        if self.scaler_regression:\n","            \"\"\" With Scaler\"\"\"\n","            # If scaler_regression is True, apply normalization to the labels using the fitted scaler\n","            return self.data_set[idx], torch.tensor(self.scaler.transform(self.labels)[idx]).float(), self.masks[idx], self.globals[idx]\n","        else:\n","            \"\"\" Without Scaler \"\"\"\n","            # If scaler_regression is False, return the labels without normalization\n","            return self.data_set[idx], self.labels[idx].float(), self.masks[idx], self.globals[idx]"],"metadata":{"id":"_aBgiHc7cpOu","executionInfo":{"status":"ok","timestamp":1689005613271,"user_tz":-210,"elapsed":890,"user":{"displayName":"Mahdi Salehi","userId":"13663744920024374425"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oiDW39kGXtX1"},"source":["#### Defining Train, Validation, and Test Set"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"kXNJWz4CXtX2","outputId":"5280aa4a-2a61-4711-e614-7a2a35b448bc","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1689005613686,"user_tz":-210,"elapsed":418,"user":{"displayName":"Mahdi Salehi","userId":"13663744920024374425"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["513 64 65\n"]}],"source":["path_data_temp = folder_data_temp + \"scaffold\"+\"_\"+str(0)\n","train_set = DGLDatasetReg(address=path_data_temp+\"_train\")\n","val_set = DGLDatasetReg(address=path_data_temp+\"_val\")\n","test_set = DGLDatasetReg(address=path_data_temp+\"_test\")\n","\n","print(len(train_set), len(val_set), len(test_set))\n"]},{"cell_type":"markdown","metadata":{"id":"5wjivnkXXtX4"},"source":["#### Data Loader"]},{"cell_type":"code","source":["def collate(batch):\n","    # This function takes a list of tuples (graphs, labels, masks, globals) as input\n","    # Concatenate a sequence of graphs into a batched graph using `dgl.batch`\n","    graphs = [e[0] for e in batch]\n","    g = dgl.batch(graphs)\n","\n","    # Concatenate a sequence of tensors (labels) along a new dimension\n","    # If a scaler object is provided, apply normalization to the labels using the scaler\n","    # Otherwise, return the labels without normalization\n","    if scaler is not None:\n","        labels = [torch.tensor(scaler.transform(e[1].unsqueeze(0)).squeeze(0)).float() for e in batch]\n","    else:\n","        labels = [e[1].float() for e in batch]\n","    labels = torch.stack(labels, 0)\n","\n","    # Concatenate a sequence of tensors (masks) along a new dimension\n","    masks = [e[2] for e in batch]\n","    masks = torch.stack(masks, 0)\n","\n","    # Concatenate a sequence of tensors (globals) along a new dimension\n","    globals_ = [e[3] for e in batch]\n","    globals_ = torch.stack(globals_, 0)\n","\n","    # Return the batched graph, labels, masks, and globals\n","    return g, labels, masks, globals_\n","\n","def loader(batch_size):\n","    # This function returns a set of data loaders for the training, validation, and test sets\n","    train_dataloader = DataLoader(train_set,\n","                              batch_size=batch_size,\n","                              collate_fn=collate,\n","                              drop_last=False,\n","                              shuffle=True,\n","                              num_workers=1)\n","\n","    val_dataloader =  DataLoader(val_set,\n","                             batch_size=batch_size,\n","                             collate_fn=collate,\n","                             drop_last=False,\n","                             shuffle=False,\n","                             num_workers=1)\n","\n","    test_dataloader = DataLoader(test_set,\n","                             batch_size=batch_size,\n","                             collate_fn=collate,\n","                             drop_last=False,\n","                             shuffle=False,\n","                             num_workers=1)\n","    return train_dataloader, val_dataloader, test_dataloader"],"metadata":{"id":"Xkffdzuzdlre","executionInfo":{"status":"ok","timestamp":1689005617819,"user_tz":-210,"elapsed":4,"user":{"displayName":"Mahdi Salehi","userId":"13663744920024374425"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","execution_count":7,"metadata":{"id":"a-gFoy-oXtX5","executionInfo":{"status":"ok","timestamp":1689005619491,"user_tz":-210,"elapsed":2,"user":{"displayName":"Mahdi Salehi","userId":"13663744920024374425"}}},"outputs":[],"source":["train_dataloader, val_dataloader, test_dataloader = loader(batch_size=32)"]},{"cell_type":"markdown","metadata":{"id":"PLPEIlKKXtX5"},"source":["#### Defining A GNN"]},{"cell_type":"markdown","metadata":{"id":"DVtYqdKrXtX5"},"source":["##### Some Variables"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"dRL8fXUMXtX6","executionInfo":{"status":"ok","timestamp":1689005621649,"user_tz":-210,"elapsed":4,"user":{"displayName":"Mahdi Salehi","userId":"13663744920024374425"}}},"outputs":[],"source":["#FreeSolv dataset has 1 task. Some other datasets may have some more number of tasks, e.g., tox21 has 12 tasks.\n","num_tasks = 1\n","\n","# Size of global feature of each graph\n","global_size = 200\n","\n","# Number of epochs to train the model\n","num_epochs = 100\n","\n","# Number of steps to wait if the model performance on the validation set does not improve\n","patience = 10\n","\n","#Configurations to instantiate the model\n","config = {\"node_feature_size\":127, \"edge_feature_size\":12, \"hidden_size\":100}\n"]},{"cell_type":"markdown","source":["#GNN 1"],"metadata":{"id":"w0eFHz6qPAZD"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"wgMMKVRcXtX6"},"outputs":[],"source":["class GNN1(nn.Module):\n","    def __init__(self, config, global_size = 200, num_tasks = 1):\n","        super().__init__()\n","        self.config = config\n","        self.num_tasks = num_tasks\n","\n","        # Node feature size\n","        self.node_feature_size = self.config.get('node_feature_size', 127)\n","\n","        # Edge feature size\n","        self.edge_feature_size = self.config.get('edge_feature_size', 12)\n","\n","        # Hidden size\n","        self.hidden_size = self.config.get('hidden_size', 100)\n","\n","        self.conv1 = GraphConv(self.node_feature_size, self.hidden_size,allow_zero_in_degree=True)\n","        self.conv2 = GraphConv(self.hidden_size, self.num_tasks,allow_zero_in_degree=True)\n","\n","    # def forward(self, g, in_feat):\n","    def forward(self, mol_dgl_graph, globals):\n","        mol_dgl_graph.ndata[\"v\"]= mol_dgl_graph.ndata[\"v\"][:,:self.node_feature_size]\n","        mol_dgl_graph.edata[\"e\"] = mol_dgl_graph.edata[\"e\"][:,:self.edge_feature_size]\n","        h = self.conv1(mol_dgl_graph, mol_dgl_graph.ndata[\"v\"])\n","        h = F.relu(h)\n","        h = self.conv2(mol_dgl_graph, h)\n","        mol_dgl_graph.ndata[\"h\"] = h\n","        return dgl.mean_nodes(mol_dgl_graph, \"h\")"]},{"cell_type":"markdown","source":["#Function to Compute Score of the Model"],"metadata":{"id":"jMeJPThSe-Ok"}},{"cell_type":"code","source":["import math\n","from sklearn.preprocessing import StandardScaler\n","\n","def compute_score(model, data_loader, scaler, val_size, num_tasks=1):\n","    model.eval()\n","    loss_mean = nn.MSELoss(reduction='sum') # MSE with sum instead of mean, i.e., sum_i[(y_i)^2-(y'_i)^2]\n","    final_loss = 0\n","    if not isinstance(scaler, StandardScaler):\n","        scaler = StandardScaler()\n","        scaler.fit(data_loader.dataset.labels.numpy())\n","\n","    with torch.no_grad():\n","        for i, (mol_dgl_graph, labels, masks, globals) in enumerate(data_loader):\n","            prediction = model(mol_dgl_graph, globals)\n","            if scaler is not None:\n","                prediction = torch.tensor(scaler.inverse_transform(prediction.detach().cpu()))\n","                labels = torch.tensor(scaler.inverse_transform(labels.cpu()))\n","            loss = loss_mean(prediction, labels)\n","            final_loss += loss.item()\n","        final_loss /= val_size\n","        final_loss = math.sqrt(final_loss) # RMSE\n","    return final_loss / num_tasks"],"metadata":{"id":"w5Whw1I4L7LC","executionInfo":{"status":"ok","timestamp":1689005633268,"user_tz":-210,"elapsed":432,"user":{"displayName":"Mahdi Salehi","userId":"13663744920024374425"}}},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":["#Loss Function"],"metadata":{"id":"ox4Tpr1G1StG"}},{"cell_type":"code","source":["def loss_func(output, label, mask, num_tasks):\n","    pos_weight = torch.ones((1, num_tasks))\n","    criterion = nn.MSELoss(reduction='none')\n","    loss = mask*criterion(output,label)\n","    loss = loss.sum() / mask.sum()\n","    return loss"],"metadata":{"id":"qUWh0Veu09jf","executionInfo":{"status":"ok","timestamp":1689005634189,"user_tz":-210,"elapsed":3,"user":{"displayName":"Mahdi Salehi","userId":"13663744920024374425"}}},"execution_count":10,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MznGLA38XtX8"},"source":["#### Training and Evaluation"]},{"cell_type":"markdown","metadata":{"id":"IyULqGodXtX9"},"source":["##### Training Function"]},{"cell_type":"code","source":["def train_epoch(train_dataloader, model, optimizer, num_tasks):\n","    epoch_train_loss = 0\n","    iterations = 0\n","    model.train() # Prepare model for training\n","    for i, (mol_dgl_graph, labels, masks, globals) in enumerate(train_dataloader):\n","        optimizer.zero_grad(set_to_none=True)\n","        prediction = model(mol_dgl_graph, globals)\n","        loss_train = loss_func(prediction, labels, masks, num_tasks)\n","        loss_train.backward()\n","        optimizer.step()\n","        epoch_train_loss += loss_train.detach().item()\n","        iterations += 1\n","    epoch_train_loss /= iterations\n","    return epoch_train_loss"],"metadata":{"id":"a7-o8TscSNuY","executionInfo":{"status":"ok","timestamp":1689005635708,"user_tz":-210,"elapsed":2,"user":{"displayName":"Mahdi Salehi","userId":"13663744920024374425"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["def train_evaluate():\n","\n","    model = GNN1(config, global_size, num_tasks)\n","    optimizer = torch.optim.Adam(model.parameters(), lr = 0.001)\n","\n","    best_val = float('inf')\n","    patience_count = 1\n","    epoch = 1\n","\n","    while epoch <= num_epochs:\n","        if patience_count <= patience:\n","            model.train()\n","            loss_train = train_epoch(train_dataloader, model, optimizer, num_tasks)\n","            model.eval()\n","            score_val = compute_score(model, val_dataloader, scaler, len(val_set), num_tasks)\n","            if score_val < best_val:\n","                best_val = score_val\n","                print(\"Save checkpoint\")\n","                path = os.path.join(checkpoint_path, 'checkpoint.pth')\n","                dict_checkpoint = {\"score_val\": score_val}\n","                dict_checkpoint.update({\"model_state_dict\": model.state_dict(), \"optimizer_state\": optimizer.state_dict()})\n","                with open(path, \"wb\") as outputfile:\n","                    cloudpickle.dump(dict_checkpoint, outputfile)\n","                patience_count = 1\n","            else:\n","                print(\"Patience\", patience_count)\n","                patience_count += 1\n","\n","            print(\"Epoch: {}/{} | Training Loss: {:.3f} | Valid Score: {:.3f}\".format(\n","            epoch, num_epochs, loss_train, score_val))\n","\n","            print(\" \")\n","            print(\"Epoch: {}/{} | Best Valid Score Until Now: {:.3f}\".format(epoch, num_epochs, best_val), \"\\n\")\n","        epoch += 1\n","\n","    # best model save\n","    shutil.rmtree(best_model_path, ignore_errors=True)\n","    shutil.copytree(checkpoint_path, best_model_path)\n","\n","    print(\"Final results:\")\n","    print(\"Average Valid Score: {:.3f}\".format(best_val), \"\\n\")\n"],"metadata":{"id":"72m46wHkSclW","executionInfo":{"status":"ok","timestamp":1689005636287,"user_tz":-210,"elapsed":1,"user":{"displayName":"Mahdi Salehi","userId":"13663744920024374425"}}},"execution_count":12,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8mC9VeiZXtX-"},"source":["##### Function to compute test set score of the final saved model"]},{"cell_type":"code","source":["def test_evaluate():\n","    final_model = GNN1(config, global_size, num_tasks)\n","    path = os.path.join(best_model_path, 'checkpoint.pth')\n","    with open(path, 'rb') as f:\n","        checkpoint = cloudpickle.load(f)\n","    final_model.load_state_dict(checkpoint[\"model_state_dict\"])\n","    final_model.eval()\n","    test_score = compute_score(final_model, test_dataloader, scaler, len(test_set), num_tasks)\n","\n","    print(\"Test Score: {:.3f}\".format(test_score), \"\\n\")\n","    print(\"Execution time: {:.3f} seconds\".format(time.time() - start_time))"],"metadata":{"id":"ZCl8zpMsSycE","executionInfo":{"status":"ok","timestamp":1689005640231,"user_tz":-210,"elapsed":10,"user":{"displayName":"Mahdi Salehi","userId":"13663744920024374425"}}},"execution_count":13,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nUaNqb0oXtX-"},"source":["##### Train the model and evaluate its performance"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oObkdncJXtX-","outputId":"d69f84b0-9800-4c40-e2aa-fca538d88dc0","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1688550355052,"user_tz":-210,"elapsed":9711,"user":{"displayName":"Mahdi Salehi","userId":"13663744920024374425"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Save checkpoint\n","Epoch: 1/100 | Training Loss: 0.852 | Valid Score: 4.304\n"," \n","Epoch: 1/100 | Best Valid Score Until Now: 4.304 \n","\n","Save checkpoint\n","Epoch: 2/100 | Training Loss: 0.762 | Valid Score: 3.998\n"," \n","Epoch: 2/100 | Best Valid Score Until Now: 3.998 \n","\n","Patience 1\n","Epoch: 3/100 | Training Loss: 0.696 | Valid Score: 4.112\n"," \n","Epoch: 3/100 | Best Valid Score Until Now: 3.998 \n","\n","Patience 2\n","Epoch: 4/100 | Training Loss: 0.620 | Valid Score: 4.084\n"," \n","Epoch: 4/100 | Best Valid Score Until Now: 3.998 \n","\n","Save checkpoint\n","Epoch: 5/100 | Training Loss: 0.875 | Valid Score: 3.970\n"," \n","Epoch: 5/100 | Best Valid Score Until Now: 3.970 \n","\n","Save checkpoint\n","Epoch: 6/100 | Training Loss: 0.592 | Valid Score: 3.950\n"," \n","Epoch: 6/100 | Best Valid Score Until Now: 3.950 \n","\n","Patience 1\n","Epoch: 7/100 | Training Loss: 0.522 | Valid Score: 4.056\n"," \n","Epoch: 7/100 | Best Valid Score Until Now: 3.950 \n","\n","Patience 2\n","Epoch: 8/100 | Training Loss: 0.494 | Valid Score: 4.028\n"," \n","Epoch: 8/100 | Best Valid Score Until Now: 3.950 \n","\n","Patience 3\n","Epoch: 9/100 | Training Loss: 0.461 | Valid Score: 4.082\n"," \n","Epoch: 9/100 | Best Valid Score Until Now: 3.950 \n","\n","Patience 4\n","Epoch: 10/100 | Training Loss: 0.468 | Valid Score: 4.130\n"," \n","Epoch: 10/100 | Best Valid Score Until Now: 3.950 \n","\n","Patience 5\n","Epoch: 11/100 | Training Loss: 0.441 | Valid Score: 4.180\n"," \n","Epoch: 11/100 | Best Valid Score Until Now: 3.950 \n","\n","Patience 6\n","Epoch: 12/100 | Training Loss: 0.450 | Valid Score: 4.157\n"," \n","Epoch: 12/100 | Best Valid Score Until Now: 3.950 \n","\n","Patience 7\n","Epoch: 13/100 | Training Loss: 0.416 | Valid Score: 4.233\n"," \n","Epoch: 13/100 | Best Valid Score Until Now: 3.950 \n","\n","Patience 8\n","Epoch: 14/100 | Training Loss: 0.412 | Valid Score: 4.311\n"," \n","Epoch: 14/100 | Best Valid Score Until Now: 3.950 \n","\n","Patience 9\n","Epoch: 15/100 | Training Loss: 0.397 | Valid Score: 4.255\n"," \n","Epoch: 15/100 | Best Valid Score Until Now: 3.950 \n","\n","Patience 10\n","Epoch: 16/100 | Training Loss: 0.416 | Valid Score: 4.175\n"," \n","Epoch: 16/100 | Best Valid Score Until Now: 3.950 \n","\n","Final results:\n","Average Valid Score: 3.950 \n","\n","Test Score: 3.221 \n","\n","Execution time: 9.488 seconds\n"]}],"source":["import time\n","start_time = time.time()\n","\n","scaler = StandardScaler()\n","train_labels = train_set.labels.numpy()\n","scaler.fit(train_labels)\n","\n","train_evaluate()\n","test_evaluate()"]},{"cell_type":"markdown","source":["#GNN 2"],"metadata":{"id":"pNf7ilqnPGsl"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"4NBLj8xyPEcJ"},"outputs":[],"source":["class GNN2(nn.Module):\n","    def __init__(self, config, global_size = 200, num_tasks = 1):\n","        super().__init__()\n","        self.config = config\n","        self.num_tasks = num_tasks\n","\n","        # Node feature size\n","        self.node_feature_size = self.config.get('node_feature_size', 127)\n","\n","        # Edge feature size\n","        self.edge_feature_size = self.config.get('edge_feature_size', 12)\n","\n","        # Hidden size\n","        self.hidden_size = self.config.get('hidden_size', 100)\n","\n","        self.conv1 = GraphConv(self.node_feature_size, self.hidden_size,allow_zero_in_degree=True)\n","        self.conv2 = GraphConv(self.hidden_size, self.hidden_size,allow_zero_in_degree=True)\n","        self.conv3 = GraphConv(self.hidden_size, self.num_tasks,allow_zero_in_degree=True)\n","\n","    # def forward(self, g, in_feat):\n","    def forward(self, mol_dgl_graph, globals):\n","        mol_dgl_graph.ndata[\"v\"]= mol_dgl_graph.ndata[\"v\"][:,:self.node_feature_size]\n","        mol_dgl_graph.edata[\"e\"] = mol_dgl_graph.edata[\"e\"][:,:self.edge_feature_size]\n","        h = self.conv1(mol_dgl_graph, mol_dgl_graph.ndata[\"v\"])\n","        h = F.relu(h)\n","        h = self.conv2(mol_dgl_graph, h)\n","        h = F.relu(h)\n","        h = self.conv3(mol_dgl_graph, h)\n","        mol_dgl_graph.ndata[\"h\"] = h\n","        return dgl.mean_nodes(mol_dgl_graph, \"h\")"]},{"cell_type":"code","source":["def train_evaluate():\n","\n","    model = GNN2(config, global_size, num_tasks)\n","    optimizer = torch.optim.Adam(model.parameters(), lr = 0.001)\n","\n","    best_val = float('inf')\n","    patience_count = 1\n","    epoch = 1\n","\n","    while epoch <= num_epochs:\n","        if patience_count <= patience:\n","            model.train()\n","            loss_train = train_epoch(train_dataloader, model, optimizer, num_tasks)\n","            model.eval()\n","            score_val = compute_score(model, val_dataloader, scaler, len(val_set), num_tasks)\n","            if score_val < best_val:\n","                best_val = score_val\n","                print(\"Save checkpoint\")\n","                path = os.path.join(checkpoint_path, 'checkpoint.pth')\n","                dict_checkpoint = {\"score_val\": score_val}\n","                dict_checkpoint.update({\"model_state_dict\": model.state_dict(), \"optimizer_state\": optimizer.state_dict()})\n","                with open(path, \"wb\") as outputfile:\n","                    cloudpickle.dump(dict_checkpoint, outputfile)\n","                patience_count = 1\n","            else:\n","                print(\"Patience\", patience_count)\n","                patience_count += 1\n","\n","            print(\"Epoch: {}/{} | Training Loss: {:.3f} | Valid Score: {:.3f}\".format(\n","            epoch, num_epochs, loss_train, score_val))\n","\n","            print(\" \")\n","            print(\"Epoch: {}/{} | Best Valid Score Until Now: {:.3f}\".format(epoch, num_epochs, best_val), \"\\n\")\n","        epoch += 1\n","\n","    # best model save\n","    shutil.rmtree(best_model_path, ignore_errors=True)\n","    shutil.copytree(checkpoint_path, best_model_path)\n","\n","    print(\"Final results:\")\n","    print(\"Average Valid Score: {:.3f}\".format(best_val), \"\\n\")\n"],"metadata":{"id":"Fsfuv23MT4cJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"h1RTTRK1T4cJ"},"source":["##### Function to compute test set score of the final saved model"]},{"cell_type":"code","source":["def test_evaluate():\n","    final_model = GNN2(config, global_size, num_tasks)\n","    path = os.path.join(best_model_path, 'checkpoint.pth')\n","    with open(path, 'rb') as f:\n","        checkpoint = cloudpickle.load(f)\n","    final_model.load_state_dict(checkpoint[\"model_state_dict\"])\n","    final_model.eval()\n","    test_score = compute_score(final_model, test_dataloader, scaler, len(test_set), num_tasks)\n","\n","    print(\"Test Score: {:.3f}\".format(test_score), \"\\n\")\n","    print(\"Execution time: {:.3f} seconds\".format(time.time() - start_time))"],"metadata":{"id":"YUjzIlFwT4cJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Y1umRPF1T4cJ"},"source":["##### Train the model and evaluate its performance"]},{"cell_type":"code","execution_count":null,"metadata":{"outputId":"13943898-c44c-463e-94fd-358c8dd0417b","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1688550220365,"user_tz":-210,"elapsed":13787,"user":{"displayName":"Mahdi Salehi","userId":"13663744920024374425"}},"id":"5ZW_3eNpT4cK"},"outputs":[{"output_type":"stream","name":"stdout","text":["Save checkpoint\n","Epoch: 1/100 | Training Loss: 0.839 | Valid Score: 4.125\n"," \n","Epoch: 1/100 | Best Valid Score Until Now: 4.125 \n","\n","Save checkpoint\n","Epoch: 2/100 | Training Loss: 0.708 | Valid Score: 3.942\n"," \n","Epoch: 2/100 | Best Valid Score Until Now: 3.942 \n","\n","Patience 1\n","Epoch: 3/100 | Training Loss: 0.645 | Valid Score: 3.963\n"," \n","Epoch: 3/100 | Best Valid Score Until Now: 3.942 \n","\n","Save checkpoint\n","Epoch: 4/100 | Training Loss: 0.565 | Valid Score: 3.875\n"," \n","Epoch: 4/100 | Best Valid Score Until Now: 3.875 \n","\n","Patience 1\n","Epoch: 5/100 | Training Loss: 0.534 | Valid Score: 3.880\n"," \n","Epoch: 5/100 | Best Valid Score Until Now: 3.875 \n","\n","Save checkpoint\n","Epoch: 6/100 | Training Loss: 0.569 | Valid Score: 3.820\n"," \n","Epoch: 6/100 | Best Valid Score Until Now: 3.820 \n","\n","Save checkpoint\n","Epoch: 7/100 | Training Loss: 0.481 | Valid Score: 3.761\n"," \n","Epoch: 7/100 | Best Valid Score Until Now: 3.761 \n","\n","Save checkpoint\n","Epoch: 8/100 | Training Loss: 0.486 | Valid Score: 3.747\n"," \n","Epoch: 8/100 | Best Valid Score Until Now: 3.747 \n","\n","Patience 1\n","Epoch: 9/100 | Training Loss: 0.456 | Valid Score: 3.841\n"," \n","Epoch: 9/100 | Best Valid Score Until Now: 3.747 \n","\n","Patience 2\n","Epoch: 10/100 | Training Loss: 0.452 | Valid Score: 3.836\n"," \n","Epoch: 10/100 | Best Valid Score Until Now: 3.747 \n","\n","Patience 3\n","Epoch: 11/100 | Training Loss: 0.446 | Valid Score: 3.753\n"," \n","Epoch: 11/100 | Best Valid Score Until Now: 3.747 \n","\n","Save checkpoint\n","Epoch: 12/100 | Training Loss: 0.440 | Valid Score: 3.724\n"," \n","Epoch: 12/100 | Best Valid Score Until Now: 3.724 \n","\n","Patience 1\n","Epoch: 13/100 | Training Loss: 0.416 | Valid Score: 3.776\n"," \n","Epoch: 13/100 | Best Valid Score Until Now: 3.724 \n","\n","Patience 2\n","Epoch: 14/100 | Training Loss: 0.399 | Valid Score: 3.831\n"," \n","Epoch: 14/100 | Best Valid Score Until Now: 3.724 \n","\n","Patience 3\n","Epoch: 15/100 | Training Loss: 0.419 | Valid Score: 3.815\n"," \n","Epoch: 15/100 | Best Valid Score Until Now: 3.724 \n","\n","Patience 4\n","Epoch: 16/100 | Training Loss: 0.413 | Valid Score: 3.919\n"," \n","Epoch: 16/100 | Best Valid Score Until Now: 3.724 \n","\n","Patience 5\n","Epoch: 17/100 | Training Loss: 0.427 | Valid Score: 3.897\n"," \n","Epoch: 17/100 | Best Valid Score Until Now: 3.724 \n","\n","Patience 6\n","Epoch: 18/100 | Training Loss: 0.390 | Valid Score: 4.035\n"," \n","Epoch: 18/100 | Best Valid Score Until Now: 3.724 \n","\n","Patience 7\n","Epoch: 19/100 | Training Loss: 0.543 | Valid Score: 3.996\n"," \n","Epoch: 19/100 | Best Valid Score Until Now: 3.724 \n","\n","Patience 8\n","Epoch: 20/100 | Training Loss: 0.366 | Valid Score: 3.958\n"," \n","Epoch: 20/100 | Best Valid Score Until Now: 3.724 \n","\n","Patience 9\n","Epoch: 21/100 | Training Loss: 0.352 | Valid Score: 4.196\n"," \n","Epoch: 21/100 | Best Valid Score Until Now: 3.724 \n","\n","Patience 10\n","Epoch: 22/100 | Training Loss: 0.376 | Valid Score: 4.053\n"," \n","Epoch: 22/100 | Best Valid Score Until Now: 3.724 \n","\n","Final results:\n","Average Valid Score: 3.724 \n","\n","Test Score: 3.160 \n","\n","Execution time: 12.949 seconds\n"]}],"source":["import time\n","start_time = time.time()\n","\n","scaler = StandardScaler()\n","train_labels = train_set.labels.numpy()\n","scaler.fit(train_labels)\n","\n","train_evaluate()\n","test_evaluate()"]},{"cell_type":"markdown","source":["#GNN 3"],"metadata":{"id":"MBZib291aMgc"}},{"cell_type":"code","source":["class GNN3(nn.Module):\n","    def __init__(self, config, global_size = 200, num_tasks = 1):\n","        super().__init__()\n","        self.config = config\n","        self.num_tasks = num_tasks\n","\n","        # Node feature size\n","        self.node_feature_size = self.config.get('node_feature_size', 127)\n","\n","        # Edge feature size\n","        self.edge_feature_size = self.config.get('edge_feature_size', 12)\n","\n","        # Hidden size\n","        self.hidden_size = self.config.get('hidden_size', 100)\n","\n","\n","        self.conv1 = GraphConv(self.node_feature_size, 64 ,allow_zero_in_degree=True)\n","        self.conv2 = GraphConv(64, 128,allow_zero_in_degree=True)\n","        self.conv3 = GraphConv(128, 256,allow_zero_in_degree=True)\n","        self.conv4 = GraphConv(256, 128,allow_zero_in_degree=True)\n","        self.conv5 = GraphConv(128, self.num_tasks,allow_zero_in_degree=True)\n","\n","        self.dropout = nn.Dropout(p=0.0)\n","        #self.bn1 = nn.BatchNorm1d(64)\n","        #self.bn2 = nn.BatchNorm1d(128)\n","        #self.bn3 = nn.BatchNorm1d(256)\n","\n","    # def forward(self, g, in_feat):\n","    def forward(self, mol_dgl_graph, globals):\n","        mol_dgl_graph.ndata[\"v\"]= mol_dgl_graph.ndata[\"v\"][:,:self.node_feature_size]\n","        mol_dgl_graph.edata[\"e\"] = mol_dgl_graph.edata[\"e\"][:,:self.edge_feature_size]\n","        h = self.conv1(mol_dgl_graph, mol_dgl_graph.ndata[\"v\"])\n","        h = F.relu(h)\n","        #h = self.bn1(h)\n","        h = self.conv2(mol_dgl_graph, h)\n","        h = F.relu(h)\n","        h = self.dropout(h)\n","        #h = self.bn2(h)\n","        h = self.conv3(mol_dgl_graph, h)\n","        h = F.relu(h)\n","        h = self.dropout(h)\n","        #h = self.bn3(h)\n","        h = self.conv4(mol_dgl_graph, h)\n","        h = F.relu(h)\n","        #h = self.bn2(h)\n","        h = self.conv5(mol_dgl_graph, h)\n","        mol_dgl_graph.ndata[\"h\"] = h\n","        return dgl.mean_nodes(mol_dgl_graph, \"h\")"],"metadata":{"id":"NZk2d4EJRKm7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def train_evaluate():\n","\n","    model = GNN3(config, global_size, num_tasks)\n","    optimizer = torch.optim.Adam(model.parameters(), lr = 0.001)\n","\n","    best_val = float('inf')\n","    patience_count = 1\n","    epoch = 1\n","\n","    while epoch <= num_epochs:\n","        if patience_count <= patience:\n","            model.train()\n","            loss_train = train_epoch(train_dataloader, model, optimizer, num_tasks)\n","            model.eval()\n","            score_val = compute_score(model, val_dataloader, scaler, len(val_set), num_tasks)\n","            if score_val < best_val:\n","                best_val = score_val\n","                print(\"Save checkpoint\")\n","                path = os.path.join(checkpoint_path, 'checkpoint.pth')\n","                dict_checkpoint = {\"score_val\": score_val}\n","                dict_checkpoint.update({\"model_state_dict\": model.state_dict(), \"optimizer_state\": optimizer.state_dict()})\n","                with open(path, \"wb\") as outputfile:\n","                    cloudpickle.dump(dict_checkpoint, outputfile)\n","                patience_count = 1\n","            else:\n","                print(\"Patience\", patience_count)\n","                patience_count += 1\n","\n","            print(\"Epoch: {}/{} | Training Loss: {:.3f} | Valid Score: {:.3f}\".format(\n","            epoch, num_epochs, loss_train, score_val))\n","\n","            print(\" \")\n","            print(\"Epoch: {}/{} | Best Valid Score Until Now: {:.3f}\".format(epoch, num_epochs, best_val), \"\\n\")\n","        epoch += 1\n","\n","    # best model save\n","    shutil.rmtree(best_model_path, ignore_errors=True)\n","    shutil.copytree(checkpoint_path, best_model_path)\n","\n","    print(\"Final results:\")\n","    print(\"Average Valid Score: {:.3f}\".format(best_val), \"\\n\")\n"],"metadata":{"id":"vQGODNNMc47_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qcNeBDpFc47_"},"source":["##### Function to compute test set score of the final saved model"]},{"cell_type":"code","source":["def test_evaluate():\n","    final_model = GNN3(config, global_size, num_tasks)\n","    path = os.path.join(best_model_path, 'checkpoint.pth')\n","    with open(path, 'rb') as f:\n","        checkpoint = cloudpickle.load(f)\n","    final_model.load_state_dict(checkpoint[\"model_state_dict\"])\n","    final_model.eval()\n","    test_score = compute_score(final_model, test_dataloader, scaler, len(test_set), num_tasks)\n","\n","    print(\"Test Score: {:.3f}\".format(test_score), \"\\n\")\n","    print(\"Execution time: {:.3f} seconds\".format(time.time() - start_time))"],"metadata":{"id":"-9Hoe1Tzc48A"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SRMu9zljc48A"},"source":["##### Train the model and evaluate its performance"]},{"cell_type":"code","execution_count":null,"metadata":{"outputId":"d5e45ca9-ca7e-4f0e-f131-7340e347cd23","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1688550599580,"user_tz":-210,"elapsed":42249,"user":{"displayName":"Mahdi Salehi","userId":"13663744920024374425"}},"id":"tMqYYX9jc48A"},"outputs":[{"output_type":"stream","name":"stdout","text":["Save checkpoint\n","Epoch: 1/100 | Training Loss: 0.895 | Valid Score: 4.237\n"," \n","Epoch: 1/100 | Best Valid Score Until Now: 4.237 \n","\n","Patience 1\n","Epoch: 2/100 | Training Loss: 0.684 | Valid Score: 4.292\n"," \n","Epoch: 2/100 | Best Valid Score Until Now: 4.237 \n","\n","Save checkpoint\n","Epoch: 3/100 | Training Loss: 0.642 | Valid Score: 3.829\n"," \n","Epoch: 3/100 | Best Valid Score Until Now: 3.829 \n","\n","Save checkpoint\n","Epoch: 4/100 | Training Loss: 0.573 | Valid Score: 3.690\n"," \n","Epoch: 4/100 | Best Valid Score Until Now: 3.690 \n","\n","Patience 1\n","Epoch: 5/100 | Training Loss: 0.488 | Valid Score: 4.077\n"," \n","Epoch: 5/100 | Best Valid Score Until Now: 3.690 \n","\n","Save checkpoint\n","Epoch: 6/100 | Training Loss: 0.542 | Valid Score: 3.680\n"," \n","Epoch: 6/100 | Best Valid Score Until Now: 3.680 \n","\n","Save checkpoint\n","Epoch: 7/100 | Training Loss: 0.467 | Valid Score: 3.674\n"," \n","Epoch: 7/100 | Best Valid Score Until Now: 3.674 \n","\n","Patience 1\n","Epoch: 8/100 | Training Loss: 0.452 | Valid Score: 3.750\n"," \n","Epoch: 8/100 | Best Valid Score Until Now: 3.674 \n","\n","Save checkpoint\n","Epoch: 9/100 | Training Loss: 0.618 | Valid Score: 3.674\n"," \n","Epoch: 9/100 | Best Valid Score Until Now: 3.674 \n","\n","Patience 1\n","Epoch: 10/100 | Training Loss: 0.454 | Valid Score: 3.716\n"," \n","Epoch: 10/100 | Best Valid Score Until Now: 3.674 \n","\n","Patience 2\n","Epoch: 11/100 | Training Loss: 0.438 | Valid Score: 3.772\n"," \n","Epoch: 11/100 | Best Valid Score Until Now: 3.674 \n","\n","Patience 3\n","Epoch: 12/100 | Training Loss: 0.386 | Valid Score: 3.945\n"," \n","Epoch: 12/100 | Best Valid Score Until Now: 3.674 \n","\n","Patience 4\n","Epoch: 13/100 | Training Loss: 0.368 | Valid Score: 3.838\n"," \n","Epoch: 13/100 | Best Valid Score Until Now: 3.674 \n","\n","Patience 5\n","Epoch: 14/100 | Training Loss: 0.357 | Valid Score: 3.758\n"," \n","Epoch: 14/100 | Best Valid Score Until Now: 3.674 \n","\n","Save checkpoint\n","Epoch: 15/100 | Training Loss: 0.377 | Valid Score: 3.621\n"," \n","Epoch: 15/100 | Best Valid Score Until Now: 3.621 \n","\n","Patience 1\n","Epoch: 16/100 | Training Loss: 0.605 | Valid Score: 3.675\n"," \n","Epoch: 16/100 | Best Valid Score Until Now: 3.621 \n","\n","Save checkpoint\n","Epoch: 17/100 | Training Loss: 0.394 | Valid Score: 3.487\n"," \n","Epoch: 17/100 | Best Valid Score Until Now: 3.487 \n","\n","Save checkpoint\n","Epoch: 18/100 | Training Loss: 0.360 | Valid Score: 3.141\n"," \n","Epoch: 18/100 | Best Valid Score Until Now: 3.141 \n","\n","Patience 1\n","Epoch: 19/100 | Training Loss: 0.326 | Valid Score: 3.500\n"," \n","Epoch: 19/100 | Best Valid Score Until Now: 3.141 \n","\n","Patience 2\n","Epoch: 20/100 | Training Loss: 0.315 | Valid Score: 3.349\n"," \n","Epoch: 20/100 | Best Valid Score Until Now: 3.141 \n","\n","Patience 3\n","Epoch: 21/100 | Training Loss: 0.310 | Valid Score: 3.541\n"," \n","Epoch: 21/100 | Best Valid Score Until Now: 3.141 \n","\n","Patience 4\n","Epoch: 22/100 | Training Loss: 0.314 | Valid Score: 3.540\n"," \n","Epoch: 22/100 | Best Valid Score Until Now: 3.141 \n","\n","Patience 5\n","Epoch: 23/100 | Training Loss: 0.294 | Valid Score: 3.315\n"," \n","Epoch: 23/100 | Best Valid Score Until Now: 3.141 \n","\n","Patience 6\n","Epoch: 24/100 | Training Loss: 0.274 | Valid Score: 3.515\n"," \n","Epoch: 24/100 | Best Valid Score Until Now: 3.141 \n","\n","Patience 7\n","Epoch: 25/100 | Training Loss: 0.304 | Valid Score: 3.548\n"," \n","Epoch: 25/100 | Best Valid Score Until Now: 3.141 \n","\n","Save checkpoint\n","Epoch: 26/100 | Training Loss: 0.338 | Valid Score: 3.093\n"," \n","Epoch: 26/100 | Best Valid Score Until Now: 3.093 \n","\n","Patience 1\n","Epoch: 27/100 | Training Loss: 0.303 | Valid Score: 3.772\n"," \n","Epoch: 27/100 | Best Valid Score Until Now: 3.093 \n","\n","Save checkpoint\n","Epoch: 28/100 | Training Loss: 0.375 | Valid Score: 2.885\n"," \n","Epoch: 28/100 | Best Valid Score Until Now: 2.885 \n","\n","Patience 1\n","Epoch: 29/100 | Training Loss: 0.375 | Valid Score: 3.419\n"," \n","Epoch: 29/100 | Best Valid Score Until Now: 2.885 \n","\n","Patience 2\n","Epoch: 30/100 | Training Loss: 0.340 | Valid Score: 3.268\n"," \n","Epoch: 30/100 | Best Valid Score Until Now: 2.885 \n","\n","Patience 3\n","Epoch: 31/100 | Training Loss: 0.272 | Valid Score: 3.405\n"," \n","Epoch: 31/100 | Best Valid Score Until Now: 2.885 \n","\n","Patience 4\n","Epoch: 32/100 | Training Loss: 0.284 | Valid Score: 3.022\n"," \n","Epoch: 32/100 | Best Valid Score Until Now: 2.885 \n","\n","Patience 5\n","Epoch: 33/100 | Training Loss: 0.276 | Valid Score: 3.625\n"," \n","Epoch: 33/100 | Best Valid Score Until Now: 2.885 \n","\n","Patience 6\n","Epoch: 34/100 | Training Loss: 0.277 | Valid Score: 3.041\n"," \n","Epoch: 34/100 | Best Valid Score Until Now: 2.885 \n","\n","Patience 7\n","Epoch: 35/100 | Training Loss: 0.250 | Valid Score: 3.211\n"," \n","Epoch: 35/100 | Best Valid Score Until Now: 2.885 \n","\n","Patience 8\n","Epoch: 36/100 | Training Loss: 0.229 | Valid Score: 2.891\n"," \n","Epoch: 36/100 | Best Valid Score Until Now: 2.885 \n","\n","Save checkpoint\n","Epoch: 37/100 | Training Loss: 0.317 | Valid Score: 2.826\n"," \n","Epoch: 37/100 | Best Valid Score Until Now: 2.826 \n","\n","Patience 1\n","Epoch: 38/100 | Training Loss: 0.217 | Valid Score: 2.880\n"," \n","Epoch: 38/100 | Best Valid Score Until Now: 2.826 \n","\n","Patience 2\n","Epoch: 39/100 | Training Loss: 0.208 | Valid Score: 2.952\n"," \n","Epoch: 39/100 | Best Valid Score Until Now: 2.826 \n","\n","Save checkpoint\n","Epoch: 40/100 | Training Loss: 0.204 | Valid Score: 2.779\n"," \n","Epoch: 40/100 | Best Valid Score Until Now: 2.779 \n","\n","Patience 1\n","Epoch: 41/100 | Training Loss: 0.213 | Valid Score: 2.951\n"," \n","Epoch: 41/100 | Best Valid Score Until Now: 2.779 \n","\n","Patience 2\n","Epoch: 42/100 | Training Loss: 0.198 | Valid Score: 2.953\n"," \n","Epoch: 42/100 | Best Valid Score Until Now: 2.779 \n","\n","Save checkpoint\n","Epoch: 43/100 | Training Loss: 0.199 | Valid Score: 2.599\n"," \n","Epoch: 43/100 | Best Valid Score Until Now: 2.599 \n","\n","Patience 1\n","Epoch: 44/100 | Training Loss: 0.205 | Valid Score: 2.675\n"," \n","Epoch: 44/100 | Best Valid Score Until Now: 2.599 \n","\n","Save checkpoint\n","Epoch: 45/100 | Training Loss: 0.218 | Valid Score: 2.561\n"," \n","Epoch: 45/100 | Best Valid Score Until Now: 2.561 \n","\n","Patience 1\n","Epoch: 46/100 | Training Loss: 0.218 | Valid Score: 2.863\n"," \n","Epoch: 46/100 | Best Valid Score Until Now: 2.561 \n","\n","Patience 2\n","Epoch: 47/100 | Training Loss: 0.271 | Valid Score: 2.674\n"," \n","Epoch: 47/100 | Best Valid Score Until Now: 2.561 \n","\n","Save checkpoint\n","Epoch: 48/100 | Training Loss: 0.192 | Valid Score: 2.500\n"," \n","Epoch: 48/100 | Best Valid Score Until Now: 2.500 \n","\n","Patience 1\n","Epoch: 49/100 | Training Loss: 0.291 | Valid Score: 3.584\n"," \n","Epoch: 49/100 | Best Valid Score Until Now: 2.500 \n","\n","Patience 2\n","Epoch: 50/100 | Training Loss: 0.494 | Valid Score: 3.392\n"," \n","Epoch: 50/100 | Best Valid Score Until Now: 2.500 \n","\n","Patience 3\n","Epoch: 51/100 | Training Loss: 0.300 | Valid Score: 2.903\n"," \n","Epoch: 51/100 | Best Valid Score Until Now: 2.500 \n","\n","Patience 4\n","Epoch: 52/100 | Training Loss: 0.227 | Valid Score: 2.678\n"," \n","Epoch: 52/100 | Best Valid Score Until Now: 2.500 \n","\n","Patience 5\n","Epoch: 53/100 | Training Loss: 0.208 | Valid Score: 2.961\n"," \n","Epoch: 53/100 | Best Valid Score Until Now: 2.500 \n","\n","Patience 6\n","Epoch: 54/100 | Training Loss: 0.196 | Valid Score: 2.645\n"," \n","Epoch: 54/100 | Best Valid Score Until Now: 2.500 \n","\n","Save checkpoint\n","Epoch: 55/100 | Training Loss: 0.195 | Valid Score: 2.398\n"," \n","Epoch: 55/100 | Best Valid Score Until Now: 2.398 \n","\n","Patience 1\n","Epoch: 56/100 | Training Loss: 0.192 | Valid Score: 2.783\n"," \n","Epoch: 56/100 | Best Valid Score Until Now: 2.398 \n","\n","Patience 2\n","Epoch: 57/100 | Training Loss: 0.176 | Valid Score: 2.686\n"," \n","Epoch: 57/100 | Best Valid Score Until Now: 2.398 \n","\n","Patience 3\n","Epoch: 58/100 | Training Loss: 0.174 | Valid Score: 2.679\n"," \n","Epoch: 58/100 | Best Valid Score Until Now: 2.398 \n","\n","Patience 4\n","Epoch: 59/100 | Training Loss: 0.201 | Valid Score: 2.733\n"," \n","Epoch: 59/100 | Best Valid Score Until Now: 2.398 \n","\n","Patience 5\n","Epoch: 60/100 | Training Loss: 0.217 | Valid Score: 2.739\n"," \n","Epoch: 60/100 | Best Valid Score Until Now: 2.398 \n","\n","Patience 6\n","Epoch: 61/100 | Training Loss: 0.221 | Valid Score: 3.403\n"," \n","Epoch: 61/100 | Best Valid Score Until Now: 2.398 \n","\n","Patience 7\n","Epoch: 62/100 | Training Loss: 0.239 | Valid Score: 3.007\n"," \n","Epoch: 62/100 | Best Valid Score Until Now: 2.398 \n","\n","Patience 8\n","Epoch: 63/100 | Training Loss: 0.193 | Valid Score: 2.978\n"," \n","Epoch: 63/100 | Best Valid Score Until Now: 2.398 \n","\n","Patience 9\n","Epoch: 64/100 | Training Loss: 0.168 | Valid Score: 2.684\n"," \n","Epoch: 64/100 | Best Valid Score Until Now: 2.398 \n","\n","Patience 10\n","Epoch: 65/100 | Training Loss: 0.165 | Valid Score: 2.752\n"," \n","Epoch: 65/100 | Best Valid Score Until Now: 2.398 \n","\n","Final results:\n","Average Valid Score: 2.398 \n","\n","Test Score: 2.722 \n","\n","Execution time: 42.170 seconds\n"]}],"source":["import time\n","start_time = time.time()\n","\n","scaler = StandardScaler()\n","train_labels = train_set.labels.numpy()\n","scaler.fit(train_labels)\n","\n","train_evaluate()\n","test_evaluate()"]},{"cell_type":"markdown","source":["#Function to Compute Score of the Model"],"metadata":{"id":"YuPDzA9uaMgk"}},{"cell_type":"markdown","source":["#Graph SAGE 1"],"metadata":{"id":"qRQy86mUEv-6"}},{"cell_type":"code","source":["import dgl.function as fn\n","from dgl.nn import SAGEConv\n","\n","class GraphSAGE(nn.Module):\n","    def __init__(self, config, global_size = 200, num_tasks = 1):\n","        super().__init__()\n","        self.config = config\n","        self.num_tasks = num_tasks\n","\n","        # Node feature size\n","        self.node_feature_size = self.config.get('node_feature_size', 127)\n","\n","        # Edge feature size\n","        self.edge_feature_size = self.config.get('edge_feature_size', 12)\n","\n","        # Hidden size\n","        self.hidden_size = self.config.get('hidden_size', 100)\n","\n","        self.conv1 = SAGEConv(self.node_feature_size, self.hidden_size, aggregator_type='mean')\n","        self.conv2 = SAGEConv(self.hidden_size, self.num_tasks, aggregator_type='mean')\n","\n","    def forward(self, mol_dgl_graph, globals):\n","        mol_dgl_graph.ndata[\"v\"] = mol_dgl_graph.ndata[\"v\"][:,:self.node_feature_size]\n","        mol_dgl_graph.edata[\"e\"] = mol_dgl_graph.edata[\"e\"][:,:self.edge_feature_size]\n","        h = self.conv1(mol_dgl_graph, mol_dgl_graph.ndata[\"v\"])\n","        h = F.relu(h)\n","        h = self.conv2(mol_dgl_graph, h)\n","        mol_dgl_graph.ndata[\"h\"] = h\n","        return dgl.mean_nodes(mol_dgl_graph, \"h\")"],"metadata":{"id":"zyYZLW2i_If9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def train_evaluate():\n","\n","    model = GraphSAGE(config, global_size, num_tasks)\n","    optimizer = torch.optim.Adam(model.parameters(), lr = 0.001)\n","\n","    best_val = float('inf')\n","    patience_count = 1\n","    epoch = 1\n","\n","    while epoch <= num_epochs:\n","        if patience_count <= patience:\n","            model.train()\n","            loss_train = train_epoch(train_dataloader, model, optimizer, num_tasks)\n","            model.eval()\n","            score_val = compute_score(model, val_dataloader, scaler, len(val_set), num_tasks)\n","            if score_val < best_val:\n","                best_val = score_val\n","                print(\"Save checkpoint\")\n","                path = os.path.join(checkpoint_path, 'checkpoint.pth')\n","                dict_checkpoint = {\"score_val\": score_val}\n","                dict_checkpoint.update({\"model_state_dict\": model.state_dict(), \"optimizer_state\": optimizer.state_dict()})\n","                with open(path, \"wb\") as outputfile:\n","                    cloudpickle.dump(dict_checkpoint, outputfile)\n","                patience_count = 1\n","            else:\n","                print(\"Patience\", patience_count)\n","                patience_count += 1\n","\n","            print(\"Epoch: {}/{} | Training Loss: {:.3f} | Valid Score: {:.3f}\".format(\n","            epoch, num_epochs, loss_train, score_val))\n","\n","            print(\" \")\n","            print(\"Epoch: {}/{} | Best Valid Score Until Now: {:.3f}\".format(epoch, num_epochs, best_val), \"\\n\")\n","        epoch += 1\n","\n","    # best model save\n","    shutil.rmtree(best_model_path, ignore_errors=True)\n","    shutil.copytree(checkpoint_path, best_model_path)\n","\n","    print(\"Final results:\")\n","    print(\"Average Valid Score: {:.3f}\".format(best_val), \"\\n\")\n"],"metadata":{"id":"mrfgSUJWfqdr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mahldqkbfqdr"},"source":["##### Function to compute test set score of the final saved model"]},{"cell_type":"code","source":["def test_evaluate():\n","    final_model = GraphSAGE(config, global_size, num_tasks)\n","    path = os.path.join(best_model_path, 'checkpoint.pth')\n","    with open(path, 'rb') as f:\n","        checkpoint = cloudpickle.load(f)\n","    final_model.load_state_dict(checkpoint[\"model_state_dict\"])\n","    final_model.eval()\n","    test_score = compute_score(final_model, test_dataloader, scaler, len(test_set), num_tasks)\n","\n","    print(\"Test Score: {:.3f}\".format(test_score), \"\\n\")\n","    print(\"Execution time: {:.3f} seconds\".format(time.time() - start_time))"],"metadata":{"id":"rfz7uT0-fqdr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ERyh7OuYfqdr"},"source":["##### Train the model and evaluate its performance"]},{"cell_type":"code","execution_count":null,"metadata":{"outputId":"f4f448a5-daf4-4b70-95aa-4dec4fb38cc1","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1688551320903,"user_tz":-210,"elapsed":32332,"user":{"displayName":"Mahdi Salehi","userId":"13663744920024374425"}},"id":"oZZ82Hq9fqdr"},"outputs":[{"output_type":"stream","name":"stdout","text":["Save checkpoint\n","Epoch: 1/100 | Training Loss: 0.838 | Valid Score: 4.692\n"," \n","Epoch: 1/100 | Best Valid Score Until Now: 4.692 \n","\n","Patience 1\n","Epoch: 2/100 | Training Loss: 0.524 | Valid Score: 4.799\n"," \n","Epoch: 2/100 | Best Valid Score Until Now: 4.692 \n","\n","Save checkpoint\n","Epoch: 3/100 | Training Loss: 0.451 | Valid Score: 4.646\n"," \n","Epoch: 3/100 | Best Valid Score Until Now: 4.646 \n","\n","Save checkpoint\n","Epoch: 4/100 | Training Loss: 0.401 | Valid Score: 4.537\n"," \n","Epoch: 4/100 | Best Valid Score Until Now: 4.537 \n","\n","Save checkpoint\n","Epoch: 5/100 | Training Loss: 0.372 | Valid Score: 4.470\n"," \n","Epoch: 5/100 | Best Valid Score Until Now: 4.470 \n","\n","Save checkpoint\n","Epoch: 6/100 | Training Loss: 0.293 | Valid Score: 4.371\n"," \n","Epoch: 6/100 | Best Valid Score Until Now: 4.371 \n","\n","Save checkpoint\n","Epoch: 7/100 | Training Loss: 0.264 | Valid Score: 4.303\n"," \n","Epoch: 7/100 | Best Valid Score Until Now: 4.303 \n","\n","Save checkpoint\n","Epoch: 8/100 | Training Loss: 0.238 | Valid Score: 4.188\n"," \n","Epoch: 8/100 | Best Valid Score Until Now: 4.188 \n","\n","Save checkpoint\n","Epoch: 9/100 | Training Loss: 0.220 | Valid Score: 4.163\n"," \n","Epoch: 9/100 | Best Valid Score Until Now: 4.163 \n","\n","Save checkpoint\n","Epoch: 10/100 | Training Loss: 0.191 | Valid Score: 4.057\n"," \n","Epoch: 10/100 | Best Valid Score Until Now: 4.057 \n","\n","Save checkpoint\n","Epoch: 11/100 | Training Loss: 0.193 | Valid Score: 4.052\n"," \n","Epoch: 11/100 | Best Valid Score Until Now: 4.052 \n","\n","Save checkpoint\n","Epoch: 12/100 | Training Loss: 0.185 | Valid Score: 3.966\n"," \n","Epoch: 12/100 | Best Valid Score Until Now: 3.966 \n","\n","Patience 1\n","Epoch: 13/100 | Training Loss: 0.168 | Valid Score: 3.968\n"," \n","Epoch: 13/100 | Best Valid Score Until Now: 3.966 \n","\n","Patience 2\n","Epoch: 14/100 | Training Loss: 0.155 | Valid Score: 4.013\n"," \n","Epoch: 14/100 | Best Valid Score Until Now: 3.966 \n","\n","Save checkpoint\n","Epoch: 15/100 | Training Loss: 0.150 | Valid Score: 3.930\n"," \n","Epoch: 15/100 | Best Valid Score Until Now: 3.930 \n","\n","Save checkpoint\n","Epoch: 16/100 | Training Loss: 0.145 | Valid Score: 3.852\n"," \n","Epoch: 16/100 | Best Valid Score Until Now: 3.852 \n","\n","Patience 1\n","Epoch: 17/100 | Training Loss: 0.145 | Valid Score: 3.875\n"," \n","Epoch: 17/100 | Best Valid Score Until Now: 3.852 \n","\n","Patience 2\n","Epoch: 18/100 | Training Loss: 0.140 | Valid Score: 3.961\n"," \n","Epoch: 18/100 | Best Valid Score Until Now: 3.852 \n","\n","Patience 3\n","Epoch: 19/100 | Training Loss: 0.160 | Valid Score: 3.905\n"," \n","Epoch: 19/100 | Best Valid Score Until Now: 3.852 \n","\n","Patience 4\n","Epoch: 20/100 | Training Loss: 0.135 | Valid Score: 3.866\n"," \n","Epoch: 20/100 | Best Valid Score Until Now: 3.852 \n","\n","Save checkpoint\n","Epoch: 21/100 | Training Loss: 0.136 | Valid Score: 3.778\n"," \n","Epoch: 21/100 | Best Valid Score Until Now: 3.778 \n","\n","Save checkpoint\n","Epoch: 22/100 | Training Loss: 0.127 | Valid Score: 3.685\n"," \n","Epoch: 22/100 | Best Valid Score Until Now: 3.685 \n","\n","Patience 1\n","Epoch: 23/100 | Training Loss: 0.118 | Valid Score: 3.781\n"," \n","Epoch: 23/100 | Best Valid Score Until Now: 3.685 \n","\n","Patience 2\n","Epoch: 24/100 | Training Loss: 0.111 | Valid Score: 3.768\n"," \n","Epoch: 24/100 | Best Valid Score Until Now: 3.685 \n","\n","Patience 3\n","Epoch: 25/100 | Training Loss: 0.105 | Valid Score: 3.752\n"," \n","Epoch: 25/100 | Best Valid Score Until Now: 3.685 \n","\n","Save checkpoint\n","Epoch: 26/100 | Training Loss: 0.102 | Valid Score: 3.673\n"," \n","Epoch: 26/100 | Best Valid Score Until Now: 3.673 \n","\n","Patience 1\n","Epoch: 27/100 | Training Loss: 0.121 | Valid Score: 3.682\n"," \n","Epoch: 27/100 | Best Valid Score Until Now: 3.673 \n","\n","Patience 2\n","Epoch: 28/100 | Training Loss: 0.107 | Valid Score: 3.750\n"," \n","Epoch: 28/100 | Best Valid Score Until Now: 3.673 \n","\n","Save checkpoint\n","Epoch: 29/100 | Training Loss: 0.097 | Valid Score: 3.621\n"," \n","Epoch: 29/100 | Best Valid Score Until Now: 3.621 \n","\n","Patience 1\n","Epoch: 30/100 | Training Loss: 0.319 | Valid Score: 3.705\n"," \n","Epoch: 30/100 | Best Valid Score Until Now: 3.621 \n","\n","Patience 2\n","Epoch: 31/100 | Training Loss: 0.161 | Valid Score: 3.811\n"," \n","Epoch: 31/100 | Best Valid Score Until Now: 3.621 \n","\n","Patience 3\n","Epoch: 32/100 | Training Loss: 0.128 | Valid Score: 3.695\n"," \n","Epoch: 32/100 | Best Valid Score Until Now: 3.621 \n","\n","Patience 4\n","Epoch: 33/100 | Training Loss: 0.101 | Valid Score: 3.621\n"," \n","Epoch: 33/100 | Best Valid Score Until Now: 3.621 \n","\n","Save checkpoint\n","Epoch: 34/100 | Training Loss: 0.090 | Valid Score: 3.619\n"," \n","Epoch: 34/100 | Best Valid Score Until Now: 3.619 \n","\n","Save checkpoint\n","Epoch: 35/100 | Training Loss: 0.088 | Valid Score: 3.601\n"," \n","Epoch: 35/100 | Best Valid Score Until Now: 3.601 \n","\n","Save checkpoint\n","Epoch: 36/100 | Training Loss: 0.088 | Valid Score: 3.538\n"," \n","Epoch: 36/100 | Best Valid Score Until Now: 3.538 \n","\n","Patience 1\n","Epoch: 37/100 | Training Loss: 0.120 | Valid Score: 3.605\n"," \n","Epoch: 37/100 | Best Valid Score Until Now: 3.538 \n","\n","Save checkpoint\n","Epoch: 38/100 | Training Loss: 0.127 | Valid Score: 3.425\n"," \n","Epoch: 38/100 | Best Valid Score Until Now: 3.425 \n","\n","Save checkpoint\n","Epoch: 39/100 | Training Loss: 0.101 | Valid Score: 3.356\n"," \n","Epoch: 39/100 | Best Valid Score Until Now: 3.356 \n","\n","Patience 1\n","Epoch: 40/100 | Training Loss: 0.093 | Valid Score: 3.484\n"," \n","Epoch: 40/100 | Best Valid Score Until Now: 3.356 \n","\n","Patience 2\n","Epoch: 41/100 | Training Loss: 0.085 | Valid Score: 3.465\n"," \n","Epoch: 41/100 | Best Valid Score Until Now: 3.356 \n","\n","Patience 3\n","Epoch: 42/100 | Training Loss: 0.076 | Valid Score: 3.372\n"," \n","Epoch: 42/100 | Best Valid Score Until Now: 3.356 \n","\n","Patience 4\n","Epoch: 43/100 | Training Loss: 0.077 | Valid Score: 3.533\n"," \n","Epoch: 43/100 | Best Valid Score Until Now: 3.356 \n","\n","Patience 5\n","Epoch: 44/100 | Training Loss: 0.079 | Valid Score: 3.578\n"," \n","Epoch: 44/100 | Best Valid Score Until Now: 3.356 \n","\n","Patience 6\n","Epoch: 45/100 | Training Loss: 0.108 | Valid Score: 3.414\n"," \n","Epoch: 45/100 | Best Valid Score Until Now: 3.356 \n","\n","Patience 7\n","Epoch: 46/100 | Training Loss: 0.088 | Valid Score: 3.565\n"," \n","Epoch: 46/100 | Best Valid Score Until Now: 3.356 \n","\n","Patience 8\n","Epoch: 47/100 | Training Loss: 0.088 | Valid Score: 3.644\n"," \n","Epoch: 47/100 | Best Valid Score Until Now: 3.356 \n","\n","Patience 9\n","Epoch: 48/100 | Training Loss: 0.087 | Valid Score: 3.451\n"," \n","Epoch: 48/100 | Best Valid Score Until Now: 3.356 \n","\n","Patience 10\n","Epoch: 49/100 | Training Loss: 0.078 | Valid Score: 3.422\n"," \n","Epoch: 49/100 | Best Valid Score Until Now: 3.356 \n","\n","Final results:\n","Average Valid Score: 3.356 \n","\n","Test Score: 2.480 \n","\n","Execution time: 27.993 seconds\n"]}],"source":["import time\n","start_time = time.time()\n","\n","scaler = StandardScaler()\n","train_labels = train_set.labels.numpy()\n","scaler.fit(train_labels)\n","\n","train_evaluate()\n","test_evaluate()"]},{"cell_type":"markdown","source":["#Graph SAGE 2\n"],"metadata":{"id":"-26bxpYdHJXt"}},{"cell_type":"code","source":["import dgl.function as fn\n","from dgl.nn import SAGEConv\n","\n","class GraphSAGE2(nn.Module):\n","    def __init__(self, config, global_size = 200, num_tasks = 1):\n","        super().__init__()\n","        self.config = config\n","        self.num_tasks = num_tasks\n","\n","        # Node feature size\n","        self.node_feature_size = self.config.get('node_feature_size', 127)\n","\n","        # Edge feature size\n","        self.edge_feature_size = self.config.get('edge_feature_size', 12)\n","\n","        # Hidden size\n","        self.hidden_size = self.config.get('hidden_size', 100)\n","\n","        self.conv1 = SAGEConv(self.node_feature_size, self.hidden_size, aggregator_type='mean')\n","        self.conv2 = SAGEConv(self.hidden_size, self.hidden_size , aggregator_type='mean')\n","        self.conv3 = SAGEConv(self.hidden_size, self.num_tasks, aggregator_type='mean')\n","\n","    def forward(self, mol_dgl_graph, globals):\n","        mol_dgl_graph.ndata[\"v\"] = mol_dgl_graph.ndata[\"v\"][:,:self.node_feature_size]\n","        mol_dgl_graph.edata[\"e\"] = mol_dgl_graph.edata[\"e\"][:,:self.edge_feature_size]\n","        h = self.conv1(mol_dgl_graph, mol_dgl_graph.ndata[\"v\"])\n","        h = F.relu(h)\n","        h = self.conv2(mol_dgl_graph, h)\n","        h = F.relu(h)\n","        h = self.conv3(mol_dgl_graph, h)\n","        mol_dgl_graph.ndata[\"h\"] = h\n","        return dgl.mean_nodes(mol_dgl_graph, \"h\")"],"metadata":{"id":"z6FLAUGTDobB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def train_evaluate():\n","\n","    model = GraphSAGE2(config, global_size, num_tasks)\n","    optimizer = torch.optim.Adam(model.parameters(), lr = 0.001)\n","\n","    best_val = float('inf')\n","    patience_count = 1\n","    epoch = 1\n","\n","    while epoch <= num_epochs:\n","        if patience_count <= patience:\n","            model.train()\n","            loss_train = train_epoch(train_dataloader, model, optimizer, num_tasks)\n","            model.eval()\n","            score_val = compute_score(model, val_dataloader, scaler, len(val_set), num_tasks)\n","            if score_val < best_val:\n","                best_val = score_val\n","                print(\"Save checkpoint\")\n","                path = os.path.join(checkpoint_path, 'checkpoint.pth')\n","                dict_checkpoint = {\"score_val\": score_val}\n","                dict_checkpoint.update({\"model_state_dict\": model.state_dict(), \"optimizer_state\": optimizer.state_dict()})\n","                with open(path, \"wb\") as outputfile:\n","                    cloudpickle.dump(dict_checkpoint, outputfile)\n","                patience_count = 1\n","            else:\n","                print(\"Patience\", patience_count)\n","                patience_count += 1\n","\n","            print(\"Epoch: {}/{} | Training Loss: {:.3f} | Valid Score: {:.3f}\".format(\n","            epoch, num_epochs, loss_train, score_val))\n","\n","            print(\" \")\n","            print(\"Epoch: {}/{} | Best Valid Score Until Now: {:.3f}\".format(epoch, num_epochs, best_val), \"\\n\")\n","        epoch += 1\n","\n","    # best model save\n","    shutil.rmtree(best_model_path, ignore_errors=True)\n","    shutil.copytree(checkpoint_path, best_model_path)\n","\n","    print(\"Final results:\")\n","    print(\"Average Valid Score: {:.3f}\".format(best_val), \"\\n\")\n"],"metadata":{"id":"S83w9vXCgW8o"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8giw1M4bgW8p"},"source":["##### Function to compute test set score of the final saved model"]},{"cell_type":"code","source":["def test_evaluate():\n","    final_model = GraphSAGE2(config, global_size, num_tasks)\n","    path = os.path.join(best_model_path, 'checkpoint.pth')\n","    with open(path, 'rb') as f:\n","        checkpoint = cloudpickle.load(f)\n","    final_model.load_state_dict(checkpoint[\"model_state_dict\"])\n","    final_model.eval()\n","    test_score = compute_score(final_model, test_dataloader, scaler, len(test_set), num_tasks)\n","\n","    print(\"Test Score: {:.3f}\".format(test_score), \"\\n\")\n","    print(\"Execution time: {:.3f} seconds\".format(time.time() - start_time))"],"metadata":{"id":"H9zNkQ3XgW8p"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AYzTY6Q0gW8p"},"source":["##### Train the model and evaluate its performance"]},{"cell_type":"code","execution_count":null,"metadata":{"outputId":"49f77ef7-fbf0-452d-bb2e-84a32b9cb88e","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1688551454447,"user_tz":-210,"elapsed":25141,"user":{"displayName":"Mahdi Salehi","userId":"13663744920024374425"}},"id":"6qj5eyiOgW8p"},"outputs":[{"output_type":"stream","name":"stdout","text":["Save checkpoint\n","Epoch: 1/100 | Training Loss: 3.745 | Valid Score: 4.914\n"," \n","Epoch: 1/100 | Best Valid Score Until Now: 4.914 \n","\n","Save checkpoint\n","Epoch: 2/100 | Training Loss: 1.031 | Valid Score: 3.718\n"," \n","Epoch: 2/100 | Best Valid Score Until Now: 3.718 \n","\n","Save checkpoint\n","Epoch: 3/100 | Training Loss: 0.563 | Valid Score: 3.668\n"," \n","Epoch: 3/100 | Best Valid Score Until Now: 3.668 \n","\n","Save checkpoint\n","Epoch: 4/100 | Training Loss: 0.393 | Valid Score: 3.505\n"," \n","Epoch: 4/100 | Best Valid Score Until Now: 3.505 \n","\n","Save checkpoint\n","Epoch: 5/100 | Training Loss: 0.325 | Valid Score: 3.435\n"," \n","Epoch: 5/100 | Best Valid Score Until Now: 3.435 \n","\n","Patience 1\n","Epoch: 6/100 | Training Loss: 0.296 | Valid Score: 3.461\n"," \n","Epoch: 6/100 | Best Valid Score Until Now: 3.435 \n","\n","Patience 2\n","Epoch: 7/100 | Training Loss: 0.279 | Valid Score: 3.553\n"," \n","Epoch: 7/100 | Best Valid Score Until Now: 3.435 \n","\n","Save checkpoint\n","Epoch: 8/100 | Training Loss: 0.223 | Valid Score: 3.385\n"," \n","Epoch: 8/100 | Best Valid Score Until Now: 3.385 \n","\n","Patience 1\n","Epoch: 9/100 | Training Loss: 0.181 | Valid Score: 3.460\n"," \n","Epoch: 9/100 | Best Valid Score Until Now: 3.385 \n","\n","Patience 2\n","Epoch: 10/100 | Training Loss: 0.173 | Valid Score: 3.462\n"," \n","Epoch: 10/100 | Best Valid Score Until Now: 3.385 \n","\n","Save checkpoint\n","Epoch: 11/100 | Training Loss: 0.166 | Valid Score: 3.357\n"," \n","Epoch: 11/100 | Best Valid Score Until Now: 3.357 \n","\n","Save checkpoint\n","Epoch: 12/100 | Training Loss: 0.150 | Valid Score: 3.342\n"," \n","Epoch: 12/100 | Best Valid Score Until Now: 3.342 \n","\n","Patience 1\n","Epoch: 13/100 | Training Loss: 0.140 | Valid Score: 3.378\n"," \n","Epoch: 13/100 | Best Valid Score Until Now: 3.342 \n","\n","Save checkpoint\n","Epoch: 14/100 | Training Loss: 0.130 | Valid Score: 3.286\n"," \n","Epoch: 14/100 | Best Valid Score Until Now: 3.286 \n","\n","Save checkpoint\n","Epoch: 15/100 | Training Loss: 0.119 | Valid Score: 3.277\n"," \n","Epoch: 15/100 | Best Valid Score Until Now: 3.277 \n","\n","Patience 1\n","Epoch: 16/100 | Training Loss: 0.115 | Valid Score: 3.336\n"," \n","Epoch: 16/100 | Best Valid Score Until Now: 3.277 \n","\n","Save checkpoint\n","Epoch: 17/100 | Training Loss: 0.139 | Valid Score: 3.150\n"," \n","Epoch: 17/100 | Best Valid Score Until Now: 3.150 \n","\n","Patience 1\n","Epoch: 18/100 | Training Loss: 0.253 | Valid Score: 3.206\n"," \n","Epoch: 18/100 | Best Valid Score Until Now: 3.150 \n","\n","Save checkpoint\n","Epoch: 19/100 | Training Loss: 0.165 | Valid Score: 3.079\n"," \n","Epoch: 19/100 | Best Valid Score Until Now: 3.079 \n","\n","Patience 1\n","Epoch: 20/100 | Training Loss: 0.110 | Valid Score: 3.250\n"," \n","Epoch: 20/100 | Best Valid Score Until Now: 3.079 \n","\n","Patience 2\n","Epoch: 21/100 | Training Loss: 0.094 | Valid Score: 3.227\n"," \n","Epoch: 21/100 | Best Valid Score Until Now: 3.079 \n","\n","Patience 3\n","Epoch: 22/100 | Training Loss: 0.092 | Valid Score: 3.214\n"," \n","Epoch: 22/100 | Best Valid Score Until Now: 3.079 \n","\n","Patience 4\n","Epoch: 23/100 | Training Loss: 0.142 | Valid Score: 3.348\n"," \n","Epoch: 23/100 | Best Valid Score Until Now: 3.079 \n","\n","Patience 5\n","Epoch: 24/100 | Training Loss: 0.102 | Valid Score: 3.267\n"," \n","Epoch: 24/100 | Best Valid Score Until Now: 3.079 \n","\n","Patience 6\n","Epoch: 25/100 | Training Loss: 0.115 | Valid Score: 3.261\n"," \n","Epoch: 25/100 | Best Valid Score Until Now: 3.079 \n","\n","Patience 7\n","Epoch: 26/100 | Training Loss: 0.096 | Valid Score: 3.094\n"," \n","Epoch: 26/100 | Best Valid Score Until Now: 3.079 \n","\n","Patience 8\n","Epoch: 27/100 | Training Loss: 0.094 | Valid Score: 3.149\n"," \n","Epoch: 27/100 | Best Valid Score Until Now: 3.079 \n","\n","Save checkpoint\n","Epoch: 28/100 | Training Loss: 0.091 | Valid Score: 3.033\n"," \n","Epoch: 28/100 | Best Valid Score Until Now: 3.033 \n","\n","Save checkpoint\n","Epoch: 29/100 | Training Loss: 0.076 | Valid Score: 2.938\n"," \n","Epoch: 29/100 | Best Valid Score Until Now: 2.938 \n","\n","Patience 1\n","Epoch: 30/100 | Training Loss: 0.081 | Valid Score: 3.139\n"," \n","Epoch: 30/100 | Best Valid Score Until Now: 2.938 \n","\n","Patience 2\n","Epoch: 31/100 | Training Loss: 0.112 | Valid Score: 3.392\n"," \n","Epoch: 31/100 | Best Valid Score Until Now: 2.938 \n","\n","Save checkpoint\n","Epoch: 32/100 | Training Loss: 0.079 | Valid Score: 2.810\n"," \n","Epoch: 32/100 | Best Valid Score Until Now: 2.810 \n","\n","Patience 1\n","Epoch: 33/100 | Training Loss: 0.121 | Valid Score: 3.255\n"," \n","Epoch: 33/100 | Best Valid Score Until Now: 2.810 \n","\n","Patience 2\n","Epoch: 34/100 | Training Loss: 0.130 | Valid Score: 3.032\n"," \n","Epoch: 34/100 | Best Valid Score Until Now: 2.810 \n","\n","Patience 3\n","Epoch: 35/100 | Training Loss: 0.102 | Valid Score: 3.126\n"," \n","Epoch: 35/100 | Best Valid Score Until Now: 2.810 \n","\n","Patience 4\n","Epoch: 36/100 | Training Loss: 0.073 | Valid Score: 2.968\n"," \n","Epoch: 36/100 | Best Valid Score Until Now: 2.810 \n","\n","Patience 5\n","Epoch: 37/100 | Training Loss: 0.064 | Valid Score: 3.009\n"," \n","Epoch: 37/100 | Best Valid Score Until Now: 2.810 \n","\n","Patience 6\n","Epoch: 38/100 | Training Loss: 0.068 | Valid Score: 3.052\n"," \n","Epoch: 38/100 | Best Valid Score Until Now: 2.810 \n","\n","Patience 7\n","Epoch: 39/100 | Training Loss: 0.058 | Valid Score: 2.901\n"," \n","Epoch: 39/100 | Best Valid Score Until Now: 2.810 \n","\n","Patience 8\n","Epoch: 40/100 | Training Loss: 0.057 | Valid Score: 3.002\n"," \n","Epoch: 40/100 | Best Valid Score Until Now: 2.810 \n","\n","Patience 9\n","Epoch: 41/100 | Training Loss: 0.056 | Valid Score: 2.937\n"," \n","Epoch: 41/100 | Best Valid Score Until Now: 2.810 \n","\n","Patience 10\n","Epoch: 42/100 | Training Loss: 0.060 | Valid Score: 2.994\n"," \n","Epoch: 42/100 | Best Valid Score Until Now: 2.810 \n","\n","Final results:\n","Average Valid Score: 2.810 \n","\n","Test Score: 2.245 \n","\n","Execution time: 25.111 seconds\n"]}],"source":["import time\n","start_time = time.time()\n","\n","scaler = StandardScaler()\n","train_labels = train_set.labels.numpy()\n","scaler.fit(train_labels)\n","\n","train_evaluate()\n","test_evaluate()"]},{"cell_type":"markdown","source":["#Graph SAGE 3\n"],"metadata":{"id":"mN9WFnZkJqSQ"}},{"cell_type":"code","source":["from dgl.nn.pytorch.conv import SAGEConv\n","class GraphSAGE3(nn.Module):\n","    def __init__(self, config, global_size = 200, num_tasks = 1):\n","        super().__init__()\n","        self.config = config\n","        self.num_tasks = num_tasks\n","\n","        # Node feature size\n","        self.node_feature_size = self.config.get('node_feature_size', 127)\n","\n","        # Edge feature size\n","        self.edge_feature_size = self.config.get('edge_feature_size', 12)\n","\n","        # Hidden size\n","        self.hidden_size = self.config.get('hidden_size', 100)\n","\n","        self.conv1 = SAGEConv(self.node_feature_size, 64 ,aggregator_type='lstm')\n","        self.conv2 = SAGEConv(64, 128,aggregator_type='lstm')\n","        self.conv3 = SAGEConv(128, 256,aggregator_type='lstm')\n","        self.conv4 = SAGEConv(256, 128,aggregator_type='lstm')\n","        self.conv5 = SAGEConv(128, self.num_tasks,aggregator_type='lstm')\n","\n","        self.dropout = nn.Dropout(p=0.2)\n","        self.bn1 = nn.BatchNorm1d(64)\n","        self.bn2 = nn.BatchNorm1d(128)\n","        self.bn3 = nn.BatchNorm1d(256)\n","\n","    # def forward(self, g, in_feat):\n","    def forward(self, mol_dgl_graph, globals):\n","        mol_dgl_graph.ndata[\"v\"]= mol_dgl_graph.ndata[\"v\"][:,:self.node_feature_size]\n","        mol_dgl_graph.edata[\"e\"] = mol_dgl_graph.edata[\"e\"][:,:self.edge_feature_size]\n","        h = self.conv1(mol_dgl_graph, mol_dgl_graph.ndata[\"v\"])\n","        h = F.relu(h)\n","        #h = self.bn1(h)\n","        h = self.conv2(mol_dgl_graph, h)\n","        h = F.relu(h)\n","        h = self.dropout(h)\n","        #h = self.bn2(h)\n","        h = self.conv3(mol_dgl_graph, h)\n","        h = F.relu(h)\n","        h = self.dropout(h)\n","        #h = self.bn3(h)\n","        h = self.conv4(mol_dgl_graph, h)\n","        h = F.relu(h)\n","        #h = self.bn2(h)\n","        h = self.conv5(mol_dgl_graph, h)\n","        mol_dgl_graph.ndata[\"h\"] = h\n","        return dgl.mean_nodes(mol_dgl_graph, \"h\")"],"metadata":{"id":"hoXL7x0nJqSQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def train_evaluate():\n","\n","    model = GraphSAGE3(config, global_size, num_tasks)\n","    optimizer = torch.optim.Adam(model.parameters(), lr = 0.001)\n","\n","    best_val = float('inf')\n","    patience_count = 1\n","    epoch = 1\n","\n","    while epoch <= num_epochs:\n","        if patience_count <= patience:\n","            model.train()\n","            loss_train = train_epoch(train_dataloader, model, optimizer, num_tasks)\n","            model.eval()\n","            score_val = compute_score(model, val_dataloader, scaler, len(val_set), num_tasks)\n","            if score_val < best_val:\n","                best_val = score_val\n","                print(\"Save checkpoint\")\n","                path = os.path.join(checkpoint_path, 'checkpoint.pth')\n","                dict_checkpoint = {\"score_val\": score_val}\n","                dict_checkpoint.update({\"model_state_dict\": model.state_dict(), \"optimizer_state\": optimizer.state_dict()})\n","                with open(path, \"wb\") as outputfile:\n","                    cloudpickle.dump(dict_checkpoint, outputfile)\n","                patience_count = 1\n","            else:\n","                print(\"Patience\", patience_count)\n","                patience_count += 1\n","\n","            print(\"Epoch: {}/{} | Training Loss: {:.3f} | Valid Score: {:.3f}\".format(\n","            epoch, num_epochs, loss_train, score_val))\n","\n","            print(\" \")\n","            print(\"Epoch: {}/{} | Best Valid Score Until Now: {:.3f}\".format(epoch, num_epochs, best_val), \"\\n\")\n","        epoch += 1\n","\n","    # best model save\n","    shutil.rmtree(best_model_path, ignore_errors=True)\n","    shutil.copytree(checkpoint_path, best_model_path)\n","\n","    print(\"Final results:\")\n","    print(\"Average Valid Score: {:.3f}\".format(best_val), \"\\n\")\n"],"metadata":{"id":"Gz-Gqn1JP9hl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tIvRolDHP9hl"},"source":["##### Function to compute test set score of the final saved model"]},{"cell_type":"code","source":["def test_evaluate():\n","    final_model = GraphSAGE3(config, global_size, num_tasks)\n","    path = os.path.join(best_model_path, 'checkpoint.pth')\n","    with open(path, 'rb') as f:\n","        checkpoint = cloudpickle.load(f)\n","    final_model.load_state_dict(checkpoint[\"model_state_dict\"])\n","    final_model.eval()\n","    test_score = compute_score(final_model, test_dataloader, scaler, len(test_set), num_tasks)\n","\n","    print(\"Test Score: {:.3f}\".format(test_score), \"\\n\")\n","    print(\"Execution time: {:.3f} seconds\".format(time.time() - start_time))"],"metadata":{"id":"aT9HXM05P9hl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PbobWa8xP9hm"},"source":["##### Train the model and evaluate its performance"]},{"cell_type":"code","source":["import time\n","start_time = time.time()\n","\n","scaler = StandardScaler()\n","train_labels = train_set.labels.numpy()\n","scaler.fit(train_labels)\n","\n","train_evaluate()\n","test_evaluate()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0Mh08ZjlSBW5","executionInfo":{"status":"ok","timestamp":1688564545023,"user_tz":-210,"elapsed":73187,"user":{"displayName":"Mahdi Salehi","userId":"13663744920024374425"}},"outputId":"9b71e175-c5c0-4a37-dae6-cb8833f7c3d0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Save checkpoint\n","Epoch: 1/100 | Training Loss: 0.959 | Valid Score: 3.725\n"," \n","Epoch: 1/100 | Best Valid Score Until Now: 3.725 \n","\n","Patience 1\n","Epoch: 2/100 | Training Loss: 0.523 | Valid Score: 3.957\n"," \n","Epoch: 2/100 | Best Valid Score Until Now: 3.725 \n","\n","Save checkpoint\n","Epoch: 3/100 | Training Loss: 0.390 | Valid Score: 3.669\n"," \n","Epoch: 3/100 | Best Valid Score Until Now: 3.669 \n","\n","Save checkpoint\n","Epoch: 4/100 | Training Loss: 0.304 | Valid Score: 2.793\n"," \n","Epoch: 4/100 | Best Valid Score Until Now: 2.793 \n","\n","Save checkpoint\n","Epoch: 5/100 | Training Loss: 0.204 | Valid Score: 2.645\n"," \n","Epoch: 5/100 | Best Valid Score Until Now: 2.645 \n","\n","Save checkpoint\n","Epoch: 6/100 | Training Loss: 0.193 | Valid Score: 2.304\n"," \n","Epoch: 6/100 | Best Valid Score Until Now: 2.304 \n","\n","Save checkpoint\n","Epoch: 7/100 | Training Loss: 0.201 | Valid Score: 1.896\n"," \n","Epoch: 7/100 | Best Valid Score Until Now: 1.896 \n","\n","Patience 1\n","Epoch: 8/100 | Training Loss: 0.166 | Valid Score: 2.282\n"," \n","Epoch: 8/100 | Best Valid Score Until Now: 1.896 \n","\n","Patience 2\n","Epoch: 9/100 | Training Loss: 0.201 | Valid Score: 2.524\n"," \n","Epoch: 9/100 | Best Valid Score Until Now: 1.896 \n","\n","Patience 3\n","Epoch: 10/100 | Training Loss: 0.233 | Valid Score: 2.574\n"," \n","Epoch: 10/100 | Best Valid Score Until Now: 1.896 \n","\n","Save checkpoint\n","Epoch: 11/100 | Training Loss: 0.168 | Valid Score: 1.836\n"," \n","Epoch: 11/100 | Best Valid Score Until Now: 1.836 \n","\n","Patience 1\n","Epoch: 12/100 | Training Loss: 0.127 | Valid Score: 1.930\n"," \n","Epoch: 12/100 | Best Valid Score Until Now: 1.836 \n","\n","Patience 2\n","Epoch: 13/100 | Training Loss: 0.107 | Valid Score: 2.104\n"," \n","Epoch: 13/100 | Best Valid Score Until Now: 1.836 \n","\n","Patience 3\n","Epoch: 14/100 | Training Loss: 0.096 | Valid Score: 2.044\n"," \n","Epoch: 14/100 | Best Valid Score Until Now: 1.836 \n","\n","Patience 4\n","Epoch: 15/100 | Training Loss: 0.090 | Valid Score: 1.857\n"," \n","Epoch: 15/100 | Best Valid Score Until Now: 1.836 \n","\n","Save checkpoint\n","Epoch: 16/100 | Training Loss: 0.087 | Valid Score: 1.832\n"," \n","Epoch: 16/100 | Best Valid Score Until Now: 1.832 \n","\n","Patience 1\n","Epoch: 17/100 | Training Loss: 0.078 | Valid Score: 1.902\n"," \n","Epoch: 17/100 | Best Valid Score Until Now: 1.832 \n","\n","Save checkpoint\n","Epoch: 18/100 | Training Loss: 0.091 | Valid Score: 1.729\n"," \n","Epoch: 18/100 | Best Valid Score Until Now: 1.729 \n","\n","Patience 1\n","Epoch: 19/100 | Training Loss: 0.083 | Valid Score: 1.751\n"," \n","Epoch: 19/100 | Best Valid Score Until Now: 1.729 \n","\n","Patience 2\n","Epoch: 20/100 | Training Loss: 0.089 | Valid Score: 1.943\n"," \n","Epoch: 20/100 | Best Valid Score Until Now: 1.729 \n","\n","Patience 3\n","Epoch: 21/100 | Training Loss: 0.101 | Valid Score: 2.126\n"," \n","Epoch: 21/100 | Best Valid Score Until Now: 1.729 \n","\n","Patience 4\n","Epoch: 22/100 | Training Loss: 0.084 | Valid Score: 1.994\n"," \n","Epoch: 22/100 | Best Valid Score Until Now: 1.729 \n","\n","Patience 5\n","Epoch: 23/100 | Training Loss: 0.110 | Valid Score: 1.941\n"," \n","Epoch: 23/100 | Best Valid Score Until Now: 1.729 \n","\n","Patience 6\n","Epoch: 24/100 | Training Loss: 0.068 | Valid Score: 1.765\n"," \n","Epoch: 24/100 | Best Valid Score Until Now: 1.729 \n","\n","Patience 7\n","Epoch: 25/100 | Training Loss: 0.075 | Valid Score: 1.811\n"," \n","Epoch: 25/100 | Best Valid Score Until Now: 1.729 \n","\n","Patience 8\n","Epoch: 26/100 | Training Loss: 0.068 | Valid Score: 1.777\n"," \n","Epoch: 26/100 | Best Valid Score Until Now: 1.729 \n","\n","Patience 9\n","Epoch: 27/100 | Training Loss: 0.059 | Valid Score: 1.772\n"," \n","Epoch: 27/100 | Best Valid Score Until Now: 1.729 \n","\n","Patience 10\n","Epoch: 28/100 | Training Loss: 0.066 | Valid Score: 2.140\n"," \n","Epoch: 28/100 | Best Valid Score Until Now: 1.729 \n","\n","Final results:\n","Average Valid Score: 1.729 \n","\n","Test Score: 1.565 \n","\n","Execution time: 73.134 seconds\n"]}]},{"cell_type":"markdown","source":["# GAT 1"],"metadata":{"id":"EarbDqzrnKMA"}},{"cell_type":"code","source":["from dgl.nn.pytorch import GATConv\n","class GATConv1(nn.Module):\n","    def __init__(self, config, global_size=200, num_tasks=1):\n","        super().__init__()\n","        self.config = config\n","        self.num_tasks = num_tasks\n","\n","        # Node feature size\n","        self.node_feature_size = self.config.get('node_feature_size', 127)\n","\n","        # Edge feature size\n","        self.edge_feature_size = self.config.get('edge_feature_size', 12)\n","\n","        # Hidden size\n","        self.hidden_size = self.config.get('hidden_size', 100)\n","\n","        # Number of attention heads\n","        self.num_heads = self.config.get('num_heads', 1)\n","\n","        # Dropout probability\n","        self.dropout = self.config.get('dropout', 0.0)\n","\n","        # GAT layer\n","        self.conv1 = GATConv(\n","            self.node_feature_size,\n","            self.hidden_size,\n","            num_heads=self.num_heads,\n","            feat_drop=self.dropout,\n","            attn_drop=self.dropout,allow_zero_in_degree=True\n","        )\n","\n","        # Linear layer\n","        self.fc = nn.Linear(\n","            self.hidden_size * self.num_heads,\n","            self.hidden_size\n","        )\n","\n","        # GAT layer\n","        self.conv2 = GATConv(\n","            self.hidden_size,\n","            self.num_tasks,\n","            num_heads=1,\n","            feat_drop=self.dropout,\n","            attn_drop=self.dropout,allow_zero_in_degree=True\n","        )\n","\n","    def forward(self, mol_dgl_graph, globals):\n","        mol_dgl_graph.ndata[\"v\"] = mol_dgl_graph.ndata[\"v\"][:, :self.node_feature_size]\n","        mol_dgl_graph.edata[\"e\"] = mol_dgl_graph.edata[\"e\"][:, :self.edge_feature_size]\n","\n","        # First GAT layer\n","        h = self.conv1(mol_dgl_graph, mol_dgl_graph.ndata[\"v\"]).flatten(1)\n","        h = F.relu(h)\n","        h = self.fc(h)\n","        h = F.dropout(h, p=self.dropout, training=self.training)\n","\n","        # Second GAT layer\n","        h = self.conv2(mol_dgl_graph, h).squeeze(1)\n","        mol_dgl_graph.ndata[\"h\"] = h\n","\n","        return dgl.mean_nodes(mol_dgl_graph, \"h\")"],"metadata":{"id":"mHupBXIeDiuF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def train_evaluate():\n","\n","    model = GATConv1(config, global_size, num_tasks)\n","    optimizer = torch.optim.Adam(model.parameters(), lr = 0.001)\n","\n","    best_val = float('inf')\n","    patience_count = 1\n","    epoch = 1\n","\n","    while epoch <= num_epochs:\n","        if patience_count <= patience:\n","            model.train()\n","            loss_train = train_epoch(train_dataloader, model, optimizer, num_tasks)\n","            model.eval()\n","            score_val = compute_score(model, val_dataloader, scaler, len(val_set), num_tasks)\n","            if score_val < best_val:\n","                best_val = score_val\n","                print(\"Save checkpoint\")\n","                path = os.path.join(checkpoint_path, 'checkpoint.pth')\n","                dict_checkpoint = {\"score_val\": score_val}\n","                dict_checkpoint.update({\"model_state_dict\": model.state_dict(), \"optimizer_state\": optimizer.state_dict()})\n","                with open(path, \"wb\") as outputfile:\n","                    cloudpickle.dump(dict_checkpoint, outputfile)\n","                patience_count = 1\n","            else:\n","                print(\"Patience\", patience_count)\n","                patience_count += 1\n","\n","            print(\"Epoch: {}/{} | Training Loss: {:.3f} | Valid Score: {:.3f}\".format(\n","            epoch, num_epochs, loss_train, score_val))\n","\n","            print(\" \")\n","            print(\"Epoch: {}/{} | Best Valid Score Until Now: {:.3f}\".format(epoch, num_epochs, best_val), \"\\n\")\n","        epoch += 1\n","\n","    # best model save\n","    shutil.rmtree(best_model_path, ignore_errors=True)\n","    shutil.copytree(checkpoint_path, best_model_path)\n","\n","    print(\"Final results:\")\n","    print(\"Average Valid Score: {:.3f}\".format(best_val), \"\\n\")\n"],"metadata":{"id":"SLCFVAz9S_8i"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cWV0aNdeS_8i"},"source":["##### Function to compute test set score of the final saved model"]},{"cell_type":"code","source":["def test_evaluate():\n","    final_model = GATConv1(config, global_size, num_tasks)\n","    path = os.path.join(best_model_path, 'checkpoint.pth')\n","    with open(path, 'rb') as f:\n","        checkpoint = cloudpickle.load(f)\n","    final_model.load_state_dict(checkpoint[\"model_state_dict\"])\n","    final_model.eval()\n","    test_score = compute_score(final_model, test_dataloader, scaler, len(test_set), num_tasks)\n","\n","    print(\"Test Score: {:.3f}\".format(test_score), \"\\n\")\n","    print(\"Execution time: {:.3f} seconds\".format(time.time() - start_time))"],"metadata":{"id":"CGtNbrMaS_8i"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"evvU0dDdS_8i"},"source":["##### Train the model and evaluate its performance"]},{"cell_type":"code","source":["import time\n","start_time = time.time()\n","\n","scaler = StandardScaler()\n","train_labels = train_set.labels.numpy()\n","scaler.fit(train_labels)\n","\n","train_evaluate()\n","test_evaluate()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1688564946512,"user_tz":-210,"elapsed":27963,"user":{"displayName":"Mahdi Salehi","userId":"13663744920024374425"}},"outputId":"501e83f1-4c48-4c1a-895c-362f020b6624","id":"OLY695pgS_8j"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Save checkpoint\n","Epoch: 1/100 | Training Loss: 0.831 | Valid Score: 3.946\n"," \n","Epoch: 1/100 | Best Valid Score Until Now: 3.946 \n","\n","Save checkpoint\n","Epoch: 2/100 | Training Loss: 0.662 | Valid Score: 3.544\n"," \n","Epoch: 2/100 | Best Valid Score Until Now: 3.544 \n","\n","Patience 1\n","Epoch: 3/100 | Training Loss: 0.534 | Valid Score: 3.805\n"," \n","Epoch: 3/100 | Best Valid Score Until Now: 3.544 \n","\n","Save checkpoint\n","Epoch: 4/100 | Training Loss: 0.481 | Valid Score: 3.350\n"," \n","Epoch: 4/100 | Best Valid Score Until Now: 3.350 \n","\n","Patience 1\n","Epoch: 5/100 | Training Loss: 0.488 | Valid Score: 3.578\n"," \n","Epoch: 5/100 | Best Valid Score Until Now: 3.350 \n","\n","Save checkpoint\n","Epoch: 6/100 | Training Loss: 0.411 | Valid Score: 3.316\n"," \n","Epoch: 6/100 | Best Valid Score Until Now: 3.316 \n","\n","Patience 1\n","Epoch: 7/100 | Training Loss: 0.374 | Valid Score: 3.383\n"," \n","Epoch: 7/100 | Best Valid Score Until Now: 3.316 \n","\n","Patience 2\n","Epoch: 8/100 | Training Loss: 0.361 | Valid Score: 3.557\n"," \n","Epoch: 8/100 | Best Valid Score Until Now: 3.316 \n","\n","Patience 3\n","Epoch: 9/100 | Training Loss: 0.383 | Valid Score: 3.489\n"," \n","Epoch: 9/100 | Best Valid Score Until Now: 3.316 \n","\n","Patience 4\n","Epoch: 10/100 | Training Loss: 0.381 | Valid Score: 3.590\n"," \n","Epoch: 10/100 | Best Valid Score Until Now: 3.316 \n","\n","Patience 5\n","Epoch: 11/100 | Training Loss: 0.331 | Valid Score: 3.391\n"," \n","Epoch: 11/100 | Best Valid Score Until Now: 3.316 \n","\n","Patience 6\n","Epoch: 12/100 | Training Loss: 0.317 | Valid Score: 3.416\n"," \n","Epoch: 12/100 | Best Valid Score Until Now: 3.316 \n","\n","Save checkpoint\n","Epoch: 13/100 | Training Loss: 0.321 | Valid Score: 3.309\n"," \n","Epoch: 13/100 | Best Valid Score Until Now: 3.309 \n","\n","Patience 1\n","Epoch: 14/100 | Training Loss: 0.403 | Valid Score: 3.360\n"," \n","Epoch: 14/100 | Best Valid Score Until Now: 3.309 \n","\n","Patience 2\n","Epoch: 15/100 | Training Loss: 0.345 | Valid Score: 3.425\n"," \n","Epoch: 15/100 | Best Valid Score Until Now: 3.309 \n","\n","Patience 3\n","Epoch: 16/100 | Training Loss: 0.318 | Valid Score: 3.320\n"," \n","Epoch: 16/100 | Best Valid Score Until Now: 3.309 \n","\n","Save checkpoint\n","Epoch: 17/100 | Training Loss: 0.307 | Valid Score: 3.301\n"," \n","Epoch: 17/100 | Best Valid Score Until Now: 3.301 \n","\n","Patience 1\n","Epoch: 18/100 | Training Loss: 0.293 | Valid Score: 3.373\n"," \n","Epoch: 18/100 | Best Valid Score Until Now: 3.301 \n","\n","Patience 2\n","Epoch: 19/100 | Training Loss: 0.278 | Valid Score: 3.427\n"," \n","Epoch: 19/100 | Best Valid Score Until Now: 3.301 \n","\n","Patience 3\n","Epoch: 20/100 | Training Loss: 0.278 | Valid Score: 3.303\n"," \n","Epoch: 20/100 | Best Valid Score Until Now: 3.301 \n","\n","Patience 4\n","Epoch: 21/100 | Training Loss: 0.286 | Valid Score: 3.398\n"," \n","Epoch: 21/100 | Best Valid Score Until Now: 3.301 \n","\n","Patience 5\n","Epoch: 22/100 | Training Loss: 0.323 | Valid Score: 3.340\n"," \n","Epoch: 22/100 | Best Valid Score Until Now: 3.301 \n","\n","Save checkpoint\n","Epoch: 23/100 | Training Loss: 0.320 | Valid Score: 3.215\n"," \n","Epoch: 23/100 | Best Valid Score Until Now: 3.215 \n","\n","Patience 1\n","Epoch: 24/100 | Training Loss: 0.285 | Valid Score: 3.267\n"," \n","Epoch: 24/100 | Best Valid Score Until Now: 3.215 \n","\n","Save checkpoint\n","Epoch: 25/100 | Training Loss: 0.265 | Valid Score: 3.198\n"," \n","Epoch: 25/100 | Best Valid Score Until Now: 3.198 \n","\n","Patience 1\n","Epoch: 26/100 | Training Loss: 0.263 | Valid Score: 3.234\n"," \n","Epoch: 26/100 | Best Valid Score Until Now: 3.198 \n","\n","Save checkpoint\n","Epoch: 27/100 | Training Loss: 0.256 | Valid Score: 3.196\n"," \n","Epoch: 27/100 | Best Valid Score Until Now: 3.196 \n","\n","Patience 1\n","Epoch: 28/100 | Training Loss: 0.254 | Valid Score: 3.258\n"," \n","Epoch: 28/100 | Best Valid Score Until Now: 3.196 \n","\n","Patience 2\n","Epoch: 29/100 | Training Loss: 0.253 | Valid Score: 3.422\n"," \n","Epoch: 29/100 | Best Valid Score Until Now: 3.196 \n","\n","Patience 3\n","Epoch: 30/100 | Training Loss: 0.250 | Valid Score: 3.296\n"," \n","Epoch: 30/100 | Best Valid Score Until Now: 3.196 \n","\n","Patience 4\n","Epoch: 31/100 | Training Loss: 0.346 | Valid Score: 3.284\n"," \n","Epoch: 31/100 | Best Valid Score Until Now: 3.196 \n","\n","Patience 5\n","Epoch: 32/100 | Training Loss: 0.242 | Valid Score: 3.282\n"," \n","Epoch: 32/100 | Best Valid Score Until Now: 3.196 \n","\n","Patience 6\n","Epoch: 33/100 | Training Loss: 0.240 | Valid Score: 3.390\n"," \n","Epoch: 33/100 | Best Valid Score Until Now: 3.196 \n","\n","Patience 7\n","Epoch: 34/100 | Training Loss: 0.260 | Valid Score: 3.303\n"," \n","Epoch: 34/100 | Best Valid Score Until Now: 3.196 \n","\n","Save checkpoint\n","Epoch: 35/100 | Training Loss: 0.248 | Valid Score: 3.184\n"," \n","Epoch: 35/100 | Best Valid Score Until Now: 3.184 \n","\n","Patience 1\n","Epoch: 36/100 | Training Loss: 0.238 | Valid Score: 3.315\n"," \n","Epoch: 36/100 | Best Valid Score Until Now: 3.184 \n","\n","Patience 2\n","Epoch: 37/100 | Training Loss: 0.231 | Valid Score: 3.279\n"," \n","Epoch: 37/100 | Best Valid Score Until Now: 3.184 \n","\n","Patience 3\n","Epoch: 38/100 | Training Loss: 0.234 | Valid Score: 3.235\n"," \n","Epoch: 38/100 | Best Valid Score Until Now: 3.184 \n","\n","Patience 4\n","Epoch: 39/100 | Training Loss: 0.293 | Valid Score: 3.410\n"," \n","Epoch: 39/100 | Best Valid Score Until Now: 3.184 \n","\n","Patience 5\n","Epoch: 40/100 | Training Loss: 0.338 | Valid Score: 3.353\n"," \n","Epoch: 40/100 | Best Valid Score Until Now: 3.184 \n","\n","Patience 6\n","Epoch: 41/100 | Training Loss: 0.309 | Valid Score: 3.276\n"," \n","Epoch: 41/100 | Best Valid Score Until Now: 3.184 \n","\n","Patience 7\n","Epoch: 42/100 | Training Loss: 0.244 | Valid Score: 3.283\n"," \n","Epoch: 42/100 | Best Valid Score Until Now: 3.184 \n","\n","Patience 8\n","Epoch: 43/100 | Training Loss: 0.228 | Valid Score: 3.184\n"," \n","Epoch: 43/100 | Best Valid Score Until Now: 3.184 \n","\n","Patience 9\n","Epoch: 44/100 | Training Loss: 0.224 | Valid Score: 3.256\n"," \n","Epoch: 44/100 | Best Valid Score Until Now: 3.184 \n","\n","Patience 10\n","Epoch: 45/100 | Training Loss: 0.238 | Valid Score: 3.234\n"," \n","Epoch: 45/100 | Best Valid Score Until Now: 3.184 \n","\n","Final results:\n","Average Valid Score: 3.184 \n","\n","Test Score: 2.732 \n","\n","Execution time: 28.285 seconds\n"]}]},{"cell_type":"markdown","source":["# GAT 2"],"metadata":{"id":"Hl2BlGQxTQTn"}},{"cell_type":"code","source":["from dgl.nn.pytorch import GATConv\n","class GATConv2(nn.Module):\n","    def __init__(self, config, global_size=200, num_tasks=1):\n","        super().__init__()\n","        self.config = config\n","        self.num_tasks = num_tasks\n","\n","        # Node feature size\n","        self.node_feature_size = self.config.get('node_feature_size', 127)\n","\n","        # Edge feature size\n","        self.edge_feature_size = self.config.get('edge_feature_size', 12)\n","\n","        # Hidden size\n","        self.hidden_size = self.config.get('hidden_size', 100)\n","\n","        # Number of attention heads\n","        self.num_heads = self.config.get('num_heads', 1)\n","\n","        # Dropout probability\n","        self.dropout = self.config.get('dropout', 0.0)\n","\n","        # GAT layer\n","        self.conv1 = GATConv(self.node_feature_size,self.hidden_size,num_heads=self.num_heads,feat_drop=self.dropout,attn_drop=self.dropout,allow_zero_in_degree=True)\n","\n","        # Linear layer\n","        self.fc = nn.Linear(self.hidden_size * self.num_heads,self.hidden_size)\n","\n","        # GAT layer\n","        self.conv2 = GATConv(self.hidden_size,self.hidden_size,num_heads=1,feat_drop=self.dropout,attn_drop=self.dropout,allow_zero_in_degree=True)\n","\n","        # GAT layer\n","        self.conv3 = GATConv(self.hidden_size,self.num_tasks,num_heads=1,feat_drop=self.dropout,attn_drop=self.dropout,allow_zero_in_degree=True)\n","\n","    def forward(self, mol_dgl_graph, globals):\n","        mol_dgl_graph.ndata[\"v\"] = mol_dgl_graph.ndata[\"v\"][:, :self.node_feature_size]\n","        mol_dgl_graph.edata[\"e\"] = mol_dgl_graph.edata[\"e\"][:, :self.edge_feature_size]\n","\n","        # First GAT layer\n","        h = self.conv1(mol_dgl_graph, mol_dgl_graph.ndata[\"v\"]).flatten(1)\n","        h = F.relu(h)\n","        h = self.fc(h)\n","        h = F.dropout(h, p=self.dropout, training=self.training)\n","\n","        # Second GAT layer\n","        h = self.conv2(mol_dgl_graph, h).squeeze(1)\n","        h = F.relu(h)\n","        h = F.dropout(h, p=self.dropout, training=self.training)\n","\n","        # Third GAT layer\n","        if self.num_tasks == 1:\n","            h = self.conv3(mol_dgl_graph, h).squeeze(1)\n","        else:\n","            hs = []\n","            for i in range(self.num_tasks):\n","                hi = self.conv3(mol_dgl_graph, h).squeeze(1)\n","                hs.append(hi)\n","            h = torch.stack(hs, dim=1)\n","\n","        mol_dgl_graph.ndata[\"h\"] = h\n","\n","        if self.num_tasks == 1:\n","            return dgl.mean_nodes(mol_dgl_graph, \"h\")\n","        else:\n","            return dgl.mean_nodes(mol_dgl_graph, \"h\"), h"],"metadata":{"id":"FoDynBi1TQT0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def train_evaluate():\n","\n","    model = GATConv2(config, global_size, num_tasks)\n","    optimizer = torch.optim.Adam(model.parameters(), lr = 0.001)\n","\n","    best_val = float('inf')\n","    patience_count = 1\n","    epoch = 1\n","\n","    while epoch <= num_epochs:\n","        if patience_count <= patience:\n","            model.train()\n","            loss_train = train_epoch(train_dataloader, model, optimizer, num_tasks)\n","            model.eval()\n","            score_val = compute_score(model, val_dataloader, scaler, len(val_set), num_tasks)\n","            if score_val < best_val:\n","                best_val = score_val\n","                print(\"Save checkpoint\")\n","                path = os.path.join(checkpoint_path, 'checkpoint.pth')\n","                dict_checkpoint = {\"score_val\": score_val}\n","                dict_checkpoint.update({\"model_state_dict\": model.state_dict(), \"optimizer_state\": optimizer.state_dict()})\n","                with open(path, \"wb\") as outputfile:\n","                    cloudpickle.dump(dict_checkpoint, outputfile)\n","                patience_count = 1\n","            else:\n","                print(\"Patience\", patience_count)\n","                patience_count += 1\n","\n","            print(\"Epoch: {}/{} | Training Loss: {:.3f} | Valid Score: {:.3f}\".format(\n","            epoch, num_epochs, loss_train, score_val))\n","\n","            print(\" \")\n","            print(\"Epoch: {}/{} | Best Valid Score Until Now: {:.3f}\".format(epoch, num_epochs, best_val), \"\\n\")\n","        epoch += 1\n","\n","    # best model save\n","    shutil.rmtree(best_model_path, ignore_errors=True)\n","    shutil.copytree(checkpoint_path, best_model_path)\n","\n","    print(\"Final results:\")\n","    print(\"Average Valid Score: {:.3f}\".format(best_val), \"\\n\")\n"],"metadata":{"id":"w8thyfqvTFGm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8y9A0mqrTFGm"},"source":["##### Function to compute test set score of the final saved model"]},{"cell_type":"code","source":["def test_evaluate():\n","    final_model = GATConv2(config, global_size, num_tasks)\n","    path = os.path.join(best_model_path, 'checkpoint.pth')\n","    with open(path, 'rb') as f:\n","        checkpoint = cloudpickle.load(f)\n","    final_model.load_state_dict(checkpoint[\"model_state_dict\"])\n","    final_model.eval()\n","    test_score = compute_score(final_model, test_dataloader, scaler, len(test_set), num_tasks)\n","\n","    print(\"Test Score: {:.3f}\".format(test_score), \"\\n\")\n","    print(\"Execution time: {:.3f} seconds\".format(time.time() - start_time))"],"metadata":{"id":"TKbGb_UjTFGm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"M14boY7jTFGm"},"source":["##### Train the model and evaluate its performance"]},{"cell_type":"code","source":["import time\n","start_time = time.time()\n","\n","scaler = StandardScaler()\n","train_labels = train_set.labels.numpy()\n","scaler.fit(train_labels)\n","\n","train_evaluate()\n","test_evaluate()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1688565583336,"user_tz":-210,"elapsed":55679,"user":{"displayName":"Mahdi Salehi","userId":"13663744920024374425"}},"outputId":"c153371c-4689-405b-e764-59ef7a382e88","id":"xyE_O_uMTFGn"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Save checkpoint\n","Epoch: 1/100 | Training Loss: 0.844 | Valid Score: 4.449\n"," \n","Epoch: 1/100 | Best Valid Score Until Now: 4.449 \n","\n","Save checkpoint\n","Epoch: 2/100 | Training Loss: 0.670 | Valid Score: 4.281\n"," \n","Epoch: 2/100 | Best Valid Score Until Now: 4.281 \n","\n","Save checkpoint\n","Epoch: 3/100 | Training Loss: 0.635 | Valid Score: 4.155\n"," \n","Epoch: 3/100 | Best Valid Score Until Now: 4.155 \n","\n","Save checkpoint\n","Epoch: 4/100 | Training Loss: 0.585 | Valid Score: 4.100\n"," \n","Epoch: 4/100 | Best Valid Score Until Now: 4.100 \n","\n","Save checkpoint\n","Epoch: 5/100 | Training Loss: 0.594 | Valid Score: 3.968\n"," \n","Epoch: 5/100 | Best Valid Score Until Now: 3.968 \n","\n","Save checkpoint\n","Epoch: 6/100 | Training Loss: 0.581 | Valid Score: 3.941\n"," \n","Epoch: 6/100 | Best Valid Score Until Now: 3.941 \n","\n","Patience 1\n","Epoch: 7/100 | Training Loss: 0.585 | Valid Score: 4.045\n"," \n","Epoch: 7/100 | Best Valid Score Until Now: 3.941 \n","\n","Patience 2\n","Epoch: 8/100 | Training Loss: 0.516 | Valid Score: 4.096\n"," \n","Epoch: 8/100 | Best Valid Score Until Now: 3.941 \n","\n","Patience 3\n","Epoch: 9/100 | Training Loss: 0.532 | Valid Score: 4.009\n"," \n","Epoch: 9/100 | Best Valid Score Until Now: 3.941 \n","\n","Patience 4\n","Epoch: 10/100 | Training Loss: 0.469 | Valid Score: 4.108\n"," \n","Epoch: 10/100 | Best Valid Score Until Now: 3.941 \n","\n","Patience 5\n","Epoch: 11/100 | Training Loss: 0.470 | Valid Score: 3.978\n"," \n","Epoch: 11/100 | Best Valid Score Until Now: 3.941 \n","\n","Patience 6\n","Epoch: 12/100 | Training Loss: 0.436 | Valid Score: 4.121\n"," \n","Epoch: 12/100 | Best Valid Score Until Now: 3.941 \n","\n","Patience 7\n","Epoch: 13/100 | Training Loss: 0.435 | Valid Score: 4.065\n"," \n","Epoch: 13/100 | Best Valid Score Until Now: 3.941 \n","\n","Save checkpoint\n","Epoch: 14/100 | Training Loss: 0.459 | Valid Score: 3.899\n"," \n","Epoch: 14/100 | Best Valid Score Until Now: 3.899 \n","\n","Patience 1\n","Epoch: 15/100 | Training Loss: 0.419 | Valid Score: 4.047\n"," \n","Epoch: 15/100 | Best Valid Score Until Now: 3.899 \n","\n","Patience 2\n","Epoch: 16/100 | Training Loss: 0.412 | Valid Score: 4.306\n"," \n","Epoch: 16/100 | Best Valid Score Until Now: 3.899 \n","\n","Patience 3\n","Epoch: 17/100 | Training Loss: 0.401 | Valid Score: 3.956\n"," \n","Epoch: 17/100 | Best Valid Score Until Now: 3.899 \n","\n","Patience 4\n","Epoch: 18/100 | Training Loss: 0.408 | Valid Score: 4.062\n"," \n","Epoch: 18/100 | Best Valid Score Until Now: 3.899 \n","\n","Patience 5\n","Epoch: 19/100 | Training Loss: 0.378 | Valid Score: 4.110\n"," \n","Epoch: 19/100 | Best Valid Score Until Now: 3.899 \n","\n","Patience 6\n","Epoch: 20/100 | Training Loss: 0.371 | Valid Score: 3.907\n"," \n","Epoch: 20/100 | Best Valid Score Until Now: 3.899 \n","\n","Save checkpoint\n","Epoch: 21/100 | Training Loss: 0.389 | Valid Score: 3.879\n"," \n","Epoch: 21/100 | Best Valid Score Until Now: 3.879 \n","\n","Save checkpoint\n","Epoch: 22/100 | Training Loss: 0.370 | Valid Score: 3.685\n"," \n","Epoch: 22/100 | Best Valid Score Until Now: 3.685 \n","\n","Patience 1\n","Epoch: 23/100 | Training Loss: 0.351 | Valid Score: 3.907\n"," \n","Epoch: 23/100 | Best Valid Score Until Now: 3.685 \n","\n","Patience 2\n","Epoch: 24/100 | Training Loss: 0.356 | Valid Score: 3.859\n"," \n","Epoch: 24/100 | Best Valid Score Until Now: 3.685 \n","\n","Patience 3\n","Epoch: 25/100 | Training Loss: 0.342 | Valid Score: 3.967\n"," \n","Epoch: 25/100 | Best Valid Score Until Now: 3.685 \n","\n","Patience 4\n","Epoch: 26/100 | Training Loss: 0.450 | Valid Score: 3.785\n"," \n","Epoch: 26/100 | Best Valid Score Until Now: 3.685 \n","\n","Patience 5\n","Epoch: 27/100 | Training Loss: 0.341 | Valid Score: 4.035\n"," \n","Epoch: 27/100 | Best Valid Score Until Now: 3.685 \n","\n","Patience 6\n","Epoch: 28/100 | Training Loss: 0.349 | Valid Score: 3.870\n"," \n","Epoch: 28/100 | Best Valid Score Until Now: 3.685 \n","\n","Patience 7\n","Epoch: 29/100 | Training Loss: 0.465 | Valid Score: 3.859\n"," \n","Epoch: 29/100 | Best Valid Score Until Now: 3.685 \n","\n","Patience 8\n","Epoch: 30/100 | Training Loss: 0.313 | Valid Score: 3.912\n"," \n","Epoch: 30/100 | Best Valid Score Until Now: 3.685 \n","\n","Save checkpoint\n","Epoch: 31/100 | Training Loss: 0.321 | Valid Score: 3.587\n"," \n","Epoch: 31/100 | Best Valid Score Until Now: 3.587 \n","\n","Save checkpoint\n","Epoch: 32/100 | Training Loss: 0.348 | Valid Score: 3.473\n"," \n","Epoch: 32/100 | Best Valid Score Until Now: 3.473 \n","\n","Patience 1\n","Epoch: 33/100 | Training Loss: 0.328 | Valid Score: 3.609\n"," \n","Epoch: 33/100 | Best Valid Score Until Now: 3.473 \n","\n","Patience 2\n","Epoch: 34/100 | Training Loss: 0.301 | Valid Score: 3.913\n"," \n","Epoch: 34/100 | Best Valid Score Until Now: 3.473 \n","\n","Patience 3\n","Epoch: 35/100 | Training Loss: 0.323 | Valid Score: 3.612\n"," \n","Epoch: 35/100 | Best Valid Score Until Now: 3.473 \n","\n","Save checkpoint\n","Epoch: 36/100 | Training Loss: 0.355 | Valid Score: 3.411\n"," \n","Epoch: 36/100 | Best Valid Score Until Now: 3.411 \n","\n","Patience 1\n","Epoch: 37/100 | Training Loss: 0.293 | Valid Score: 3.479\n"," \n","Epoch: 37/100 | Best Valid Score Until Now: 3.411 \n","\n","Save checkpoint\n","Epoch: 38/100 | Training Loss: 0.291 | Valid Score: 3.282\n"," \n","Epoch: 38/100 | Best Valid Score Until Now: 3.282 \n","\n","Patience 1\n","Epoch: 39/100 | Training Loss: 0.286 | Valid Score: 3.611\n"," \n","Epoch: 39/100 | Best Valid Score Until Now: 3.282 \n","\n","Patience 2\n","Epoch: 40/100 | Training Loss: 0.280 | Valid Score: 3.553\n"," \n","Epoch: 40/100 | Best Valid Score Until Now: 3.282 \n","\n","Patience 3\n","Epoch: 41/100 | Training Loss: 0.274 | Valid Score: 3.736\n"," \n","Epoch: 41/100 | Best Valid Score Until Now: 3.282 \n","\n","Save checkpoint\n","Epoch: 42/100 | Training Loss: 0.275 | Valid Score: 3.104\n"," \n","Epoch: 42/100 | Best Valid Score Until Now: 3.104 \n","\n","Patience 1\n","Epoch: 43/100 | Training Loss: 0.270 | Valid Score: 3.320\n"," \n","Epoch: 43/100 | Best Valid Score Until Now: 3.104 \n","\n","Patience 2\n","Epoch: 44/100 | Training Loss: 0.268 | Valid Score: 3.115\n"," \n","Epoch: 44/100 | Best Valid Score Until Now: 3.104 \n","\n","Save checkpoint\n","Epoch: 45/100 | Training Loss: 0.258 | Valid Score: 3.050\n"," \n","Epoch: 45/100 | Best Valid Score Until Now: 3.050 \n","\n","Patience 1\n","Epoch: 46/100 | Training Loss: 0.417 | Valid Score: 3.522\n"," \n","Epoch: 46/100 | Best Valid Score Until Now: 3.050 \n","\n","Patience 2\n","Epoch: 47/100 | Training Loss: 0.421 | Valid Score: 4.006\n"," \n","Epoch: 47/100 | Best Valid Score Until Now: 3.050 \n","\n","Patience 3\n","Epoch: 48/100 | Training Loss: 0.418 | Valid Score: 4.096\n"," \n","Epoch: 48/100 | Best Valid Score Until Now: 3.050 \n","\n","Patience 4\n","Epoch: 49/100 | Training Loss: 0.278 | Valid Score: 3.199\n"," \n","Epoch: 49/100 | Best Valid Score Until Now: 3.050 \n","\n","Patience 5\n","Epoch: 50/100 | Training Loss: 0.389 | Valid Score: 4.079\n"," \n","Epoch: 50/100 | Best Valid Score Until Now: 3.050 \n","\n","Save checkpoint\n","Epoch: 51/100 | Training Loss: 0.406 | Valid Score: 2.933\n"," \n","Epoch: 51/100 | Best Valid Score Until Now: 2.933 \n","\n","Patience 1\n","Epoch: 52/100 | Training Loss: 0.279 | Valid Score: 3.087\n"," \n","Epoch: 52/100 | Best Valid Score Until Now: 2.933 \n","\n","Save checkpoint\n","Epoch: 53/100 | Training Loss: 0.258 | Valid Score: 2.801\n"," \n","Epoch: 53/100 | Best Valid Score Until Now: 2.801 \n","\n","Patience 1\n","Epoch: 54/100 | Training Loss: 0.255 | Valid Score: 2.943\n"," \n","Epoch: 54/100 | Best Valid Score Until Now: 2.801 \n","\n","Patience 2\n","Epoch: 55/100 | Training Loss: 0.239 | Valid Score: 2.829\n"," \n","Epoch: 55/100 | Best Valid Score Until Now: 2.801 \n","\n","Patience 3\n","Epoch: 56/100 | Training Loss: 0.238 | Valid Score: 2.849\n"," \n","Epoch: 56/100 | Best Valid Score Until Now: 2.801 \n","\n","Save checkpoint\n","Epoch: 57/100 | Training Loss: 0.238 | Valid Score: 2.734\n"," \n","Epoch: 57/100 | Best Valid Score Until Now: 2.734 \n","\n","Patience 1\n","Epoch: 58/100 | Training Loss: 0.325 | Valid Score: 2.797\n"," \n","Epoch: 58/100 | Best Valid Score Until Now: 2.734 \n","\n","Patience 2\n","Epoch: 59/100 | Training Loss: 0.239 | Valid Score: 2.758\n"," \n","Epoch: 59/100 | Best Valid Score Until Now: 2.734 \n","\n","Save checkpoint\n","Epoch: 60/100 | Training Loss: 0.285 | Valid Score: 2.687\n"," \n","Epoch: 60/100 | Best Valid Score Until Now: 2.687 \n","\n","Patience 1\n","Epoch: 61/100 | Training Loss: 0.258 | Valid Score: 3.067\n"," \n","Epoch: 61/100 | Best Valid Score Until Now: 2.687 \n","\n","Save checkpoint\n","Epoch: 62/100 | Training Loss: 0.292 | Valid Score: 2.653\n"," \n","Epoch: 62/100 | Best Valid Score Until Now: 2.653 \n","\n","Patience 1\n","Epoch: 63/100 | Training Loss: 0.285 | Valid Score: 3.790\n"," \n","Epoch: 63/100 | Best Valid Score Until Now: 2.653 \n","\n","Patience 2\n","Epoch: 64/100 | Training Loss: 0.309 | Valid Score: 3.503\n"," \n","Epoch: 64/100 | Best Valid Score Until Now: 2.653 \n","\n","Patience 3\n","Epoch: 65/100 | Training Loss: 0.243 | Valid Score: 3.094\n"," \n","Epoch: 65/100 | Best Valid Score Until Now: 2.653 \n","\n","Patience 4\n","Epoch: 66/100 | Training Loss: 0.225 | Valid Score: 2.942\n"," \n","Epoch: 66/100 | Best Valid Score Until Now: 2.653 \n","\n","Patience 5\n","Epoch: 67/100 | Training Loss: 0.230 | Valid Score: 2.866\n"," \n","Epoch: 67/100 | Best Valid Score Until Now: 2.653 \n","\n","Patience 6\n","Epoch: 68/100 | Training Loss: 0.252 | Valid Score: 2.783\n"," \n","Epoch: 68/100 | Best Valid Score Until Now: 2.653 \n","\n","Patience 7\n","Epoch: 69/100 | Training Loss: 0.213 | Valid Score: 2.932\n"," \n","Epoch: 69/100 | Best Valid Score Until Now: 2.653 \n","\n","Patience 8\n","Epoch: 70/100 | Training Loss: 0.221 | Valid Score: 2.698\n"," \n","Epoch: 70/100 | Best Valid Score Until Now: 2.653 \n","\n","Save checkpoint\n","Epoch: 71/100 | Training Loss: 0.266 | Valid Score: 2.629\n"," \n","Epoch: 71/100 | Best Valid Score Until Now: 2.629 \n","\n","Patience 1\n","Epoch: 72/100 | Training Loss: 0.241 | Valid Score: 2.732\n"," \n","Epoch: 72/100 | Best Valid Score Until Now: 2.629 \n","\n","Patience 2\n","Epoch: 73/100 | Training Loss: 0.215 | Valid Score: 2.649\n"," \n","Epoch: 73/100 | Best Valid Score Until Now: 2.629 \n","\n","Patience 3\n","Epoch: 74/100 | Training Loss: 0.210 | Valid Score: 2.667\n"," \n","Epoch: 74/100 | Best Valid Score Until Now: 2.629 \n","\n","Patience 4\n","Epoch: 75/100 | Training Loss: 0.210 | Valid Score: 2.702\n"," \n","Epoch: 75/100 | Best Valid Score Until Now: 2.629 \n","\n","Patience 5\n","Epoch: 76/100 | Training Loss: 0.201 | Valid Score: 2.675\n"," \n","Epoch: 76/100 | Best Valid Score Until Now: 2.629 \n","\n","Patience 6\n","Epoch: 77/100 | Training Loss: 0.193 | Valid Score: 2.729\n"," \n","Epoch: 77/100 | Best Valid Score Until Now: 2.629 \n","\n","Patience 7\n","Epoch: 78/100 | Training Loss: 0.187 | Valid Score: 2.718\n"," \n","Epoch: 78/100 | Best Valid Score Until Now: 2.629 \n","\n","Patience 8\n","Epoch: 79/100 | Training Loss: 0.258 | Valid Score: 2.709\n"," \n","Epoch: 79/100 | Best Valid Score Until Now: 2.629 \n","\n","Patience 9\n","Epoch: 80/100 | Training Loss: 0.191 | Valid Score: 2.957\n"," \n","Epoch: 80/100 | Best Valid Score Until Now: 2.629 \n","\n","Patience 10\n","Epoch: 81/100 | Training Loss: 0.213 | Valid Score: 2.826\n"," \n","Epoch: 81/100 | Best Valid Score Until Now: 2.629 \n","\n","Final results:\n","Average Valid Score: 2.629 \n","\n","Test Score: 2.976 \n","\n","Execution time: 55.642 seconds\n"]}]},{"cell_type":"markdown","source":["#GAT 3\n"],"metadata":{"id":"63aZjT9reVjF"}},{"cell_type":"code","source":["from dgl.nn.pytorch import GATConv\n","from torch.nn import BatchNorm1d\n","\n","class GATConv3(nn.Module):\n","    def __init__(self, config, global_size=200, num_tasks=1):\n","        super().__init__()\n","        self.config = config\n","        self.num_tasks = num_tasks\n","\n","        # Node feature size\n","        self.node_feature_size = self.config.get('node_feature_size', 127)\n","\n","        # Edge feature size\n","        self.edge_feature_size = self.config.get('edge_feature_size', 12)\n","\n","        # Hidden size\n","        self.hidden_size = self.config.get('hidden_size', 100)\n","\n","        # Number of attention heads\n","        self.num_heads = self.config.get('num_heads', 1)\n","\n","        # Dropout probability\n","        self.dropout = self.config.get('dropout', 0.0)\n","\n","        # GAT layer\n","        self.conv1 = GATConv(self.node_feature_size, 64, num_heads=self.num_heads,\n","                             feat_drop=self.dropout, attn_drop=self.dropout,\n","                             allow_zero_in_degree=True)\n","        self.conv2 = GATConv(64, 128, num_heads=1, feat_drop=self.dropout,\n","                             attn_drop=self.dropout, allow_zero_in_degree=True)\n","        self.conv3 = GATConv(128, 256, num_heads=1, feat_drop=self.dropout,\n","                             attn_drop=self.dropout, allow_zero_in_degree=True)\n","        self.conv4 = GATConv(256, 128, num_heads=1, feat_drop=self.dropout,\n","                             attn_drop=self.dropout, allow_zero_in_degree=True)\n","        self.conv5 = GATConv(128, self.num_tasks, num_heads=1, feat_drop=self.dropout,\n","                             attn_drop=self.dropout, allow_zero_in_degree=True)\n","\n","        # Linear layer\n","        self.fc1 = nn.Linear(64 * self.num_heads, 64)\n","        self.bn1 = BatchNorm1d(64)\n","        self.fc2 = nn.Linear(128 * self.num_heads, 128)\n","        self.bn2 = BatchNorm1d(128)\n","        self.fc3 = nn.Linear(256 * self.num_heads, 256)\n","        self.bn3 = BatchNorm1d(256)\n","\n","    def forward(self, mol_dgl_graph, globals):\n","        mol_dgl_graph.ndata[\"v\"] = mol_dgl_graph.ndata[\"v\"][:, :self.node_feature_size]\n","        mol_dgl_graph.edata[\"e\"] = mol_dgl_graph.edata[\"e\"][:, :self.edge_feature_size]\n","\n","        # First GAT layer\n","        h = self.conv1(mol_dgl_graph, mol_dgl_graph.ndata[\"v\"]).flatten(1)\n","        h = F.relu(h)\n","        h = self.fc1(h)\n","        #h = self.bn1(h)  # add batch normalization\n","        h = F.dropout(h, p=self.dropout, training=self.training)\n","\n","        # Second GAT layer\n","        h = self.conv2(mol_dgl_graph, h).squeeze(1)\n","        h = F.relu(h)\n","        h = self.fc2(h)\n","        #h = self.bn2(h)  # add batch normalization\n","        h = F.dropout(h, p=self.dropout, training=self.training)\n","\n","        h = self.conv3(mol_dgl_graph, h).squeeze(1)\n","        h = F.relu(h)\n","        h = self.fc3(h)\n","        #h = self.bn3(h)  # add batch normalization\n","        h = F.dropout(h, p=self.dropout, training=self.training)\n","\n","        h = self.conv4(mol_dgl_graph, h).squeeze(1)\n","        h = F.relu(h)\n","        h = self.fc2(h)\n","        #h = self.bn2(h)  # add batch normalization\n","        h = F.dropout(h, p=self.dropout, training=self.training)\n","\n","        # Third GAT layer\n","        if self.num_tasks == 1:\n","            h = self.conv5(mol_dgl_graph, h).squeeze(1)\n","        else:\n","            hs = []\n","            for i in range(self.num_tasks):\n","                hi = self.conv3(mol_dgl_graph, h).squeeze(1)\n","                hs.append(hi)\n","            h = torch.stack(hs, dim=1)\n","\n","        mol_dgl_graph.ndata[\"h\"]= h\n","\n","        if self.num_tasks == 1:\n","            return dgl.mean_nodes(mol_dgl_graph, \"h\")\n","        else:\n","            return dgl.mean_nodes(mol_dgl_graph, \"h\"), h"],"metadata":{"id":"MD_PBPqA-_EB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def train_evaluate():\n","\n","    model = GATConv3(config, global_size, num_tasks)\n","    optimizer = torch.optim.Adam(model.parameters(), lr = 0.001)\n","\n","    best_val = float('inf')\n","    patience_count = 1\n","    epoch = 1\n","\n","    while epoch <= num_epochs:\n","        if patience_count <= patience:\n","            model.train()\n","            loss_train = train_epoch(train_dataloader, model, optimizer, num_tasks)\n","            model.eval()\n","            score_val = compute_score(model, val_dataloader, scaler, len(val_set), num_tasks)\n","            if score_val < best_val:\n","                best_val = score_val\n","                print(\"Save checkpoint\")\n","                path = os.path.join(checkpoint_path, 'checkpoint.pth')\n","                dict_checkpoint = {\"score_val\": score_val}\n","                dict_checkpoint.update({\"model_state_dict\": model.state_dict(), \"optimizer_state\": optimizer.state_dict()})\n","                with open(path, \"wb\") as outputfile:\n","                    cloudpickle.dump(dict_checkpoint, outputfile)\n","                patience_count = 1\n","            else:\n","                print(\"Patience\", patience_count)\n","                patience_count += 1\n","\n","            print(\"Epoch: {}/{} | Training Loss: {:.3f} | Valid Score: {:.3f}\".format(\n","            epoch, num_epochs, loss_train, score_val))\n","\n","            print(\" \")\n","            print(\"Epoch: {}/{} | Best Valid Score Until Now: {:.3f}\".format(epoch, num_epochs, best_val), \"\\n\")\n","        epoch += 1\n","\n","    # best model save\n","    shutil.rmtree(best_model_path, ignore_errors=True)\n","    shutil.copytree(checkpoint_path, best_model_path)\n","\n","    print(\"Final results:\")\n","    print(\"Average Valid Score: {:.3f}\".format(best_val), \"\\n\")\n"],"metadata":{"id":"jBiq-rl2TKX6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DCv7BwFRTKX6"},"source":["##### Function to compute test set score of the final saved model"]},{"cell_type":"code","source":["def test_evaluate():\n","    final_model = GATConv3(config, global_size, num_tasks)\n","    path = os.path.join(best_model_path, 'checkpoint.pth')\n","    with open(path, 'rb') as f:\n","        checkpoint = cloudpickle.load(f)\n","    final_model.load_state_dict(checkpoint[\"model_state_dict\"])\n","    final_model.eval()\n","    test_score = compute_score(final_model, test_dataloader, scaler, len(test_set), num_tasks)\n","\n","    print(\"Test Score: {:.3f}\".format(test_score), \"\\n\")\n","    print(\"Execution time: {:.3f} seconds\".format(time.time() - start_time))"],"metadata":{"id":"qPdFI5AgTKX6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_2evYV3_TKX7"},"source":["##### Train the model and evaluate its performance"]},{"cell_type":"code","source":["import time\n","start_time = time.time()\n","\n","scaler = StandardScaler()\n","train_labels = train_set.labels.numpy()\n","scaler.fit(train_labels)\n","\n","train_evaluate()\n","test_evaluate()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1688565681126,"user_tz":-210,"elapsed":51856,"user":{"displayName":"Mahdi Salehi","userId":"13663744920024374425"}},"outputId":"253c1c8a-731c-4e80-caa0-f682345ebe7c","id":"7B0bcuDyTKX7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Save checkpoint\n","Epoch: 1/100 | Training Loss: 0.925 | Valid Score: 4.272\n"," \n","Epoch: 1/100 | Best Valid Score Until Now: 4.272 \n","\n","Patience 1\n","Epoch: 2/100 | Training Loss: 0.770 | Valid Score: 4.447\n"," \n","Epoch: 2/100 | Best Valid Score Until Now: 4.272 \n","\n","Patience 2\n","Epoch: 3/100 | Training Loss: 0.660 | Valid Score: 4.399\n"," \n","Epoch: 3/100 | Best Valid Score Until Now: 4.272 \n","\n","Patience 3\n","Epoch: 4/100 | Training Loss: 0.647 | Valid Score: 4.372\n"," \n","Epoch: 4/100 | Best Valid Score Until Now: 4.272 \n","\n","Save checkpoint\n","Epoch: 5/100 | Training Loss: 0.571 | Valid Score: 4.161\n"," \n","Epoch: 5/100 | Best Valid Score Until Now: 4.161 \n","\n","Patience 1\n","Epoch: 6/100 | Training Loss: 0.630 | Valid Score: 4.378\n"," \n","Epoch: 6/100 | Best Valid Score Until Now: 4.161 \n","\n","Save checkpoint\n","Epoch: 7/100 | Training Loss: 0.531 | Valid Score: 4.148\n"," \n","Epoch: 7/100 | Best Valid Score Until Now: 4.148 \n","\n","Patience 1\n","Epoch: 8/100 | Training Loss: 0.550 | Valid Score: 4.274\n"," \n","Epoch: 8/100 | Best Valid Score Until Now: 4.148 \n","\n","Patience 2\n","Epoch: 9/100 | Training Loss: 0.569 | Valid Score: 4.180\n"," \n","Epoch: 9/100 | Best Valid Score Until Now: 4.148 \n","\n","Patience 3\n","Epoch: 10/100 | Training Loss: 0.594 | Valid Score: 4.369\n"," \n","Epoch: 10/100 | Best Valid Score Until Now: 4.148 \n","\n","Save checkpoint\n","Epoch: 11/100 | Training Loss: 0.495 | Valid Score: 3.993\n"," \n","Epoch: 11/100 | Best Valid Score Until Now: 3.993 \n","\n","Patience 1\n","Epoch: 12/100 | Training Loss: 0.462 | Valid Score: 4.172\n"," \n","Epoch: 12/100 | Best Valid Score Until Now: 3.993 \n","\n","Patience 2\n","Epoch: 13/100 | Training Loss: 0.440 | Valid Score: 4.294\n"," \n","Epoch: 13/100 | Best Valid Score Until Now: 3.993 \n","\n","Save checkpoint\n","Epoch: 14/100 | Training Loss: 0.410 | Valid Score: 3.966\n"," \n","Epoch: 14/100 | Best Valid Score Until Now: 3.966 \n","\n","Patience 1\n","Epoch: 15/100 | Training Loss: 0.400 | Valid Score: 4.064\n"," \n","Epoch: 15/100 | Best Valid Score Until Now: 3.966 \n","\n","Save checkpoint\n","Epoch: 16/100 | Training Loss: 0.386 | Valid Score: 3.608\n"," \n","Epoch: 16/100 | Best Valid Score Until Now: 3.608 \n","\n","Save checkpoint\n","Epoch: 17/100 | Training Loss: 0.521 | Valid Score: 3.495\n"," \n","Epoch: 17/100 | Best Valid Score Until Now: 3.495 \n","\n","Patience 1\n","Epoch: 18/100 | Training Loss: 0.394 | Valid Score: 3.701\n"," \n","Epoch: 18/100 | Best Valid Score Until Now: 3.495 \n","\n","Patience 2\n","Epoch: 19/100 | Training Loss: 0.562 | Valid Score: 3.690\n"," \n","Epoch: 19/100 | Best Valid Score Until Now: 3.495 \n","\n","Patience 3\n","Epoch: 20/100 | Training Loss: 0.503 | Valid Score: 3.993\n"," \n","Epoch: 20/100 | Best Valid Score Until Now: 3.495 \n","\n","Patience 4\n","Epoch: 21/100 | Training Loss: 0.605 | Valid Score: 4.047\n"," \n","Epoch: 21/100 | Best Valid Score Until Now: 3.495 \n","\n","Patience 5\n","Epoch: 22/100 | Training Loss: 0.566 | Valid Score: 4.354\n"," \n","Epoch: 22/100 | Best Valid Score Until Now: 3.495 \n","\n","Patience 6\n","Epoch: 23/100 | Training Loss: 0.468 | Valid Score: 3.634\n"," \n","Epoch: 23/100 | Best Valid Score Until Now: 3.495 \n","\n","Patience 7\n","Epoch: 24/100 | Training Loss: 0.399 | Valid Score: 3.589\n"," \n","Epoch: 24/100 | Best Valid Score Until Now: 3.495 \n","\n","Save checkpoint\n","Epoch: 25/100 | Training Loss: 0.377 | Valid Score: 3.480\n"," \n","Epoch: 25/100 | Best Valid Score Until Now: 3.480 \n","\n","Patience 1\n","Epoch: 26/100 | Training Loss: 0.441 | Valid Score: 3.670\n"," \n","Epoch: 26/100 | Best Valid Score Until Now: 3.480 \n","\n","Save checkpoint\n","Epoch: 27/100 | Training Loss: 0.564 | Valid Score: 3.468\n"," \n","Epoch: 27/100 | Best Valid Score Until Now: 3.468 \n","\n","Patience 1\n","Epoch: 28/100 | Training Loss: 0.424 | Valid Score: 3.539\n"," \n","Epoch: 28/100 | Best Valid Score Until Now: 3.468 \n","\n","Patience 2\n","Epoch: 29/100 | Training Loss: 0.404 | Valid Score: 3.566\n"," \n","Epoch: 29/100 | Best Valid Score Until Now: 3.468 \n","\n","Save checkpoint\n","Epoch: 30/100 | Training Loss: 0.358 | Valid Score: 3.273\n"," \n","Epoch: 30/100 | Best Valid Score Until Now: 3.273 \n","\n","Patience 1\n","Epoch: 31/100 | Training Loss: 0.352 | Valid Score: 3.324\n"," \n","Epoch: 31/100 | Best Valid Score Until Now: 3.273 \n","\n","Save checkpoint\n","Epoch: 32/100 | Training Loss: 0.298 | Valid Score: 3.146\n"," \n","Epoch: 32/100 | Best Valid Score Until Now: 3.146 \n","\n","Patience 1\n","Epoch: 33/100 | Training Loss: 0.415 | Valid Score: 3.256\n"," \n","Epoch: 33/100 | Best Valid Score Until Now: 3.146 \n","\n","Save checkpoint\n","Epoch: 34/100 | Training Loss: 0.300 | Valid Score: 2.805\n"," \n","Epoch: 34/100 | Best Valid Score Until Now: 2.805 \n","\n","Patience 1\n","Epoch: 35/100 | Training Loss: 0.290 | Valid Score: 3.014\n"," \n","Epoch: 35/100 | Best Valid Score Until Now: 2.805 \n","\n","Patience 2\n","Epoch: 36/100 | Training Loss: 0.260 | Valid Score: 4.155\n"," \n","Epoch: 36/100 | Best Valid Score Until Now: 2.805 \n","\n","Patience 3\n","Epoch: 37/100 | Training Loss: 0.315 | Valid Score: 3.094\n"," \n","Epoch: 37/100 | Best Valid Score Until Now: 2.805 \n","\n","Patience 4\n","Epoch: 38/100 | Training Loss: 0.352 | Valid Score: 4.050\n"," \n","Epoch: 38/100 | Best Valid Score Until Now: 2.805 \n","\n","Save checkpoint\n","Epoch: 39/100 | Training Loss: 0.303 | Valid Score: 2.600\n"," \n","Epoch: 39/100 | Best Valid Score Until Now: 2.600 \n","\n","Patience 1\n","Epoch: 40/100 | Training Loss: 0.246 | Valid Score: 2.975\n"," \n","Epoch: 40/100 | Best Valid Score Until Now: 2.600 \n","\n","Patience 2\n","Epoch: 41/100 | Training Loss: 0.232 | Valid Score: 2.740\n"," \n","Epoch: 41/100 | Best Valid Score Until Now: 2.600 \n","\n","Patience 3\n","Epoch: 42/100 | Training Loss: 0.256 | Valid Score: 2.712\n"," \n","Epoch: 42/100 | Best Valid Score Until Now: 2.600 \n","\n","Patience 4\n","Epoch: 43/100 | Training Loss: 0.239 | Valid Score: 3.979\n"," \n","Epoch: 43/100 | Best Valid Score Until Now: 2.600 \n","\n","Patience 5\n","Epoch: 44/100 | Training Loss: 0.267 | Valid Score: 3.902\n"," \n","Epoch: 44/100 | Best Valid Score Until Now: 2.600 \n","\n","Patience 6\n","Epoch: 45/100 | Training Loss: 0.264 | Valid Score: 3.083\n"," \n","Epoch: 45/100 | Best Valid Score Until Now: 2.600 \n","\n","Patience 7\n","Epoch: 46/100 | Training Loss: 0.264 | Valid Score: 3.287\n"," \n","Epoch: 46/100 | Best Valid Score Until Now: 2.600 \n","\n","Save checkpoint\n","Epoch: 47/100 | Training Loss: 0.271 | Valid Score: 2.561\n"," \n","Epoch: 47/100 | Best Valid Score Until Now: 2.561 \n","\n","Patience 1\n","Epoch: 48/100 | Training Loss: 0.217 | Valid Score: 2.922\n"," \n","Epoch: 48/100 | Best Valid Score Until Now: 2.561 \n","\n","Patience 2\n","Epoch: 49/100 | Training Loss: 0.277 | Valid Score: 2.778\n"," \n","Epoch: 49/100 | Best Valid Score Until Now: 2.561 \n","\n","Patience 3\n","Epoch: 50/100 | Training Loss: 0.201 | Valid Score: 2.928\n"," \n","Epoch: 50/100 | Best Valid Score Until Now: 2.561 \n","\n","Patience 4\n","Epoch: 51/100 | Training Loss: 0.217 | Valid Score: 2.718\n"," \n","Epoch: 51/100 | Best Valid Score Until Now: 2.561 \n","\n","Patience 5\n","Epoch: 52/100 | Training Loss: 0.198 | Valid Score: 2.887\n"," \n","Epoch: 52/100 | Best Valid Score Until Now: 2.561 \n","\n","Patience 6\n","Epoch: 53/100 | Training Loss: 0.194 | Valid Score: 3.412\n"," \n","Epoch: 53/100 | Best Valid Score Until Now: 2.561 \n","\n","Patience 7\n","Epoch: 54/100 | Training Loss: 0.197 | Valid Score: 3.151\n"," \n","Epoch: 54/100 | Best Valid Score Until Now: 2.561 \n","\n","Patience 8\n","Epoch: 55/100 | Training Loss: 0.183 | Valid Score: 3.188\n"," \n","Epoch: 55/100 | Best Valid Score Until Now: 2.561 \n","\n","Patience 9\n","Epoch: 56/100 | Training Loss: 0.184 | Valid Score: 3.152\n"," \n","Epoch: 56/100 | Best Valid Score Until Now: 2.561 \n","\n","Patience 10\n","Epoch: 57/100 | Training Loss: 0.194 | Valid Score: 3.290\n"," \n","Epoch: 57/100 | Best Valid Score Until Now: 2.561 \n","\n","Final results:\n","Average Valid Score: 2.561 \n","\n","Test Score: 2.847 \n","\n","Execution time: 51.818 seconds\n"]}]},{"cell_type":"markdown","source":["#My architecture"],"metadata":{"id":"KsJxdJT0prPO"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","class GraphClassifier(nn.Module):\n","    def __init__(self, config, global_size=200, num_tasks=1):\n","        super().__init__()\n","        self.config = config\n","        self.num_tasks = num_tasks\n","        # Node feature size\n","        self.node_feature_size = self.config.get('node_feature_size', 127)\n","\n","        # Edge feature size\n","        self.edge_feature_size = self.config.get('edge_feature_size', 12)\n","\n","        self.conv1 = nn.Conv2d(1, 128, kernel_size=(1, self.node_feature_size))\n","        self.bn1 = nn.BatchNorm2d(128) # Batch Normalization\n","        self.dropout1 = nn.Dropout(p=0.1) # Dropout\n","        self.conv2 = nn.Conv2d(128, 256, kernel_size=(1, 1))\n","        self.bn2 = nn.BatchNorm2d(256) # Batch Normalization\n","        self.dropout2 = nn.Dropout(p=0.1) # Dropout\n","        self.conv3 = nn.Conv2d(256,512,kernel_size=(1,1))\n","        self.bn3 = nn.BatchNorm2d(512) # Batch Normalization\n","        self.dropout3 = nn.Dropout(p=0.2) # Dropout\n","        self.conv4 = nn.Conv2d(512,256,kernel_size=(1,1))\n","        self.bn4 = nn.BatchNorm2d(256) # Batch Normalization\n","        self.dropout4 = nn.Dropout(p=0.1) # Dropout\n","        self.conv5 = nn.Conv2d(256,128,kernel_size=(1,1))\n","        self.bn5 = nn.BatchNorm2d(128) # Batch Normalization\n","        self.dropout5 = nn.Dropout(p=0.1) # Dropout\n","        self.fc1 = nn.Linear(128, 64)\n","        self.bn6 = nn.BatchNorm1d(64) # Batch Normalization\n","        self.dropout6 = nn.Dropout(p=0.0) # Dropout\n","        self.fc2 = nn.Linear(64, 32)\n","        self.bn7 = nn.BatchNorm1d(32) # Batch Normalization\n","        self.dropout7 = nn.Dropout(p=0.0) # Dropout\n","        self.fc3 = nn.Linear(32,num_tasks)\n","\n","    def forward(self, mol_dgl_graph, globals):\n","        # The forward method takes a molecular graph and a global feature vector as input\n","        mol_dgl_graph.ndata[\"v\"] = mol_dgl_graph.ndata[\"v\"][:, :self.node_feature_size]\n","        # Truncate the node feature size to the specified size\n","        mol_dgl_graph.edata[\"e\"] = mol_dgl_graph.edata[\"e\"][:, :self.edge_feature_size]\n","        # Truncate the edge feature size to the specified size\n","        x = mol_dgl_graph.ndata[\"v\"].unsqueeze(1)\n","        x = x.view(-1, 1, mol_dgl_graph.number_of_nodes(), self.node_feature_size)\n","        x = self.conv1(x)\n","        #x = self.bn1(x) # Batch Normalization\n","        x = F.relu(x)\n","        x = self.dropout1(x) # Dropout\n","        x = self.conv2(x)\n","        #x = self.bn2(x) # Batch Normalization\n","        x = F.relu(x)\n","        x = self.dropout2(x) # Dropout\n","        x = self.conv3(x)\n","        #x = self.bn3(x) # Batch Normalization\n","        x = F.relu(x)\n","        x = self.dropout3(x) # Dropout\n","        x = self.conv4(x)\n","        #x = self.bn4(x) # Batch Normalization\n","        x = F.relu(x)\n","        x = self.dropout4(x) # Dropout\n","        x = self.conv5(x)\n","        #x = self.bn5(x) # Batch Normalization\n","        x = F.relu(x)\n","        x = self.dropout5(x) # Dropout\n","        x = torch.max(x, dim=-1)[0]\n","        x = x.view(-1, 128)\n","        x = self.fc1(x)\n","        #x = self.bn6(x) # Batch Normalization\n","        x = F.relu(x)\n","        x = self.dropout6(x) # Dropout\n","        x = self.fc2(x)\n","        #x = self.bn7(x) # Batch Normalization\n","        x = F.relu(x)\n","        x = self.dropout7(x) # Dropout\n","        x = self.fc3(x)\n","        mol_dgl_graph.ndata[\"h\"] = x\n","\n","        return dgl.mean_nodes(mol_dgl_graph, \"h\")"],"metadata":{"id":"mv8CH2svvOrQ","executionInfo":{"status":"ok","timestamp":1689006062277,"user_tz":-210,"elapsed":5,"user":{"displayName":"Mahdi Salehi","userId":"13663744920024374425"}}},"execution_count":23,"outputs":[]},{"cell_type":"code","source":["def train_evaluate():\n","\n","    model = GraphClassifier(config, global_size, num_tasks)\n","    optimizer = torch.optim.Adam(model.parameters(), lr = 0.001)\n","\n","    best_val = float('inf')\n","    patience_count = 1\n","    epoch = 1\n","\n","    while epoch <= num_epochs:\n","        if patience_count <= patience:\n","            model.train()\n","            loss_train = train_epoch(train_dataloader, model, optimizer, num_tasks)\n","            model.eval()\n","            score_val = compute_score(model, val_dataloader, scaler, len(val_set), num_tasks)\n","            if score_val < best_val:\n","                best_val = score_val\n","                print(\"Save checkpoint\")\n","                path = os.path.join(checkpoint_path, 'checkpoint.pth')\n","                dict_checkpoint = {\"score_val\": score_val}\n","                dict_checkpoint.update({\"model_state_dict\": model.state_dict(), \"optimizer_state\": optimizer.state_dict()})\n","                with open(path, \"wb\") as outputfile:\n","                    cloudpickle.dump(dict_checkpoint, outputfile)\n","                patience_count = 1\n","            else:\n","                print(\"Patience\", patience_count)\n","                patience_count += 1\n","\n","            print(\"Epoch: {}/{} | Training Loss: {:.3f} | Valid Score: {:.3f}\".format(\n","            epoch, num_epochs, loss_train, score_val))\n","\n","            print(\" \")\n","            print(\"Epoch: {}/{} | Best Valid Score Until Now: {:.3f}\".format(epoch, num_epochs, best_val), \"\\n\")\n","        epoch += 1\n","\n","    # best model save\n","    shutil.rmtree(best_model_path, ignore_errors=True)\n","    shutil.copytree(checkpoint_path, best_model_path)\n","\n","    print(\"Final results:\")\n","    print(\"Average Valid Score: {:.3f}\".format(best_val), \"\\n\")\n"],"metadata":{"id":"8pV30VhWmOQG","executionInfo":{"status":"ok","timestamp":1689006062277,"user_tz":-210,"elapsed":3,"user":{"displayName":"Mahdi Salehi","userId":"13663744920024374425"}}},"execution_count":24,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wChs1iWAmOQH"},"source":["##### Function to compute test set score of the final saved model"]},{"cell_type":"code","source":["def test_evaluate():\n","    final_model = GraphClassifier(config, global_size, num_tasks)\n","    path = os.path.join(best_model_path, 'checkpoint.pth')\n","    with open(path, 'rb') as f:\n","        checkpoint = cloudpickle.load(f)\n","    final_model.load_state_dict(checkpoint[\"model_state_dict\"])\n","    final_model.eval()\n","    test_score = compute_score(final_model, test_dataloader, scaler, len(test_set), num_tasks)\n","\n","    print(\"Test Score: {:.3f}\".format(test_score), \"\\n\")\n","    print(\"Execution time: {:.3f} seconds\".format(time.time() - start_time))"],"metadata":{"id":"TPyPLQahmOQH","executionInfo":{"status":"ok","timestamp":1689006062666,"user_tz":-210,"elapsed":2,"user":{"displayName":"Mahdi Salehi","userId":"13663744920024374425"}}},"execution_count":25,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8zWNqaxVnEG6"},"source":["##### Train the model and evaluate its performance"]},{"cell_type":"code","source":["start_time = time.time()\n","\n","scaler = StandardScaler()\n","train_labels = train_set.labels.numpy()\n","scaler.fit(train_labels)\n","\n","train_evaluate()\n","test_evaluate()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lwxviQvFmV-r","executionInfo":{"status":"ok","timestamp":1689006479181,"user_tz":-210,"elapsed":22518,"user":{"displayName":"Mahdi Salehi","userId":"13663744920024374425"}},"outputId":"46dd6f55-d049-4db6-cc24-9afe2ad7c4fe"},"execution_count":32,"outputs":[{"output_type":"stream","name":"stdout","text":["Save checkpoint\n","Epoch: 1/100 | Training Loss: 0.951 | Valid Score: 4.482\n"," \n","Epoch: 1/100 | Best Valid Score Until Now: 4.482 \n","\n","Patience 1\n","Epoch: 2/100 | Training Loss: 0.974 | Valid Score: 4.504\n"," \n","Epoch: 2/100 | Best Valid Score Until Now: 4.482 \n","\n","Patience 2\n","Epoch: 3/100 | Training Loss: 1.056 | Valid Score: 4.498\n"," \n","Epoch: 3/100 | Best Valid Score Until Now: 4.482 \n","\n","Patience 3\n","Epoch: 4/100 | Training Loss: 0.963 | Valid Score: 4.531\n"," \n","Epoch: 4/100 | Best Valid Score Until Now: 4.482 \n","\n","Patience 4\n","Epoch: 5/100 | Training Loss: 0.952 | Valid Score: 4.549\n"," \n","Epoch: 5/100 | Best Valid Score Until Now: 4.482 \n","\n","Patience 5\n","Epoch: 6/100 | Training Loss: 0.978 | Valid Score: 4.505\n"," \n","Epoch: 6/100 | Best Valid Score Until Now: 4.482 \n","\n","Save checkpoint\n","Epoch: 7/100 | Training Loss: 0.966 | Valid Score: 4.473\n"," \n","Epoch: 7/100 | Best Valid Score Until Now: 4.473 \n","\n","Patience 1\n","Epoch: 8/100 | Training Loss: 0.944 | Valid Score: 4.496\n"," \n","Epoch: 8/100 | Best Valid Score Until Now: 4.473 \n","\n","Patience 2\n","Epoch: 9/100 | Training Loss: 0.972 | Valid Score: 4.493\n"," \n","Epoch: 9/100 | Best Valid Score Until Now: 4.473 \n","\n","Save checkpoint\n","Epoch: 10/100 | Training Loss: 0.947 | Valid Score: 4.461\n"," \n","Epoch: 10/100 | Best Valid Score Until Now: 4.461 \n","\n","Save checkpoint\n","Epoch: 11/100 | Training Loss: 0.971 | Valid Score: 4.429\n"," \n","Epoch: 11/100 | Best Valid Score Until Now: 4.429 \n","\n","Patience 1\n","Epoch: 12/100 | Training Loss: 1.106 | Valid Score: 4.486\n"," \n","Epoch: 12/100 | Best Valid Score Until Now: 4.429 \n","\n","Patience 2\n","Epoch: 13/100 | Training Loss: 1.041 | Valid Score: 4.455\n"," \n","Epoch: 13/100 | Best Valid Score Until Now: 4.429 \n","\n","Patience 3\n","Epoch: 14/100 | Training Loss: 0.964 | Valid Score: 4.478\n"," \n","Epoch: 14/100 | Best Valid Score Until Now: 4.429 \n","\n","Patience 4\n","Epoch: 15/100 | Training Loss: 0.945 | Valid Score: 4.463\n"," \n","Epoch: 15/100 | Best Valid Score Until Now: 4.429 \n","\n","Patience 5\n","Epoch: 16/100 | Training Loss: 1.128 | Valid Score: 4.437\n"," \n","Epoch: 16/100 | Best Valid Score Until Now: 4.429 \n","\n","Patience 6\n","Epoch: 17/100 | Training Loss: 1.106 | Valid Score: 4.515\n"," \n","Epoch: 17/100 | Best Valid Score Until Now: 4.429 \n","\n","Patience 7\n","Epoch: 18/100 | Training Loss: 0.944 | Valid Score: 4.480\n"," \n","Epoch: 18/100 | Best Valid Score Until Now: 4.429 \n","\n","Patience 8\n","Epoch: 19/100 | Training Loss: 0.955 | Valid Score: 4.480\n"," \n","Epoch: 19/100 | Best Valid Score Until Now: 4.429 \n","\n","Patience 9\n","Epoch: 20/100 | Training Loss: 1.833 | Valid Score: 4.479\n"," \n","Epoch: 20/100 | Best Valid Score Until Now: 4.429 \n","\n","Save checkpoint\n","Epoch: 21/100 | Training Loss: 0.951 | Valid Score: 4.405\n"," \n","Epoch: 21/100 | Best Valid Score Until Now: 4.405 \n","\n","Patience 1\n","Epoch: 22/100 | Training Loss: 0.950 | Valid Score: 4.410\n"," \n","Epoch: 22/100 | Best Valid Score Until Now: 4.405 \n","\n","Patience 2\n","Epoch: 23/100 | Training Loss: 1.095 | Valid Score: 4.435\n"," \n","Epoch: 23/100 | Best Valid Score Until Now: 4.405 \n","\n","Patience 3\n","Epoch: 24/100 | Training Loss: 0.946 | Valid Score: 4.411\n"," \n","Epoch: 24/100 | Best Valid Score Until Now: 4.405 \n","\n","Patience 4\n","Epoch: 25/100 | Training Loss: 0.946 | Valid Score: 4.431\n"," \n","Epoch: 25/100 | Best Valid Score Until Now: 4.405 \n","\n","Patience 5\n","Epoch: 26/100 | Training Loss: 0.962 | Valid Score: 4.453\n"," \n","Epoch: 26/100 | Best Valid Score Until Now: 4.405 \n","\n","Patience 6\n","Epoch: 27/100 | Training Loss: 0.982 | Valid Score: 4.480\n"," \n","Epoch: 27/100 | Best Valid Score Until Now: 4.405 \n","\n","Patience 7\n","Epoch: 28/100 | Training Loss: 0.944 | Valid Score: 4.497\n"," \n","Epoch: 28/100 | Best Valid Score Until Now: 4.405 \n","\n","Patience 8\n","Epoch: 29/100 | Training Loss: 0.944 | Valid Score: 4.513\n"," \n","Epoch: 29/100 | Best Valid Score Until Now: 4.405 \n","\n","Patience 9\n","Epoch: 30/100 | Training Loss: 1.003 | Valid Score: 4.505\n"," \n","Epoch: 30/100 | Best Valid Score Until Now: 4.405 \n","\n","Patience 10\n","Epoch: 31/100 | Training Loss: 0.947 | Valid Score: 4.464\n"," \n","Epoch: 31/100 | Best Valid Score Until Now: 4.405 \n","\n","Final results:\n","Average Valid Score: 4.405 \n","\n","Test Score: 4.260 \n","\n","Execution time: 22.151 seconds\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"G58f3Z4Yn6yB"},"execution_count":null,"outputs":[]}]}